{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "# DEEP PROJECT NOTEBOOK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "starting doing all the import necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.amp\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "\n",
    "from CLIP import clip\n",
    "\n",
    "from COOP.models import OurCLIP\n",
    "from COOP.utils import get_optimizer, get_cost_function, log_values\n",
    "from COOP.functions import training_step, test_step\n",
    "from COOP.dataloader import get_data\n",
    "from loaders import Augmixer\n",
    "from tqdm import tqdm\n",
    "from utils import entropy, avg_entropy, load_pretrained_coop, batch_report, make_histogram\n",
    "from copy import deepcopy\n",
    "\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we implement TTA. We use AVGentropy as loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tta_net_train(batch, net, optimizer, scaler, cost_function, id2classes, device=\"cuda\", debug=False):\n",
    "    batch_idx, inputs, targets = batch\n",
    "    # Set the network to training mode\n",
    "    net.train()\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = net(inputs)\n",
    "\n",
    "    # Filter out the predictions with high entropy\n",
    "    entropies = [entropy(t).item() for t in outputs.softmax(-1)]\n",
    "    # Calculate the threshold for the lowest entropies values\n",
    "    threshold = np.percentile(entropies, 15)\n",
    "    if scaler is None:\n",
    "        outputs = outputs.softmax(-1)\n",
    "        entropies = [0 if val > threshold else val for val in entropies]\n",
    "        indices = torch.nonzero(torch.tensor(entropies)).squeeze(1)\n",
    "        filtered_outputs = outputs[indices]\n",
    "        filtered_inputs = inputs[indices]\n",
    "        avg_predictions = torch.mean(filtered_outputs, dim=0).unsqueeze(0)\n",
    "        prediction_entropy = entropy(avg_predictions).item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # loss = cost_function(avg_predictions, targets)\n",
    "        loss = avg_entropy(filtered_outputs)\n",
    "        \n",
    "        loss.backward()\n",
    "        if debug:\n",
    "            if torch.isnan(net.prompt_learner.ctx.grad).any():\n",
    "                print(\"NaN in context tokens gradient\")\n",
    "                raise ValueError(\"NaN in context tokens gradient\")\n",
    "            if torch.isinf(net.prompt_learner.ctx.grad).any():\n",
    "                print(\"Inf in context tokens gradient\")\n",
    "                raise ValueError(\"Inf in context tokens gradient\")\n",
    "\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = outputs.softmax(-1)\n",
    "            entropies = [0 if val > threshold else val for val in entropies]\n",
    "            indices = torch.nonzero(torch.tensor(entropies)).squeeze(1)\n",
    "            filtered_outputs = outputs[indices]\n",
    "            filtered_inputs = inputs[indices]\n",
    "            avg_predictions = torch.mean(filtered_outputs, dim=0).unsqueeze(0)\n",
    "            prediction_entropy = entropy(avg_predictions).item()\n",
    "            loss = avg_entropy(filtered_outputs)\n",
    "            scaler.scale(loss).backward()\n",
    "            if debug:\n",
    "                if torch.isnan(net.prompt_learner.ctx.grad).any():\n",
    "                    print(\"NaN in context tokens gradient\")\n",
    "                    raise ValueError(\"NaN in context tokens gradient\")\n",
    "                if torch.isinf(net.prompt_learner.ctx.grad).any():\n",
    "                    print(\"Inf in context tokens gradient\")\n",
    "                    raise ValueError(\"Inf in context tokens gradient\")\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "    \n",
    "    if torch.isnan(net.prompt_learner.ctx).any():\n",
    "        print(\"NaN in context tokens\")\n",
    "        raise ValueError(\"NaN in context tokens\")\n",
    "    \n",
    "    if torch.isinf(net.prompt_learner.ctx).any():\n",
    "        print(\"Inf in context tokens\")\n",
    "        raise ValueError(\"Inf in context tokens\")\n",
    "    # show batch\n",
    "    if debug:\n",
    "        batch_report(filtered_inputs, filtered_outputs, avg_predictions, targets, id2classes, batch_n=batch_idx)\n",
    "\n",
    "    prediction = avg_predictions.argmax(dim=1)\n",
    "    return loss.item(), prediction, prediction_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we implement tpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpt_train_loop(data_loader, net, optimizer, scaler, cost_function, writer, id2classes, device=\"cuda\", debug=False):\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "    top1 = 0\n",
    "    top5 = 0\n",
    "\n",
    "    no_tpt_class_acc = {c: [] for c in id2classes.values()}\n",
    "    tpt_class_acc = {c: [] for c in id2classes.values()}\n",
    "    loss_diff = 0.0\n",
    "\n",
    "    optimizer_state = deepcopy(optimizer.state_dict())\n",
    "\n",
    "    try:\n",
    "        # Disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
    "        pbar = tqdm(data_loader, desc=\"Testing\", position=0, leave=True, total=len(data_loader))\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            # Reset the prompt_learner to its initial state and the optimizer to its initial state\n",
    "            with torch.no_grad():\n",
    "                net.reset()\n",
    "                optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "            # Optimize prompts using TTA and augmentations\n",
    "            # Get prediction without prompt optimization      \n",
    "            _loss, no_tpt_prediction, no_tpt_prediction_entropy = tta_net_train((batch_idx, inputs, targets), net, optimizer, scaler, cost_function, id2classes, device=device, debug=debug)\n",
    "            #_loss, no_tpt_prediction, no_tpt_prediction_entropy = 0, torch.tensor(-1), 0\n",
    "\n",
    "            if no_tpt_prediction.item() == targets.item():\n",
    "                no_tpt_class_acc[id2classes[no_tpt_prediction.item()]].append(1)\n",
    "            else:\n",
    "                no_tpt_class_acc[id2classes[no_tpt_prediction.item()]].append(0)\n",
    "\n",
    "            # Evaluate the trained prompts on the single sample\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs[0].unsqueeze(0).to(device)\n",
    "                targets = targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                loss = cost_function(outputs, targets)\n",
    "                cumulative_loss += loss.item()\n",
    "                samples += 1\n",
    "                prediction = outputs.argmax(dim=1)\n",
    "                prediction_entropy = entropy(prediction).item()\n",
    "\n",
    "                values, predictions = outputs.topk(5)\n",
    "                if prediction == targets:\n",
    "                    top1 += 1\n",
    "                    tpt_class_acc[id2classes[no_tpt_prediction.item()]].append(1)\n",
    "                else:\n",
    "                    tpt_class_acc[id2classes[no_tpt_prediction.item()]].append(0)\n",
    "                    pass\n",
    "                if targets.item() in predictions:\n",
    "                    top5 += (targets.view(-1, 1) == predictions).sum().item()\n",
    "\n",
    "                top1_str = id2classes[prediction.item()]\n",
    "                top5_str = [id2classes[pred] for pred in predictions[0].tolist()]\n",
    "                target_str = id2classes[targets.item()]\n",
    "                loss_diff +=  _loss - loss.item() # comparison of loss with and without TPT\n",
    "                entropy_diff = prediction_entropy - no_tpt_prediction_entropy # comparison of entropy with and without TPT\n",
    "                \n",
    "            writer.add_scalar(\"Delta_loss/test\", loss_diff, batch_idx)\n",
    "            writer.add_scalar(\"Delta_entropy/test\", entropy_diff, batch_idx)\n",
    "\n",
    "            pbar.set_postfix(test_loss=loss.item(), top1=top1/samples * 100, top5=top5/samples * 100)\n",
    "            pbar.update(1)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"User keyboard interrupt\")\n",
    "    except Exception:\n",
    "        for c in id2classes.values():\n",
    "            if len(no_tpt_class_acc[c]) == 0 or len(tpt_class_acc[c]) == 0:\n",
    "                continue\n",
    "            no_tpt_acc = sum(no_tpt_class_acc[c]) / len(no_tpt_class_acc[c])\n",
    "            tpt_acc = sum(tpt_class_acc[c]) / len(tpt_class_acc[c])\n",
    "            writer.add_scalar(f\"Class accuracy/{c}\", no_tpt_acc, 0)\n",
    "            writer.add_scalar(f\"Class accuracy/{c}\", tpt_acc, 1)\n",
    "        # TODO plot histogram\n",
    "        raise\n",
    "        \n",
    "    pbar.close()\n",
    "    # Log the final values and class accuracies\n",
    "    # create histogram for class accuracies and log it with tensorboard\n",
    "    # Create single histograms for each class with a column for TPT and one for no TPT\n",
    "    \n",
    "    no_tpt_accuracies = {}\n",
    "    accuracies = {}\n",
    "\n",
    "    for c in id2classes.values():\n",
    "        if len(no_tpt_class_acc[c]) == 0 or len(tpt_class_acc[c]) == 0:\n",
    "            continue\n",
    "        no_tpt_accuracies[c] = sum(no_tpt_class_acc[c]) / len(no_tpt_class_acc[c])\n",
    "        accuracies[c] = sum(tpt_class_acc[c]) / len(tpt_class_acc[c])\n",
    "    \n",
    "    image = make_histogram(no_tpt_accuracies, accuracies, 'No TPT','TPT', save_path=\"results/imagenet_A/plots/accuracy_by_class.png\")\n",
    "    writer.add_image(\"Class accuracies\", image, 0, dataformats=\"HWC\")\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the main we load and augment the data, we load CLIp and COOP. We use AdamW as optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_name=\"imagenet_a\",\n",
    "    backbone=\"RN50\",\n",
    "    device=\"mps\",\n",
    "    batch_size=16,\n",
    "    learning_rate=0.005,\n",
    "    tta_steps=2,\n",
    "    run_name=\"exp5\",\n",
    "    n_ctx=4,\n",
    "    ctx_init=\"a_photo_of_a\",\n",
    "    class_token_position=\"end\",\n",
    "    csc=False,\n",
    "    debug=DEBUG\n",
    "):\n",
    "    print(\"Using manual seed\")\n",
    "    torch.manual_seed(0)\n",
    "    # Create a logger for the experiment\n",
    "    writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
    "\n",
    "    _, preprocess = clip.load(backbone, device=device)\n",
    "    \n",
    "    data_transform = Augmixer(preprocess, batch_size, severity=3)\n",
    "    # Get dataloaders\n",
    "    _, _, test_loader, classnames, id2class = get_data(\n",
    "        dataset_name, 1, data_transform, train_size=0, val_size=0, shuffle=True\n",
    "    )    \n",
    "\n",
    "    # Instantiate the network and move it to the chosen device (GPU)\n",
    "    net = OurCLIP(\n",
    "        classnames=classnames,\n",
    "        n_ctx=n_ctx,\n",
    "        ctx_init=ctx_init,\n",
    "        class_token_position=class_token_position,\n",
    "        backbone=backbone,\n",
    "        csc=csc,\n",
    "    ).to(device)\n",
    "\n",
    "    load_pretrained_coop(backbone, net)\n",
    "\n",
    "    print(\"Turning off gradients in both the image and the text encoder\")\n",
    "    for name, param in net.named_parameters():\n",
    "        if \"prompt_learner\" not in name:\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in net.parameters()):,}\")\n",
    "    print(\n",
    "        f\"Total trainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\"\n",
    "    )\n",
    "\n",
    "    trainable_param = net.prompt_learner.parameters()\n",
    "    optimizer = torch.optim.AdamW(trainable_param, learning_rate)\n",
    "    if device == 'cuda':\n",
    "        scaler = torch.cuda.amp.GradScaler(init_scale=1000)\n",
    "    else:\n",
    "        scaler = None\n",
    "    # Define the cost function\n",
    "    cost_function = get_cost_function()\n",
    "\n",
    "    print(\"Beginning testing with TPT:\")\n",
    "    test_loss, test_accuracy = tpt_train_loop(test_loader, net, optimizer, scaler, cost_function, writer, id2classes=id2class, device=device, debug=debug)\n",
    "    print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
    "    # Closes the logger\n",
    "    \n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
