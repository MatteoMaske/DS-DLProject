{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_-3Y34jBzv-",
        "vscode": {
          "languageId": "latex"
        }
      },
      "source": [
        "# DEEP PROJECT NOTEBOOK\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Installing requirements and preparing the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ECzfKMCLkdbB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: open_clip_torch in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (2.26.1)\n",
            "Requirement already satisfied: gdown in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (5.2.0)\n",
            "Requirement already satisfied: matplotlib in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (3.9.2)\n",
            "Requirement already satisfied: transformers in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (4.44.2)\n",
            "Collecting tensorboard\n",
            "  Downloading tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: torch>=1.9.0 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from open_clip_torch) (2.4.0)\n",
            "Requirement already satisfied: torchvision in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from open_clip_torch) (0.19.0)\n",
            "Requirement already satisfied: regex in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from open_clip_torch) (2024.7.24)\n",
            "Requirement already satisfied: ftfy in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from open_clip_torch) (6.2.3)\n",
            "Requirement already satisfied: tqdm in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from open_clip_torch) (4.66.5)\n",
            "Requirement already satisfied: huggingface-hub in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from open_clip_torch) (0.24.6)\n",
            "Requirement already satisfied: timm in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from open_clip_torch) (1.0.9)\n",
            "Requirement already satisfied: beautifulsoup4 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from gdown) (3.15.4)\n",
            "Requirement already satisfied: requests[socks] in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.23 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from matplotlib) (2.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=8 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from matplotlib) (2.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
            "Collecting absl-py>=0.4 (from tensorboard)\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting grpcio>=1.48.2 (from tensorboard)\n",
            "  Downloading grpcio-1.66.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard)\n",
            "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard)\n",
            "  Downloading protobuf-5.27.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from tensorboard) (72.1.0)\n",
            "Requirement already satisfied: six>1.9 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from tensorboard) (1.16.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
            "  Downloading werkzeug-3.0.4-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from huggingface-hub->open_clip_torch) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from huggingface-hub->open_clip_torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch>=1.9.0->open_clip_torch) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9.0->open_clip_torch) (12.6.20)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from ftfy->open_clip_torch) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from requests[socks]->gdown) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from requests[socks]->gdown) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from requests[socks]->gdown) (2024.7.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from sympy->torch>=1.9.0->open_clip_torch) (1.3.0)\n",
            "Downloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m118.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "Downloading grpcio-1.66.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m136.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
            "Downloading protobuf-5.27.3-cp38-abi3-manylinux2014_x86_64.whl (309 kB)\n",
            "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m158.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.0.4-py3-none-any.whl (227 kB)\n",
            "Installing collected packages: werkzeug, tensorboard-data-server, protobuf, markdown, grpcio, absl-py, tensorboard\n",
            "Successfully installed absl-py-2.1.0 grpcio-1.66.0 markdown-3.7 protobuf-5.27.3 tensorboard-2.17.1 tensorboard-data-server-0.7.2 werkzeug-3.0.4\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-58r_u6lk\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-58r_u6lk\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: ftfy in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from clip==1.0) (6.2.3)\n",
            "Requirement already satisfied: packaging in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from clip==1.0) (24.1)\n",
            "Requirement already satisfied: regex in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from clip==1.0) (2024.7.24)\n",
            "Requirement already satisfied: tqdm in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from clip==1.0) (4.66.5)\n",
            "Requirement already satisfied: torch in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from clip==1.0) (2.4.0)\n",
            "Requirement already satisfied: torchvision in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from clip==1.0) (0.19.0)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch->clip==1.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch->clip==1.0) (1.13.2)\n",
            "Requirement already satisfied: networkx in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch->clip==1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch->clip==1.0) (2024.6.1)\n",
            "Requirement already satisfied: setuptools in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch->clip==1.0) (72.1.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch->clip==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch->clip==1.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch->clip==1.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch->clip==1.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch->clip==1.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch->clip==1.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch->clip==1.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torch->clip==1.0) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->clip==1.0) (12.6.20)\n",
            "Requirement already satisfied: numpy in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torchvision->clip==1.0) (2.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from torchvision->clip==1.0) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "mkdir: cannot create directory ‘caption_reports’: File exists\n",
            "mkdir: cannot create directory ‘batch_predictions’: File exists\n",
            "mkdir: cannot create directory ‘batch_reports’: File exists\n",
            "mkdir: cannot create directory ‘runs’: File exists\n"
          ]
        }
      ],
      "source": [
        "!pip install open_clip_torch gdown matplotlib transformers tensorboard\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "%mkdir caption_reports batch_predictions batch_reports runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-srm4KrABzv_"
      },
      "source": [
        "<!-- Project Description, issues, pictures, present CLIP, TPT, COOP and CoCa briefly. -->\n",
        "# 1. Project Introduction\n",
        "Over the last decades Deep Neural Networks have achieved outstanding results in many computer vision applications, such as image classification, object detection, image segmentation, tracking and anomaly detection. However these suffer from a severe performance degradation when tested on images that are of different distribution than the training data. This major issue known as **domain shift**, is a common problem in many real-world applications, such as autonomous driving, medical imaging, and robotics. To address the issue, recent research has studied domain adaptation techniques. These often require access to downstream training data, which is difficult to collect; hence, researchers have focused on **Test-Time Adaptation** (TTA) methods that adapt the pre-trained model to the target domain at test time.\n",
        "\n",
        "We tested the zero-shot CLIP model by OpenAI [CLIP](https://github.com/openai/CLIP) on the ImageNetA and ImageNetV2 variants and evaluated the Top-1 accuracy of the dataset. Later, we improved it's performance by tuning the prompts with a prompt learner as described by Kaiyang et al. in [Learning to Prompt for Vision-Language Models](https://arxiv.org/pdf/2109.01134), this will be referred to as **Context Optimization (CoOp)**. Then we integrated [Test-Time Prompt Tuning](https://arxiv.org/pdf/2209.07511) proposed by Manli et al. both with and without CoOp. Lastly, our proposed method is an ensamble model of CLIP with CoOp and CoCa, that classifies a batch of image augmentations filtered on the entropy of the predictions. We tried stacking the logits of the filtered images with three different approaches: a standard deviation based approached proposed by Yang. et al. [Image-Caption Encoding for Improving Zero-Shot Generalisation](https://arxiv.org/pdf/2402.02662), a harmonic mean of the logits (ours) and an **entropy-weighted average** of the logits (ours) which achieved a **+2%** improvement in Top-1 accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0mltg5Lhgrl"
      },
      "source": [
        "Import the main augmix operations directly from TPTs codebase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niCqq1zRhgrl"
      },
      "source": [
        "Select the desired functions to use for augmentation.\n",
        "\n",
        "**augmentations_all** is just to keep track of all the augmentations that are available in the codebase.\n",
        "\n",
        "**augmentations_basic** are simple and less aggressive augmentations\n",
        "\n",
        "**post_augmentations** are a set of augmentations that are applied after augmix, as running them with augmix injects excessive amounts of noise into the image.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1qHlBoIhgrl"
      },
      "source": [
        "## Dataset and Dataloading\n",
        "\n",
        "For this project we tested our model on Imagenet-a and Imagenet-V2\n",
        "\n",
        "### Imagenet-a\n",
        "The ImageNet-A dataset is a subset of the broader ImageNet dataset, which is widely used in the field of computer vision for training and evaluating image recognition models.\n",
        "It was introduced to test the robustness of image classification models, especially in handling challenging or adversarial examples that typical models might misclassify. The dataset was created by collecting images that standard neural network models, trained on the original ImageNet dataset, often fail to classify correctly. The main goal is to provide a benchmark to evaluate the performance of models under more challenging conditions.\n",
        "\n",
        "The images are real-world photographs collected from various sources, containing objects that belong to the same 1,000 categories as the original ImageNet dataset.\n",
        "Each image is labeled with the class it belongs to, similar to the original ImageNet dataset.\n",
        "\n",
        "ImageNet-A addresses the need for more rigorous testing of image classification models beyond standard datasets. As models become more widely used in real-world applications, ensuring their robustness against adversarial conditions becomes increasingly important. By providing a challenging set of images that standard models struggle with, ImageNet-A pushes the development of more resilient and reliable computer vision systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDOle9Kihgrl"
      },
      "source": [
        "### Imagenet-V2\n",
        "\n",
        "ImageNet-V2 is a re-creation of the original ImageNet validation dataset, designed to measure the robustness and generalization performance of image classification models. The dataset was introduced to address concerns about potential overfitting to the original ImageNet validation set due to extensive hyperparameter tuning and model selection based on that specific data.\n",
        "\n",
        "ImageNet-V2 consists of new images that are intended to match the distribution and characteristics of the original ImageNet validation set as closely as possible.\n",
        "The dataset was created by re-sampling images from the web in a manner similar to the original ImageNet collection process. This was done to ensure that the new images are representative of the same categories and have similar visual properties.\n",
        "\n",
        "ImageNet-V2 is available in three different versions, each constructed using slightly different criteria:\n",
        "Matched Frequency (MF): This version matches the category frequency distribution of the original ImageNet validation set.\n",
        "Threshold 0.7 (T): This version includes images for which at least 70% of human annotators agreed on the label.\n",
        "Top Images (TI): This version includes images that were selected by human annotators as the best representatives of each class.\n",
        "We used the Matched Frequency version.\n",
        "\n",
        "ImageNet-V2, as Imagenet-a, contains images categorized into the same 1,000 classes as the original ImageNet dataset and\n",
        "each image is labeled with the class it belongs to, with annotations provided to match the structure of the original ImageNet dataset.\n",
        "\n",
        "ImageNet-V2 is used to evaluate the generalization performance of image classification models. By testing models on this dataset, researchers can assess how well models trained on the original ImageNet dataset generalize to new, but similar, data.\n",
        "The dataset provides a new benchmark for comparing the performance of different models and algorithms, offering insights into how well models generalize beyond the original validation set.\n",
        "ImageNet-V2 helps also in detecting overfitting with respect to the original validation set. Models that perform well on ImageNet but poorly on ImageNet-V2 may have overfitted to the specific examples in the original validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxZX0kC8hgrm"
      },
      "source": [
        "Both datasets are available for free online. After installing all the necessary requirements we can proceed to download them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initializing dataset classes and augmentations operations for later usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Some extra utils\n",
        "\n",
        "class Py_vars():\n",
        "    def __init__(self):\n",
        "        self.imagenet_a_wnids = ['n01498041', 'n01531178', 'n01534433', 'n01558993', 'n01580077', 'n01614925', 'n01616318', 'n01631663', 'n01641577', 'n01669191', 'n01677366', 'n01687978', 'n01694178', 'n01698640', 'n01735189', 'n01770081', 'n01770393', 'n01774750', 'n01784675', 'n01819313', 'n01820546', 'n01833805', 'n01843383', 'n01847000', 'n01855672', 'n01882714', 'n01910747', 'n01914609', 'n01924916', 'n01944390', 'n01985128', 'n01986214', 'n02007558', 'n02009912', 'n02037110', 'n02051845', 'n02077923', 'n02085620', 'n02099601', 'n02106550', 'n02106662', 'n02110958', 'n02119022', 'n02123394', 'n02127052', 'n02129165', 'n02133161', 'n02137549', 'n02165456', 'n02174001', 'n02177972', 'n02190166', 'n02206856', 'n02219486', 'n02226429', 'n02231487', 'n02233338', 'n02236044', 'n02259212', 'n02268443', 'n02279972', 'n02280649', 'n02281787', 'n02317335', 'n02325366', 'n02346627', 'n02356798', 'n02361337', 'n02410509', 'n02445715', 'n02454379', 'n02486410', 'n02492035', 'n02504458', 'n02655020', 'n02669723', 'n02672831', 'n02676566', 'n02690373', 'n02701002', 'n02730930', 'n02777292', 'n02782093', 'n02787622', 'n02793495', 'n02797295', 'n02802426', 'n02814860', 'n02815834', 'n02837789', 'n02879718', 'n02883205', 'n02895154', 'n02906734', 'n02948072', 'n02951358', 'n02980441', 'n02992211', 'n02999410', 'n03014705', 'n03026506', 'n03124043', 'n03125729', 'n03187595', 'n03196217', 'n03223299', 'n03250847', 'n03255030', 'n03291819', 'n03325584', 'n03355925', 'n03384352', 'n03388043', 'n03417042', 'n03443371', 'n03444034', 'n03445924', 'n03452741', 'n03483316', 'n03584829', 'n03590841', 'n03594945', 'n03617480', 'n03666591', 'n03670208', 'n03717622', 'n03720891', 'n03721384', 'n03724870', 'n03775071', 'n03788195', 'n03804744', 'n03837869', 'n03840681', 'n03854065', 'n03888257', 'n03891332', 'n03935335', 'n03982430', 'n04019541', 'n04033901', 'n04039381', 'n04067472', 'n04086273', 'n04099969', 'n04118538', 'n04131690', 'n04133789', 'n04141076', 'n04146614', 'n04147183', 'n04179913', 'n04208210', 'n04235860', 'n04252077', 'n04252225', 'n04254120', 'n04270147', 'n04275548', 'n04310018', 'n04317175', 'n04344873', 'n04347754', 'n04355338', 'n04366367', 'n04376876', 'n04389033', 'n04399382', 'n04442312', 'n04456115', 'n04482393', 'n04507155', 'n04509417', 'n04532670', 'n04540053', 'n04554684', 'n04562935', 'n04591713', 'n04606251', 'n07583066', 'n07695742', 'n07697313', 'n07697537', 'n07714990', 'n07718472', 'n07720875', 'n07734744', 'n07749582', 'n07753592', 'n07760859', 'n07768694', 'n07831146', 'n09229709', 'n09246464', 'n09472597', 'n09835506', 'n11879895', 'n12057211', 'n12144580', 'n12267677']\n",
        "\n",
        "        self.all_wnids = ['n01440764', 'n01443537', 'n01484850', 'n01491361', 'n01494475', 'n01496331', 'n01498041', 'n01514668', 'n01514859', 'n01518878', 'n01530575', 'n01531178', 'n01532829', 'n01534433', 'n01537544', 'n01558993', 'n01560419', 'n01580077', 'n01582220', 'n01592084', 'n01601694', 'n01608432', 'n01614925', 'n01616318', 'n01622779', 'n01629819', 'n01630670', 'n01631663', 'n01632458', 'n01632777', 'n01641577', 'n01644373', 'n01644900', 'n01664065', 'n01665541', 'n01667114', 'n01667778', 'n01669191', 'n01675722', 'n01677366', 'n01682714', 'n01685808', 'n01687978', 'n01688243', 'n01689811', 'n01692333', 'n01693334', 'n01694178', 'n01695060', 'n01697457', 'n01698640', 'n01704323', 'n01728572', 'n01728920', 'n01729322', 'n01729977', 'n01734418', 'n01735189', 'n01737021', 'n01739381', 'n01740131', 'n01742172', 'n01744401', 'n01748264', 'n01749939', 'n01751748', 'n01753488', 'n01755581', 'n01756291', 'n01768244', 'n01770081', 'n01770393', 'n01773157', 'n01773549', 'n01773797', 'n01774384', 'n01774750', 'n01775062', 'n01776313', 'n01784675', 'n01795545', 'n01796340', 'n01797886', 'n01798484', 'n01806143', 'n01806567', 'n01807496', 'n01817953', 'n01818515', 'n01819313', 'n01820546', 'n01824575', 'n01828970', 'n01829413', 'n01833805', 'n01843065', 'n01843383', 'n01847000', 'n01855032', 'n01855672', 'n01860187', 'n01871265', 'n01872401', 'n01873310', 'n01877812', 'n01882714', 'n01883070', 'n01910747', 'n01914609', 'n01917289', 'n01924916', 'n01930112', 'n01943899', 'n01944390', 'n01945685', 'n01950731', 'n01955084', 'n01968897', 'n01978287', 'n01978455', 'n01980166', 'n01981276', 'n01983481', 'n01984695', 'n01985128', 'n01986214', 'n01990800', 'n02002556', 'n02002724', 'n02006656', 'n02007558', 'n02009229', 'n02009912', 'n02011460', 'n02012849', 'n02013706', 'n02017213', 'n02018207', 'n02018795', 'n02025239', 'n02027492', 'n02028035', 'n02033041', 'n02037110', 'n02051845', 'n02056570', 'n02058221', 'n02066245', 'n02071294', 'n02074367', 'n02077923', 'n02085620', 'n02085782', 'n02085936', 'n02086079', 'n02086240', 'n02086646', 'n02086910', 'n02087046', 'n02087394', 'n02088094', 'n02088238', 'n02088364', 'n02088466', 'n02088632', 'n02089078', 'n02089867', 'n02089973', 'n02090379', 'n02090622', 'n02090721', 'n02091032', 'n02091134', 'n02091244', 'n02091467', 'n02091635', 'n02091831', 'n02092002', 'n02092339', 'n02093256', 'n02093428', 'n02093647', 'n02093754', 'n02093859', 'n02093991', 'n02094114', 'n02094258', 'n02094433', 'n02095314', 'n02095570', 'n02095889', 'n02096051', 'n02096177', 'n02096294', 'n02096437', 'n02096585', 'n02097047', 'n02097130', 'n02097209', 'n02097298', 'n02097474', 'n02097658', 'n02098105', 'n02098286', 'n02098413', 'n02099267', 'n02099429', 'n02099601', 'n02099712', 'n02099849', 'n02100236', 'n02100583', 'n02100735', 'n02100877', 'n02101006', 'n02101388', 'n02101556', 'n02102040', 'n02102177', 'n02102318', 'n02102480', 'n02102973', 'n02104029', 'n02104365', 'n02105056', 'n02105162', 'n02105251', 'n02105412', 'n02105505', 'n02105641', 'n02105855', 'n02106030', 'n02106166', 'n02106382', 'n02106550', 'n02106662', 'n02107142', 'n02107312', 'n02107574', 'n02107683', 'n02107908', 'n02108000', 'n02108089', 'n02108422', 'n02108551', 'n02108915', 'n02109047', 'n02109525', 'n02109961', 'n02110063', 'n02110185', 'n02110341', 'n02110627', 'n02110806', 'n02110958', 'n02111129', 'n02111277', 'n02111500', 'n02111889', 'n02112018', 'n02112137', 'n02112350', 'n02112706', 'n02113023', 'n02113186', 'n02113624', 'n02113712', 'n02113799', 'n02113978', 'n02114367', 'n02114548', 'n02114712', 'n02114855', 'n02115641', 'n02115913', 'n02116738', 'n02117135', 'n02119022', 'n02119789', 'n02120079', 'n02120505', 'n02123045', 'n02123159', 'n02123394', 'n02123597', 'n02124075', 'n02125311', 'n02127052', 'n02128385', 'n02128757', 'n02128925', 'n02129165', 'n02129604', 'n02130308', 'n02132136', 'n02133161', 'n02134084', 'n02134418', 'n02137549', 'n02138441', 'n02165105', 'n02165456', 'n02167151', 'n02168699', 'n02169497', 'n02172182', 'n02174001', 'n02177972', 'n02190166', 'n02206856', 'n02219486', 'n02226429', 'n02229544', 'n02231487', 'n02233338', 'n02236044', 'n02256656', 'n02259212', 'n02264363', 'n02268443', 'n02268853', 'n02276258', 'n02277742', 'n02279972', 'n02280649', 'n02281406', 'n02281787', 'n02317335', 'n02319095', 'n02321529', 'n02325366', 'n02326432', 'n02328150', 'n02342885', 'n02346627', 'n02356798', 'n02361337', 'n02363005', 'n02364673', 'n02389026', 'n02391049', 'n02395406', 'n02396427', 'n02397096', 'n02398521', 'n02403003', 'n02408429', 'n02410509', 'n02412080', 'n02415577', 'n02417914', 'n02422106', 'n02422699', 'n02423022', 'n02437312', 'n02437616', 'n02441942', 'n02442845', 'n02443114', 'n02443484', 'n02444819', 'n02445715', 'n02447366', 'n02454379', 'n02457408', 'n02480495', 'n02480855', 'n02481823', 'n02483362', 'n02483708', 'n02484975', 'n02486261', 'n02486410', 'n02487347', 'n02488291', 'n02488702', 'n02489166', 'n02490219', 'n02492035', 'n02492660', 'n02493509', 'n02493793', 'n02494079', 'n02497673', 'n02500267', 'n02504013', 'n02504458', 'n02509815', 'n02510455', 'n02514041', 'n02526121', 'n02536864', 'n02606052', 'n02607072', 'n02640242', 'n02641379', 'n02643566', 'n02655020', 'n02666196', 'n02667093', 'n02669723', 'n02672831', 'n02676566', 'n02687172', 'n02690373', 'n02692877', 'n02699494', 'n02701002', 'n02704792', 'n02708093', 'n02727426', 'n02730930', 'n02747177', 'n02749479', 'n02769748', 'n02776631', 'n02777292', 'n02782093', 'n02783161', 'n02786058', 'n02787622', 'n02788148', 'n02790996', 'n02791124', 'n02791270', 'n02793495', 'n02794156', 'n02795169', 'n02797295', 'n02799071', 'n02802426', 'n02804414', 'n02804610', 'n02807133', 'n02808304', 'n02808440', 'n02814533', 'n02814860', 'n02815834', 'n02817516', 'n02823428', 'n02823750', 'n02825657', 'n02834397', 'n02835271', 'n02837789', 'n02840245', 'n02841315', 'n02843684', 'n02859443', 'n02860847', 'n02865351', 'n02869837', 'n02870880', 'n02871525', 'n02877765', 'n02879718', 'n02883205', 'n02892201', 'n02892767', 'n02894605', 'n02895154', 'n02906734', 'n02909870', 'n02910353', 'n02916936', 'n02917067', 'n02927161', 'n02930766', 'n02939185', 'n02948072', 'n02950826', 'n02951358', 'n02951585', 'n02963159', 'n02965783', 'n02966193', 'n02966687', 'n02971356', 'n02974003', 'n02977058', 'n02978881', 'n02979186', 'n02980441', 'n02981792', 'n02988304', 'n02992211', 'n02992529', 'n02999410', 'n03000134', 'n03000247', 'n03000684', 'n03014705', 'n03016953', 'n03017168', 'n03018349', 'n03026506', 'n03028079', 'n03032252', 'n03041632', 'n03042490', 'n03045698', 'n03047690', 'n03062245', 'n03063599', 'n03063689', 'n03065424', 'n03075370', 'n03085013', 'n03089624', 'n03095699', 'n03100240', 'n03109150', 'n03110669', 'n03124043', 'n03124170', 'n03125729', 'n03126707', 'n03127747', 'n03127925', 'n03131574', 'n03133878', 'n03134739', 'n03141823', 'n03146219', 'n03160309', 'n03179701', 'n03180011', 'n03187595', 'n03188531', 'n03196217', 'n03197337', 'n03201208', 'n03207743', 'n03207941', 'n03208938', 'n03216828', 'n03218198', 'n03220513', 'n03223299', 'n03240683', 'n03249569', 'n03250847', 'n03255030', 'n03259280', 'n03271574', 'n03272010', 'n03272562', 'n03290653', 'n03291819', 'n03297495', 'n03314780', 'n03325584', 'n03337140', 'n03344393', 'n03345487', 'n03347037', 'n03355925', 'n03372029', 'n03376595', 'n03379051', 'n03384352', 'n03388043', 'n03388183', 'n03388549', 'n03393912', 'n03394916', 'n03400231', 'n03404251', 'n03417042', 'n03424325', 'n03425413', 'n03443371', 'n03444034', 'n03445777', 'n03445924', 'n03447447', 'n03447721', 'n03450230', 'n03452741', 'n03457902', 'n03459775', 'n03461385', 'n03467068', 'n03476684', 'n03476991', 'n03478589', 'n03481172', 'n03482405', 'n03483316', 'n03485407', 'n03485794', 'n03492542', 'n03494278', 'n03495258', 'n03496892', 'n03498962', 'n03527444', 'n03529860', 'n03530642', 'n03532672', 'n03534580', 'n03535780', 'n03538406', 'n03544143', 'n03584254', 'n03584829', 'n03590841', 'n03594734', 'n03594945', 'n03595614', 'n03598930', 'n03599486', 'n03602883', 'n03617480', 'n03623198', 'n03627232', 'n03630383', 'n03633091', 'n03637318', 'n03642806', 'n03649909', 'n03657121', 'n03658185', 'n03661043', 'n03662601', 'n03666591', 'n03670208', 'n03673027', 'n03676483', 'n03680355', 'n03690938', 'n03691459', 'n03692522', 'n03697007', 'n03706229', 'n03709823', 'n03710193', 'n03710637', 'n03710721', 'n03717622', 'n03720891', 'n03721384', 'n03724870', 'n03729826', 'n03733131', 'n03733281', 'n03733805', 'n03742115', 'n03743016', 'n03759954', 'n03761084', 'n03763968', 'n03764736', 'n03769881', 'n03770439', 'n03770679', 'n03773504', 'n03775071', 'n03775546', 'n03776460', 'n03777568', 'n03777754', 'n03781244', 'n03782006', 'n03785016', 'n03786901', 'n03787032', 'n03788195', 'n03788365', 'n03791053', 'n03792782', 'n03792972', 'n03793489', 'n03794056', 'n03796401', 'n03803284', 'n03804744', 'n03814639', 'n03814906', 'n03825788', 'n03832673', 'n03837869', 'n03838899', 'n03840681', 'n03841143', 'n03843555', 'n03854065', 'n03857828', 'n03866082', 'n03868242', 'n03868863', 'n03871628', 'n03873416', 'n03874293', 'n03874599', 'n03876231', 'n03877472', 'n03877845', 'n03884397', 'n03887697', 'n03888257', 'n03888605', 'n03891251', 'n03891332', 'n03895866', 'n03899768', 'n03902125', 'n03903868', 'n03908618', 'n03908714', 'n03916031', 'n03920288', 'n03924679', 'n03929660', 'n03929855', 'n03930313', 'n03930630', 'n03933933', 'n03935335', 'n03937543', 'n03938244', 'n03942813', 'n03944341', 'n03947888', 'n03950228', 'n03954731', 'n03956157', 'n03958227', 'n03961711', 'n03967562', 'n03970156', 'n03976467', 'n03976657', 'n03977966', 'n03980874', 'n03982430', 'n03983396', 'n03991062', 'n03992509', 'n03995372', 'n03998194', 'n04004767', 'n04005630', 'n04008634', 'n04009552', 'n04019541', 'n04023962', 'n04026417', 'n04033901', 'n04033995', 'n04037443', 'n04039381', 'n04040759', 'n04041544', 'n04044716', 'n04049303', 'n04065272', 'n04067472', 'n04069434', 'n04070727', 'n04074963', 'n04081281', 'n04086273', 'n04090263', 'n04099969', 'n04111531', 'n04116512', 'n04118538', 'n04118776', 'n04120489', 'n04125021', 'n04127249', 'n04131690', 'n04133789', 'n04136333', 'n04141076', 'n04141327', 'n04141975', 'n04146614', 'n04147183', 'n04149813', 'n04152593', 'n04153751', 'n04154565', 'n04162706', 'n04179913', 'n04192698', 'n04200800', 'n04201297', 'n04204238', 'n04204347', 'n04208210', 'n04209133', 'n04209239', 'n04228054', 'n04229816', 'n04235860', 'n04238763', 'n04239074', 'n04243546', 'n04251144', 'n04252077', 'n04252225', 'n04254120', 'n04254680', 'n04254777', 'n04258138', 'n04259630', 'n04263257', 'n04264628', 'n04265275', 'n04266014', 'n04270147', 'n04273569', 'n04275548', 'n04277352', 'n04285008', 'n04286575', 'n04296562', 'n04310018', 'n04311004', 'n04311174', 'n04317175', 'n04325704', 'n04326547', 'n04328186', 'n04330267', 'n04332243', 'n04335435', 'n04336792', 'n04344873', 'n04346328', 'n04347754', 'n04350905', 'n04355338', 'n04355933', 'n04356056', 'n04357314', 'n04366367', 'n04367480', 'n04370456', 'n04371430', 'n04371774', 'n04372370', 'n04376876', 'n04380533', 'n04389033', 'n04392985', 'n04398044', 'n04399382', 'n04404412', 'n04409515', 'n04417672', 'n04418357', 'n04423845', 'n04428191', 'n04429376', 'n04435653', 'n04442312', 'n04443257', 'n04447861', 'n04456115', 'n04458633', 'n04461696', 'n04462240', 'n04465501', 'n04467665', 'n04476259', 'n04479046', 'n04482393', 'n04483307', 'n04485082', 'n04486054', 'n04487081', 'n04487394', 'n04493381', 'n04501370', 'n04505470', 'n04507155', 'n04509417', 'n04515003', 'n04517823', 'n04522168', 'n04523525', 'n04525038', 'n04525305', 'n04532106', 'n04532670', 'n04536866', 'n04540053', 'n04542943', 'n04548280', 'n04548362', 'n04550184', 'n04552348', 'n04553703', 'n04554684', 'n04557648', 'n04560804', 'n04562935', 'n04579145', 'n04579432', 'n04584207', 'n04589890', 'n04590129', 'n04591157', 'n04591713', 'n04592741', 'n04596742', 'n04597913', 'n04599235', 'n04604644', 'n04606251', 'n04612504', 'n04613696', 'n06359193', 'n06596364', 'n06785654', 'n06794110', 'n06874185', 'n07248320', 'n07565083', 'n07579787', 'n07583066', 'n07584110', 'n07590611', 'n07613480', 'n07614500', 'n07615774', 'n07684084', 'n07693725', 'n07695742', 'n07697313', 'n07697537', 'n07711569', 'n07714571', 'n07714990', 'n07715103', 'n07716358', 'n07716906', 'n07717410', 'n07717556', 'n07718472', 'n07718747', 'n07720875', 'n07730033', 'n07734744', 'n07742313', 'n07745940', 'n07747607', 'n07749582', 'n07753113', 'n07753275', 'n07753592', 'n07754684', 'n07760859', 'n07768694', 'n07802026', 'n07831146', 'n07836838', 'n07860988', 'n07871810', 'n07873807', 'n07875152', 'n07880968', 'n07892512', 'n07920052', 'n07930864', 'n07932039', 'n09193705', 'n09229709', 'n09246464', 'n09256479', 'n09288635', 'n09332890', 'n09399592', 'n09421951', 'n09428293', 'n09468604', 'n09472597', 'n09835506', 'n10148035', 'n10565667', 'n11879895', 'n11939491', 'n12057211', 'n12144580', 'n12267677', 'n12620546', 'n12768682', 'n12985857', 'n12998815', 'n13037406', 'n13040303', 'n13044778', 'n13052670', 'n13054560', 'n13133613', 'n15075141']\n",
        "\n",
        "        self.num2class = {\"n01498041\": \"stingray\",\n",
        "            \"n01531178\": \"goldfinch\",\n",
        "            \"n01534433\": \"junco\",\n",
        "            \"n01558993\": \"American robin\",\n",
        "            \"n01580077\": \"jay\",\n",
        "            \"n01614925\": \"bald eagle\",\n",
        "            \"n01616318\": \"vulture\",\n",
        "            \"n01631663\": \"newt\",\n",
        "            \"n01641577\": \"American bullfrog\",\n",
        "            \"n01669191\": \"box turtle\",\n",
        "            \"n01677366\": \"green iguana\",\n",
        "            \"n01687978\": \"agama\",\n",
        "            \"n01694178\": \"chameleon\",\n",
        "            \"n01698640\": \"American alligator\",\n",
        "            \"n01735189\": \"garter snake\",\n",
        "            \"n01770081\": \"harvestman\",\n",
        "            \"n01770393\": \"scorpion\",\n",
        "            \"n01774750\": \"tarantula\",\n",
        "            \"n01784675\": \"centipede\",\n",
        "            \"n01819313\": \"sulphur-crested cockatoo\",\n",
        "            \"n01820546\": \"lorikeet\",\n",
        "            \"n01833805\": \"hummingbird\",\n",
        "            \"n01843383\": \"toucan\",\n",
        "            \"n01847000\": \"duck\",\n",
        "            \"n01855672\": \"goose\",\n",
        "            \"n01882714\": \"koala\",\n",
        "            \"n01910747\": \"jellyfish\",\n",
        "            \"n01914609\": \"sea anemone\",\n",
        "            \"n01924916\": \"flatworm\",\n",
        "            \"n01944390\": \"snail\",\n",
        "            \"n01985128\": \"crayfish\",\n",
        "            \"n01986214\": \"hermit crab\",\n",
        "            \"n02007558\": \"flamingo\",\n",
        "            \"n02009912\": \"great egret\",\n",
        "            \"n02037110\": \"oystercatcher\",\n",
        "            \"n02051845\": \"pelican\",\n",
        "            \"n02077923\": \"sea lion\",\n",
        "            \"n02085620\": \"Chihuahua\",\n",
        "            \"n02099601\": \"Golden Retriever\",\n",
        "            \"n02106550\": \"Rottweiler\",\n",
        "            \"n02106662\": \"German Shepherd Dog\",\n",
        "            \"n02110958\": \"pug\",\n",
        "            \"n02119022\": \"red fox\",\n",
        "            \"n02123394\": \"Persian cat\",\n",
        "            \"n02127052\": \"lynx\",\n",
        "            \"n02129165\": \"lion\",\n",
        "            \"n02133161\": \"American black bear\",\n",
        "            \"n02137549\": \"mongoose\",\n",
        "            \"n02165456\": \"ladybug\",\n",
        "            \"n02174001\": \"rhinoceros beetle\",\n",
        "            \"n02177972\": \"weevil\",\n",
        "            \"n02190166\": \"fly\",\n",
        "            \"n02206856\": \"bee\",\n",
        "            \"n02219486\": \"ant\",\n",
        "            \"n02226429\": \"grasshopper\",\n",
        "            \"n02231487\": \"stick insect\",\n",
        "            \"n02233338\": \"cockroach\",\n",
        "            \"n02236044\": \"mantis\",\n",
        "            \"n02259212\": \"leafhopper\",\n",
        "            \"n02268443\": \"dragonfly\",\n",
        "            \"n02279972\": \"monarch butterfly\",\n",
        "            \"n02280649\": \"small white\",\n",
        "            \"n02281787\": \"gossamer-winged butterfly\",\n",
        "            \"n02317335\": \"starfish\",\n",
        "            \"n02325366\": \"cottontail rabbit\",\n",
        "            \"n02346627\": \"porcupine\",\n",
        "            \"n02356798\": \"fox squirrel\",\n",
        "            \"n02361337\": \"marmot\",\n",
        "            \"n02410509\": \"bison\",\n",
        "            \"n02445715\": \"skunk\",\n",
        "            \"n02454379\": \"armadillo\",\n",
        "            \"n02486410\": \"baboon\",\n",
        "            \"n02492035\": \"white-headed capuchin\",\n",
        "            \"n02504458\": \"African bush elephant\",\n",
        "            \"n02655020\": \"pufferfish\",\n",
        "            \"n02669723\": \"academic gown\",\n",
        "            \"n02672831\": \"accordion\",\n",
        "            \"n02676566\": \"acoustic guitar\",\n",
        "            \"n02690373\": \"airliner\",\n",
        "            \"n02701002\": \"ambulance\",\n",
        "            \"n02730930\": \"apron\",\n",
        "            \"n02777292\": \"balance beam\",\n",
        "            \"n02782093\": \"balloon\",\n",
        "            \"n02787622\": \"banjo\",\n",
        "            \"n02793495\": \"barn\",\n",
        "            \"n02797295\": \"wheelbarrow\",\n",
        "            \"n02802426\": \"basketball\",\n",
        "            \"n02814860\": \"lighthouse\",\n",
        "            \"n02815834\": \"beaker\",\n",
        "            \"n02837789\": \"bikini\",\n",
        "            \"n02879718\": \"bow\",\n",
        "            \"n02883205\": \"bow tie\",\n",
        "            \"n02895154\": \"breastplate\",\n",
        "            \"n02906734\": \"broom\",\n",
        "            \"n02948072\": \"candle\",\n",
        "            \"n02951358\": \"canoe\",\n",
        "            \"n02980441\": \"castle\",\n",
        "            \"n02992211\": \"cello\",\n",
        "            \"n02999410\": \"chain\",\n",
        "            \"n03014705\": \"chest\",\n",
        "            \"n03026506\": \"Christmas stocking\",\n",
        "            \"n03124043\": \"cowboy boot\",\n",
        "            \"n03125729\": \"cradle\",\n",
        "            \"n03187595\": \"rotary dial telephone\",\n",
        "            \"n03196217\": \"digital clock\",\n",
        "            \"n03223299\": \"doormat\",\n",
        "            \"n03250847\": \"drumstick\",\n",
        "            \"n03255030\": \"dumbbell\",\n",
        "            \"n03291819\": \"envelope\",\n",
        "            \"n03325584\": \"feather boa\",\n",
        "            \"n03355925\": \"flagpole\",\n",
        "            \"n03384352\": \"forklift\",\n",
        "            \"n03388043\": \"fountain\",\n",
        "            \"n03417042\": \"garbage truck\",\n",
        "            \"n03443371\": \"goblet\",\n",
        "            \"n03444034\": \"go-kart\",\n",
        "            \"n03445924\": \"golf cart\",\n",
        "            \"n03452741\": \"grand piano\",\n",
        "            \"n03483316\": \"hair dryer\",\n",
        "            \"n03584829\": \"clothes iron\",\n",
        "            \"n03590841\": \"jack-o'-lantern\",\n",
        "            \"n03594945\": \"jeep\",\n",
        "            \"n03617480\": \"kimono\",\n",
        "            \"n03666591\": \"lighter\",\n",
        "            \"n03670208\": \"limousine\",\n",
        "            \"n03717622\": \"manhole cover\",\n",
        "            \"n03720891\": \"maraca\",\n",
        "            \"n03721384\": \"marimba\",\n",
        "            \"n03724870\": \"mask\",\n",
        "            \"n03775071\": \"mitten\",\n",
        "            \"n03788195\": \"mosque\",\n",
        "            \"n03804744\": \"nail\",\n",
        "            \"n03837869\": \"obelisk\",\n",
        "            \"n03840681\": \"ocarina\",\n",
        "            \"n03854065\": \"organ\",\n",
        "            \"n03888257\": \"parachute\",\n",
        "            \"n03891332\": \"parking meter\",\n",
        "            \"n03935335\": \"piggy bank\",\n",
        "            \"n03982430\": \"billiard table\",\n",
        "            \"n04019541\": \"hockey puck\",\n",
        "            \"n04033901\": \"quill\",\n",
        "            \"n04039381\": \"racket\",\n",
        "            \"n04067472\": \"reel\",\n",
        "            \"n04086273\": \"revolver\",\n",
        "            \"n04099969\": \"rocking chair\",\n",
        "            \"n04118538\": \"rugby ball\",\n",
        "            \"n04131690\": \"salt shaker\",\n",
        "            \"n04133789\": \"sandal\",\n",
        "            \"n04141076\": \"saxophone\",\n",
        "            \"n04146614\": \"school bus\",\n",
        "            \"n04147183\": \"schooner\",\n",
        "            \"n04179913\": \"sewing machine\",\n",
        "            \"n04208210\": \"shovel\",\n",
        "            \"n04235860\": \"sleeping bag\",\n",
        "            \"n04252077\": \"snowmobile\",\n",
        "            \"n04252225\": \"snowplow\",\n",
        "            \"n04254120\": \"soap dispenser\",\n",
        "            \"n04270147\": \"spatula\",\n",
        "            \"n04275548\": \"spider web\",\n",
        "            \"n04310018\": \"steam locomotive\",\n",
        "            \"n04317175\": \"stethoscope\",\n",
        "            \"n04344873\": \"couch\",\n",
        "            \"n04347754\": \"submarine\",\n",
        "            \"n04355338\": \"sundial\",\n",
        "            \"n04366367\": \"suspension bridge\",\n",
        "            \"n04376876\": \"syringe\",\n",
        "            \"n04389033\": \"tank\",\n",
        "            \"n04399382\": \"teddy bear\",\n",
        "            \"n04442312\": \"toaster\",\n",
        "            \"n04456115\": \"torch\",\n",
        "            \"n04482393\": \"tricycle\",\n",
        "            \"n04507155\": \"umbrella\",\n",
        "            \"n04509417\": \"unicycle\",\n",
        "            \"n04532670\": \"viaduct\",\n",
        "            \"n04540053\": \"volleyball\",\n",
        "            \"n04554684\": \"washing machine\",\n",
        "            \"n04562935\": \"water tower\",\n",
        "            \"n04591713\": \"wine bottle\",\n",
        "            \"n04606251\": \"shipwreck\",\n",
        "            \"n07583066\": \"guacamole\",\n",
        "            \"n07695742\": \"pretzel\",\n",
        "            \"n07697313\": \"cheeseburger\",\n",
        "            \"n07697537\": \"hot dog\",\n",
        "            \"n07714990\": \"broccoli\",\n",
        "            \"n07718472\": \"cucumber\",\n",
        "            \"n07720875\": \"bell pepper\",\n",
        "            \"n07734744\": \"mushroom\",\n",
        "            \"n07749582\": \"lemon\",\n",
        "            \"n07753592\": \"banana\",\n",
        "            \"n07760859\": \"custard apple\",\n",
        "            \"n07768694\": \"pomegranate\",\n",
        "            \"n07831146\": \"carbonara\",\n",
        "            \"n09229709\": \"bubble\",\n",
        "            \"n09246464\": \"cliff\",\n",
        "            \"n09472597\": \"volcano\",\n",
        "            \"n09835506\": \"baseball player\",\n",
        "            \"n11879895\": \"rapeseed\",\n",
        "            \"n12057211\": \"yellow lady's slipper\",\n",
        "            \"n12144580\": \"corn\",\n",
        "            \"n12267677\": \"acorn\"\n",
        "        }\n",
        "\n",
        "        self.num2class_v2 = {0: 'tench', 1: 'goldfish', 2: 'great white shark', 3: 'tiger shark', 4: 'hammerhead', 5: 'electric ray', 6: 'stingray', 7: 'cock', 8: 'hen', 9: 'ostrich', 10: 'brambling', 11: 'goldfinch', 12: 'house finch', 13: 'junco', 14: 'indigo bunting', 15: 'robin', 16: 'bulbul', 17: 'jay', 18: 'magpie', 19: 'chickadee', 20: 'water ouzel', 21: 'kite', 22: 'bald eagle', 23: 'vulture', 24: 'great grey owl', 25: 'European fire salamander', 26: 'common newt', 27: 'eft', 28: 'spotted salamander', 29: 'axolotl', 30: 'bullfrog', 31: 'tree frog', 32: 'tailed frog', 33: 'loggerhead', 34: 'leatherback turtle', 35: 'mud turtle', 36: 'terrapin', 37: 'box turtle', 38: 'banded gecko', 39: 'common iguana', 40: 'American chameleon', 41: 'whiptail', 42: 'agama', 43: 'frilled lizard', 44: 'alligator lizard', 45: 'Gila monster', 46: 'green lizard', 47: 'African chameleon', 48: 'Komodo dragon', 49: 'African crocodile', 50: 'American alligator', 51: 'triceratops', 52: 'thunder snake', 53: 'ringneck snake', 54: 'hognose snake', 55: 'green snake', 56: 'king snake', 57: 'garter snake', 58: 'water snake', 59: 'vine snake', 60: 'night snake', 61: 'boa constrictor', 62: 'rock python', 63: 'Indian cobra', 64: 'green mamba', 65: 'sea snake', 66: 'horned viper', 67: 'diamondback', 68: 'sidewinder', 69: 'trilobite', 70: 'harvestman', 71: 'scorpion', 72: 'black and gold garden spider', 73: 'barn spider', 74: 'garden spider', 75: 'black widow', 76: 'tarantula', 77: 'wolf spider', 78: 'tick', 79: 'centipede', 80: 'black grouse', 81: 'ptarmigan', 82: 'ruffed grouse', 83: 'prairie chicken', 84: 'peacock', 85: 'quail', 86: 'partridge', 87: 'African grey', 88: 'macaw', 89: 'sulphur-crested cockatoo', 90: 'lorikeet', 91: 'coucal', 92: 'bee eater', 93: 'hornbill', 94: 'hummingbird', 95: 'jacamar', 96: 'toucan', 97: 'drake', 98: 'red-breasted merganser', 99: 'goose', 100: 'black swan', 101: 'tusker', 102: 'echidna', 103: 'platypus', 104: 'wallaby', 105: 'koala', 106: 'wombat', 107: 'jellyfish', 108: 'sea anemone', 109: 'brain coral', 110: 'flatworm', 111: 'nematode', 112: 'conch', 113: 'snail', 114: 'slug', 115: 'sea slug', 116: 'chiton', 117: 'chambered nautilus', 118: 'Dungeness crab', 119: 'rock crab', 120: 'fiddler crab', 121: 'king crab', 122: 'American lobster', 123: 'spiny lobster', 124: 'crayfish', 125: 'hermit crab', 126: 'isopod', 127: 'white stork', 128: 'black stork', 129: 'spoonbill', 130: 'flamingo', 131: 'little blue heron', 132: 'American egret', 133: 'bittern', 134: 'crane', 135: 'limpkin', 136: 'European gallinule', 137: 'American coot', 138: 'bustard', 139: 'ruddy turnstone', 140: 'red-backed sandpiper', 141: 'redshank', 142: 'dowitcher', 143: 'oystercatcher', 144: 'pelican', 145: 'king penguin', 146: 'albatross', 147: 'grey whale', 148: 'killer whale', 149: 'dugong', 150: 'sea lion', 151: 'Chihuahua', 152: 'Japanese spaniel', 153: 'Maltese dog', 154: 'Pekinese', 155: 'Shih-Tzu', 156: 'Blenheim spaniel', 157: 'papillon', 158: 'toy terrier', 159: 'Rhodesian ridgeback', 160: 'Afghan hound', 161: 'basset', 162: 'beagle', 163: 'bloodhound', 164: 'bluetick', 165: 'black-and-tan coonhound', 166: 'Walker hound', 167: 'English foxhound', 168: 'redbone', 169: 'borzoi', 170: 'Irish wolfhound', 171: 'Italian greyhound', 172: 'whippet', 173: 'Ibizan hound', 174: 'Norwegian elkhound', 175: 'otterhound', 176: 'Saluki', 177: 'Scottish deerhound', 178: 'Weimaraner', 179: 'Staffordshire bullterrier', 180: 'American Staffordshire terrier', 181: 'Bedlington terrier', 182: 'Border terrier', 183: 'Kerry blue terrier', 184: 'Irish terrier', 185: 'Norfolk terrier', 186: 'Norwich terrier', 187: 'Yorkshire terrier', 188: 'wire-haired fox terrier', 189: 'Lakeland terrier', 190: 'Sealyham terrier', 191: 'Airedale', 192: 'cairn', 193: 'Australian terrier', 194: 'Dandie Dinmont', 195: 'Boston bull', 196: 'miniature schnauzer', 197: 'giant schnauzer', 198: 'standard schnauzer', 199: 'Scotch terrier', 200: 'Tibetan terrier', 201: 'silky terrier', 202: 'soft-coated wheaten terrier', 203: 'West Highland white terrier', 204: 'Lhasa', 205: 'flat-coated retriever', 206: 'curly-coated retriever', 207: 'golden retriever', 208: 'Labrador retriever', 209: 'Chesapeake Bay retriever', 210: 'German short-haired pointer', 211: 'vizsla', 212: 'English setter', 213: 'Irish setter', 214: 'Gordon setter', 215: 'Brittany spaniel', 216: 'clumber', 217: 'English springer', 218: 'Welsh springer spaniel', 219: 'cocker spaniel', 220: 'Sussex spaniel', 221: 'Irish water spaniel', 222: 'kuvasz', 223: 'schipperke', 224: 'groenendael', 225: 'malinois', 226: 'briard', 227: 'kelpie', 228: 'komondor', 229: 'Old English sheepdog', 230: 'Shetland sheepdog', 231: 'collie', 232: 'Border collie', 233: 'Bouvier des Flandres', 234: 'Rottweiler', 235: 'German shepherd', 236: 'Doberman', 237: 'miniature pinscher', 238: 'Greater Swiss Mountain dog', 239: 'Bernese mountain dog', 240: 'Appenzeller', 241: 'EntleBucher', 242: 'boxer', 243: 'bull mastiff', 244: 'Tibetan mastiff', 245: 'French bulldog', 246: 'Great Dane', 247: 'Saint Bernard', 248: 'Eskimo dog', 249: 'malamute', 250: 'Siberian husky', 251: 'dalmatian', 252: 'affenpinscher', 253: 'basenji', 254: 'pug', 255: 'Leonberg', 256: 'Newfoundland', 257: 'Great Pyrenees', 258: 'Samoyed', 259: 'Pomeranian', 260: 'chow', 261: 'keeshond', 262: 'Brabancon griffon', 263: 'Pembroke', 264: 'Cardigan', 265: 'toy poodle', 266: 'miniature poodle', 267: 'standard poodle', 268: 'Mexican hairless', 269: 'timber wolf', 270: 'white wolf', 271: 'red wolf', 272: 'coyote', 273: 'dingo', 274: 'dhole', 275: 'African hunting dog', 276: 'hyena', 277: 'red fox', 278: 'kit fox', 279: 'Arctic fox', 280: 'grey fox', 281: 'tabby', 282: 'tiger cat', 283: 'Persian cat', 284: 'Siamese cat', 285: 'Egyptian cat', 286: 'cougar', 287: 'lynx', 288: 'leopard', 289: 'snow leopard', 290: 'jaguar', 291: 'lion', 292: 'tiger', 293: 'cheetah', 294: 'brown bear', 295: 'American black bear', 296: 'ice bear', 297: 'sloth bear', 298: 'mongoose', 299: 'meerkat', 300: 'tiger beetle', 301: 'ladybug', 302: 'ground beetle', 303: 'long-horned beetle', 304: 'leaf beetle', 305: 'dung beetle', 306: 'rhinoceros beetle', 307: 'weevil', 308: 'fly', 309: 'bee', 310: 'ant', 311: 'grasshopper', 312: 'cricket', 313: 'walking stick', 314: 'cockroach', 315: 'mantis', 316: 'cicada', 317: 'leafhopper', 318: 'lacewing', 319: 'dragonfly', 320: 'damselfly', 321: 'admiral', 322: 'ringlet', 323: 'monarch', 324: 'cabbage butterfly', 325: 'sulphur butterfly', 326: 'lycaenid', 327: 'starfish', 328: 'sea urchin', 329: 'sea cucumber', 330: 'wood rabbit', 331: 'hare', 332: 'Angora', 333: 'hamster', 334: 'porcupine', 335: 'fox squirrel', 336: 'marmot', 337: 'beaver', 338: 'guinea pig', 339: 'sorrel', 340: 'zebra', 341: 'hog', 342: 'wild boar', 343: 'warthog', 344: 'hippopotamus', 345: 'ox', 346: 'water buffalo', 347: 'bison', 348: 'ram', 349: 'bighorn', 350: 'ibex', 351: 'hartebeest', 352: 'impala', 353: 'gazelle', 354: 'Arabian camel', 355: 'llama', 356: 'weasel', 357: 'mink', 358: 'polecat', 359: 'black-footed ferret', 360: 'otter', 361: 'skunk', 362: 'badger', 363: 'armadillo', 364: 'three-toed sloth', 365: 'orangutan', 366: 'gorilla', 367: 'chimpanzee', 368: 'gibbon', 369: 'siamang', 370: 'guenon', 371: 'patas', 372: 'baboon', 373: 'macaque', 374: 'langur', 375: 'colobus', 376: 'proboscis monkey', 377: 'marmoset', 378: 'capuchin', 379: 'howler monkey', 380: 'titi', 381: 'spider monkey', 382: 'squirrel monkey', 383: 'Madagascar cat', 384: 'indri', 385: 'Indian elephant', 386: 'African elephant', 387: 'lesser panda', 388: 'giant panda', 389: 'barracouta', 390: 'eel', 391: 'coho', 392: 'rock beauty', 393: 'anemone fish', 394: 'sturgeon', 395: 'gar', 396: 'lionfish', 397: 'puffer', 398: 'abacus', 399: 'abaya', 400: 'academic gown', 401: 'accordion', 402: 'acoustic guitar', 403: 'aircraft carrier', 404: 'airliner', 405: 'airship', 406: 'altar', 407: 'ambulance', 408: 'amphibian', 409: 'analog clock', 410: 'apiary', 411: 'apron', 412: 'ashcan', 413: 'assault rifle', 414: 'backpack', 415: 'bakery', 416: 'balance beam', 417: 'balloon', 418: 'ballpoint', 419: 'Band Aid', 420: 'banjo', 421: 'bannister', 422: 'barbell', 423: 'barber chair', 424: 'barbershop', 425: 'barn', 426: 'barometer', 427: 'barrel', 428: 'barrow', 429: 'baseball', 430: 'basketball', 431: 'bassinet', 432: 'bassoon', 433: 'bathing cap', 434: 'bath towel', 435: 'bathtub', 436: 'beach wagon', 437: 'beacon', 438: 'beaker', 439: 'bearskin', 440: 'beer bottle', 441: 'beer glass', 442: 'bell cote', 443: 'bib', 444: 'bicycle-built-for-two', 445: 'bikini', 446: 'binder', 447: 'binoculars', 448: 'birdhouse', 449: 'boathouse', 450: 'bobsled', 451: 'bolo tie', 452: 'bonnet', 453: 'bookcase', 454: 'bookshop', 455: 'bottlecap', 456: 'bow', 457: 'bow tie', 458: 'brass', 459: 'brassiere', 460: 'breakwater', 461: 'breastplate', 462: 'broom', 463: 'bucket', 464: 'buckle', 465: 'bulletproof vest', 466: 'bullet train', 467: 'butcher shop', 468: 'cab', 469: 'caldron', 470: 'candle', 471: 'cannon', 472: 'canoe', 473: 'can opener', 474: 'cardigan', 475: 'car mirror', 476: 'carousel', 477: \"carpenter's kit\", 478: 'carton', 479: 'car wheel', 480: 'cash machine', 481: 'cassette', 482: 'cassette player', 483: 'castle', 484: 'catamaran', 485: 'CD player', 486: 'cello', 487: 'cellular telephone', 488: 'chain', 489: 'chainlink fence', 490: 'chain mail', 491: 'chain saw', 492: 'chest', 493: 'chiffonier', 494: 'chime', 495: 'china cabinet', 496: 'Christmas stocking', 497: 'church', 498: 'cinema', 499: 'cleaver', 500: 'cliff dwelling', 501: 'cloak', 502: 'clog', 503: 'cocktail shaker', 504: 'coffee mug', 505: 'coffeepot', 506: 'coil', 507: 'combination lock', 508: 'computer keyboard', 509: 'confectionery', 510: 'container ship', 511: 'convertible', 512: 'corkscrew', 513: 'cornet', 514: 'cowboy boot', 515: 'cowboy hat', 516: 'cradle', 517: 'crane', 518: 'crash helmet', 519: 'crate', 520: 'crib', 521: 'Crock Pot', 522: 'croquet ball', 523: 'crutch', 524: 'cuirass', 525: 'dam', 526: 'desk', 527: 'desktop computer', 528: 'dial telephone', 529: 'diaper', 530: 'digital clock', 531: 'digital watch', 532: 'dining table', 533: 'dishrag', 534: 'dishwasher', 535: 'disk brake', 536: 'dock', 537: 'dogsled', 538: 'dome', 539: 'doormat', 540: 'drilling platform', 541: 'drum', 542: 'drumstick', 543: 'dumbbell', 544: 'Dutch oven', 545: 'electric fan', 546: 'electric guitar', 547: 'electric locomotive', 548: 'entertainment center', 549: 'envelope', 550: 'espresso maker', 551: 'face powder', 552: 'feather boa', 553: 'file', 554: 'fireboat', 555: 'fire engine', 556: 'fire screen', 557: 'flagpole', 558: 'flute', 559: 'folding chair', 560: 'football helmet', 561: 'forklift', 562: 'fountain', 563: 'fountain pen', 564: 'four-poster', 565: 'freight car', 566: 'French horn', 567: 'frying pan', 568: 'fur coat', 569: 'garbage truck', 570: 'gasmask', 571: 'gas pump', 572: 'goblet', 573: 'go-kart', 574: 'golf ball', 575: 'golfcart', 576: 'gondola', 577: 'gong', 578: 'gown', 579: 'grand piano', 580: 'greenhouse', 581: 'grille', 582: 'grocery store', 583: 'guillotine', 584: 'hair slide', 585: 'hair spray', 586: 'half track', 587: 'hammer', 588: 'hamper', 589: 'hand blower', 590: 'hand-held computer', 591: 'handkerchief', 592: 'hard disc', 593: 'harmonica', 594: 'harp', 595: 'harvester', 596: 'hatchet', 597: 'holster', 598: 'home theater', 599: 'honeycomb', 600: 'hook', 601: 'hoopskirt', 602: 'horizontal bar', 603: 'horse cart', 604: 'hourglass', 605: 'iPod', 606: 'iron', 607: \"jack-o'-lantern\", 608: 'jean', 609: 'jeep', 610: 'jersey', 611: 'jigsaw puzzle', 612: 'jinrikisha', 613: 'joystick', 614: 'kimono', 615: 'knee pad', 616: 'knot', 617: 'lab coat', 618: 'ladle', 619: 'lampshade', 620: 'laptop', 621: 'lawn mower', 622: 'lens cap', 623: 'letter opener', 624: 'library', 625: 'lifeboat', 626: 'lighter', 627: 'limousine', 628: 'liner', 629: 'lipstick', 630: 'Loafer', 631: 'lotion', 632: 'loudspeaker', 633: 'loupe', 634: 'lumbermill', 635: 'magnetic compass', 636: 'mailbag', 637: 'mailbox', 638: 'maillot', 639: 'maillot', 640: 'manhole cover', 641: 'maraca', 642: 'marimba', 643: 'mask', 644: 'matchstick', 645: 'maypole', 646: 'maze', 647: 'measuring cup', 648: 'medicine chest', 649: 'megalith', 650: 'microphone', 651: 'microwave', 652: 'military uniform', 653: 'milk can', 654: 'minibus', 655: 'miniskirt', 656: 'minivan', 657: 'missile', 658: 'mitten', 659: 'mixing bowl', 660: 'mobile home', 661: 'Model T', 662: 'modem', 663: 'monastery', 664: 'monitor', 665: 'moped', 666: 'mortar', 667: 'mortarboard', 668: 'mosque', 669: 'mosquito net', 670: 'motor scooter', 671: 'mountain bike', 672: 'mountain tent', 673: 'mouse', 674: 'mousetrap', 675: 'moving van', 676: 'muzzle', 677: 'nail', 678: 'neck brace', 679: 'necklace', 680: 'nipple', 681: 'notebook', 682: 'obelisk', 683: 'oboe', 684: 'ocarina', 685: 'odometer', 686: 'oil filter', 687: 'organ', 688: 'oscilloscope', 689: 'overskirt', 690: 'oxcart', 691: 'oxygen mask', 692: 'packet', 693: 'paddle', 694: 'paddlewheel', 695: 'padlock', 696: 'paintbrush', 697: 'pajama', 698: 'palace', 699: 'panpipe', 700: 'paper towel', 701: 'parachute', 702: 'parallel bars', 703: 'park bench', 704: 'parking meter', 705: 'passenger car', 706: 'patio', 707: 'pay-phone', 708: 'pedestal', 709: 'pencil box', 710: 'pencil sharpener', 711: 'perfume', 712: 'Petri dish', 713: 'photocopier', 714: 'pick', 715: 'pickelhaube', 716: 'picket fence', 717: 'pickup', 718: 'pier', 719: 'piggy bank', 720: 'pill bottle', 721: 'pillow', 722: 'ping-pong ball', 723: 'pinwheel', 724: 'pirate', 725: 'pitcher', 726: 'plane', 727: 'planetarium', 728: 'plastic bag', 729: 'plate rack', 730: 'plow', 731: 'plunger', 732: 'Polaroid camera', 733: 'pole', 734: 'police van', 735: 'poncho', 736: 'pool table', 737: 'pop bottle', 738: 'pot', 739: \"potter's wheel\", 740: 'power drill', 741: 'prayer rug', 742: 'printer', 743: 'prison', 744: 'projectile', 745: 'projector', 746: 'puck', 747: 'punching bag', 748: 'purse', 749: 'quill', 750: 'quilt', 751: 'racer', 752: 'racket', 753: 'radiator', 754: 'radio', 755: 'radio telescope', 756: 'rain barrel', 757: 'recreational vehicle', 758: 'reel', 759: 'reflex camera', 760: 'refrigerator', 761: 'remote control', 762: 'restaurant', 763: 'revolver', 764: 'rifle', 765: 'rocking chair', 766: 'rotisserie', 767: 'rubber eraser', 768: 'rugby ball', 769: 'rule', 770: 'running shoe', 771: 'safe', 772: 'safety pin', 773: 'saltshaker', 774: 'sandal', 775: 'sarong', 776: 'sax', 777: 'scabbard', 778: 'scale', 779: 'school bus', 780: 'schooner', 781: 'scoreboard', 782: 'screen', 783: 'screw', 784: 'screwdriver', 785: 'seat belt', 786: 'sewing machine', 787: 'shield', 788: 'shoe shop', 789: 'shoji', 790: 'shopping basket', 791: 'shopping cart', 792: 'shovel', 793: 'shower cap', 794: 'shower curtain', 795: 'ski', 796: 'ski mask', 797: 'sleeping bag', 798: 'slide rule', 799: 'sliding door', 800: 'slot', 801: 'snorkel', 802: 'snowmobile', 803: 'snowplow', 804: 'soap dispenser', 805: 'soccer ball', 806: 'sock', 807: 'solar dish', 808: 'sombrero', 809: 'soup bowl', 810: 'space bar', 811: 'space heater', 812: 'space shuttle', 813: 'spatula', 814: 'speedboat', 815: 'spider web', 816: 'spindle', 817: 'sports car', 818: 'spotlight', 819: 'stage', 820: 'steam locomotive', 821: 'steel arch bridge', 822: 'steel drum', 823: 'stethoscope', 824: 'stole', 825: 'stone wall', 826: 'stopwatch', 827: 'stove', 828: 'strainer', 829: 'streetcar', 830: 'stretcher', 831: 'studio couch', 832: 'stupa', 833: 'submarine', 834: 'suit', 835: 'sundial', 836: 'sunglass', 837: 'sunglasses', 838: 'sunscreen', 839: 'suspension bridge', 840: 'swab', 841: 'sweatshirt', 842: 'swimming trunks', 843: 'swing', 844: 'switch', 845: 'syringe', 846: 'table lamp', 847: 'tank', 848: 'tape player', 849: 'teapot', 850: 'teddy', 851: 'television', 852: 'tennis ball', 853: 'thatch', 854: 'theater curtain', 855: 'thimble', 856: 'thresher', 857: 'throne', 858: 'tile roof', 859: 'toaster', 860: 'tobacco shop', 861: 'toilet seat', 862: 'torch', 863: 'totem pole', 864: 'tow truck', 865: 'toyshop', 866: 'tractor', 867: 'trailer truck', 868: 'tray', 869: 'trench coat', 870: 'tricycle', 871: 'trimaran', 872: 'tripod', 873: 'triumphal arch', 874: 'trolleybus', 875: 'trombone', 876: 'tub', 877: 'turnstile', 878: 'typewriter keyboard', 879: 'umbrella', 880: 'unicycle', 881: 'upright', 882: 'vacuum', 883: 'vase', 884: 'vault', 885: 'velvet', 886: 'vending machine', 887: 'vestment', 888: 'viaduct', 889: 'violin', 890: 'volleyball', 891: 'waffle iron', 892: 'wall clock', 893: 'wallet', 894: 'wardrobe', 895: 'warplane', 896: 'washbasin', 897: 'washer', 898: 'water bottle', 899: 'water jug', 900: 'water tower', 901: 'whiskey jug', 902: 'whistle', 903: 'wig', 904: 'window screen', 905: 'window shade', 906: 'Windsor tie', 907: 'wine bottle', 908: 'wing', 909: 'wok', 910: 'wooden spoon', 911: 'wool', 912: 'worm fence', 913: 'wreck', 914: 'yawl', 915: 'yurt', 916: 'web site', 917: 'comic book', 918: 'crossword puzzle', 919: 'street sign', 920: 'traffic light', 921: 'book jacket', 922: 'menu', 923: 'plate', 924: 'guacamole', 925: 'consomme', 926: 'hot pot', 927: 'trifle', 928: 'ice cream', 929: 'ice lolly', 930: 'French loaf', 931: 'bagel', 932: 'pretzel', 933: 'cheeseburger', 934: 'hotdog', 935: 'mashed potato', 936: 'head cabbage', 937: 'broccoli', 938: 'cauliflower', 939: 'zucchini', 940: 'spaghetti squash', 941: 'acorn squash', 942: 'butternut squash', 943: 'cucumber', 944: 'artichoke', 945: 'bell pepper', 946: 'cardoon', 947: 'mushroom', 948: 'Granny Smith', 949: 'strawberry', 950: 'orange', 951: 'lemon', 952: 'fig', 953: 'pineapple', 954: 'banana', 955: 'jackfruit', 956: 'custard apple', 957: 'pomegranate', 958: 'hay', 959: 'carbonara', 960: 'chocolate sauce', 961: 'dough', 962: 'meat loaf', 963: 'pizza', 964: 'potpie', 965: 'burrito', 966: 'red wine', 967: 'espresso', 968: 'cup', 969: 'eggnog', 970: 'alp', 971: 'bubble', 972: 'cliff', 973: 'coral reef', 974: 'geyser', 975: 'lakeside', 976: 'promontory', 977: 'sandbar', 978: 'seashore', 979: 'valley', 980: 'volcano', 981: 'ballplayer', 982: 'groom', 983: 'scuba diver', 984: 'rapeseed', 985: 'daisy', 986: \"yellow lady's slipper\", 987: 'corn', 988: 'acorn', 989: 'hip', 990: 'buckeye', 991: 'coral fungus', 992: 'agaric', 993: 'gyromitra', 994: 'stinkhorn', 995: 'earthstar', 996: 'hen-of-the-woods', 997: 'bolete', 998: 'ear', 999: 'toilet tissue'}\n",
        "\n",
        "        self.synset = {\n",
        "    0: 'tench, Tinca tinca',\n",
        "    1: 'goldfish, Carassius auratus',\n",
        "    2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
        "    3: 'tiger shark, Galeocerdo cuvieri',\n",
        "    4: 'hammerhead, hammerhead shark',\n",
        "    5: 'electric ray, crampfish, numbfish, torpedo',\n",
        "    6: 'stingray',\n",
        "    7: 'cock',\n",
        "    8: 'hen',\n",
        "    9: 'ostrich, Struthio camelus',\n",
        "    10: 'brambling, Fringilla montifringilla',\n",
        "    11: 'goldfinch, Carduelis carduelis',\n",
        "    12: 'house finch, linnet, Carpodacus mexicanus',\n",
        "    13: 'junco, snowbird',\n",
        "    14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
        "    15: 'robin, American robin, Turdus migratorius',\n",
        "    16: 'bulbul',\n",
        "    17: 'jay',\n",
        "    18: 'magpie',\n",
        "    19: 'chickadee',\n",
        "    20: 'water ouzel, dipper',\n",
        "    21: 'kite',\n",
        "    22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
        "    23: 'vulture',\n",
        "    24: 'great grey owl, great gray owl, Strix nebulosa',\n",
        "    25: 'European fire salamander, Salamandra salamandra',\n",
        "    26: 'common newt, Triturus vulgaris',\n",
        "    27: 'eft',\n",
        "    28: 'spotted salamander, Ambystoma maculatum',\n",
        "    29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
        "    30: 'bullfrog, Rana catesbeiana',\n",
        "    31: 'tree frog, tree-frog',\n",
        "    32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
        "    33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
        "    34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
        "    35: 'mud turtle',\n",
        "    36: 'terrapin',\n",
        "    37: 'box turtle, box tortoise',\n",
        "    38: 'banded gecko',\n",
        "    39: 'common iguana, iguana, Iguana iguana',\n",
        "    40: 'American chameleon, anole, Anolis carolinensis',\n",
        "    41: 'whiptail, whiptail lizard',\n",
        "    42: 'agama',\n",
        "    43: 'frilled lizard, Chlamydosaurus kingi',\n",
        "    44: 'alligator lizard',\n",
        "    45: 'Gila monster, Heloderma suspectum',\n",
        "    46: 'green lizard, Lacerta viridis',\n",
        "    47: 'African chameleon, Chamaeleo chamaeleon',\n",
        "    48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
        "    49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
        "    50: 'American alligator, Alligator mississipiensis',\n",
        "    51: 'triceratops',\n",
        "    52: 'thunder snake, worm snake, Carphophis amoenus',\n",
        "    53: 'ringneck snake, ring-necked snake, ring snake',\n",
        "    54: 'hognose snake, puff adder, sand viper',\n",
        "    55: 'green snake, grass snake',\n",
        "    56: 'king snake, kingsnake',\n",
        "    57: 'garter snake, grass snake',\n",
        "    58: 'water snake',\n",
        "    59: 'vine snake',\n",
        "    60: 'night snake, Hypsiglena torquata',\n",
        "    61: 'boa constrictor, Constrictor constrictor',\n",
        "    62: 'rock python, rock snake, Python sebae',\n",
        "    63: 'Indian cobra, Naja naja',\n",
        "    64: 'green mamba',\n",
        "    65: 'sea snake',\n",
        "    66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
        "    67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
        "    68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
        "    69: 'trilobite',\n",
        "    70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
        "    71: 'scorpion',\n",
        "    72: 'black and gold garden spider, Argiope aurantia',\n",
        "    73: 'barn spider, Araneus cavaticus',\n",
        "    74: 'garden spider, Aranea diademata',\n",
        "    75: 'black widow, Latrodectus mactans',\n",
        "    76: 'tarantula',\n",
        "    77: 'wolf spider, hunting spider',\n",
        "    78: 'tick',\n",
        "    79: 'centipede',\n",
        "    80: 'black grouse',\n",
        "    81: 'ptarmigan',\n",
        "    82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
        "    83: 'prairie chicken, prairie grouse, prairie fowl',\n",
        "    84: 'peacock',\n",
        "    85: 'quail',\n",
        "    86: 'partridge',\n",
        "    87: 'African grey, African gray, Psittacus erithacus',\n",
        "    88: 'macaw',\n",
        "    89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
        "    90: 'lorikeet',\n",
        "    91: 'coucal',\n",
        "    92: 'bee eater',\n",
        "    93: 'hornbill',\n",
        "    94: 'hummingbird',\n",
        "    95: 'jacamar',\n",
        "    96: 'toucan',\n",
        "    97: 'drake',\n",
        "    98: 'red-breasted merganser, Mergus serrator',\n",
        "    99: 'goose',\n",
        "    100: 'black swan, Cygnus atratus',\n",
        "    101: 'tusker',\n",
        "    102: 'echidna, spiny anteater, anteater',\n",
        "    103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
        "    104: 'wallaby, brush kangaroo',\n",
        "    105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
        "    106: 'wombat',\n",
        "    107: 'jellyfish',\n",
        "    108: 'sea anemone, anemone',\n",
        "    109: 'brain coral',\n",
        "    110: 'flatworm, platyhelminth',\n",
        "    111: 'nematode, nematode worm, roundworm',\n",
        "    112: 'conch',\n",
        "    113: 'snail',\n",
        "    114: 'slug',\n",
        "    115: 'sea slug, nudibranch',\n",
        "    116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
        "    117: 'chambered nautilus, pearly nautilus, nautilus',\n",
        "    118: 'Dungeness crab, Cancer magister',\n",
        "    119: 'rock crab, Cancer irroratus',\n",
        "    120: 'fiddler crab',\n",
        "    121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
        "    122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
        "    123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
        "    124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
        "    125: 'hermit crab',\n",
        "    126: 'isopod',\n",
        "    127: 'white stork, Ciconia ciconia',\n",
        "    128: 'black stork, Ciconia nigra',\n",
        "    129: 'spoonbill',\n",
        "    130: 'flamingo',\n",
        "    131: 'little blue heron, Egretta caerulea',\n",
        "    132: 'American egret, great white heron, Egretta albus',\n",
        "    133: 'bittern',\n",
        "    134: 'crane',\n",
        "    135: 'limpkin, Aramus pictus',\n",
        "    136: 'European gallinule, Porphyrio porphyrio',\n",
        "    137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
        "    138: 'bustard',\n",
        "    139: 'ruddy turnstone, Arenaria interpres',\n",
        "    140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
        "    141: 'redshank, Tringa totanus',\n",
        "    142: 'dowitcher',\n",
        "    143: 'oystercatcher, oyster catcher',\n",
        "    144: 'pelican',\n",
        "    145: 'king penguin, Aptenodytes patagonica',\n",
        "    146: 'albatross, mollymawk',\n",
        "    147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
        "    148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
        "    149: 'dugong, Dugong dugon',\n",
        "    150: 'sea lion',\n",
        "    151: 'Chihuahua',\n",
        "    152: 'Japanese spaniel',\n",
        "    153: 'Maltese dog, Maltese terrier, Maltese',\n",
        "    154: 'Pekinese, Pekingese, Peke',\n",
        "    155: 'Shih-Tzu',\n",
        "    156: 'Blenheim spaniel',\n",
        "    157: 'papillon',\n",
        "    158: 'toy terrier',\n",
        "    159: 'Rhodesian ridgeback',\n",
        "    160: 'Afghan hound, Afghan',\n",
        "    161: 'basset, basset hound',\n",
        "    162: 'beagle',\n",
        "    163: 'bloodhound, sleuthhound',\n",
        "    164: 'bluetick',\n",
        "    165: 'black-and-tan coonhound',\n",
        "    166: 'Walker hound, Walker foxhound',\n",
        "    167: 'English foxhound',\n",
        "    168: 'redbone',\n",
        "    169: 'borzoi, Russian wolfhound',\n",
        "    170: 'Irish wolfhound',\n",
        "    171: 'Italian greyhound',\n",
        "    172: 'whippet',\n",
        "    173: 'Ibizan hound, Ibizan Podenco',\n",
        "    174: 'Norwegian elkhound, elkhound',\n",
        "    175: 'otterhound, otter hound',\n",
        "    176: 'Saluki, gazelle hound',\n",
        "    177: 'Scottish deerhound, deerhound',\n",
        "    178: 'Weimaraner',\n",
        "    179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
        "    180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
        "    181: 'Bedlington terrier',\n",
        "    182: 'Border terrier',\n",
        "    183: 'Kerry blue terrier',\n",
        "    184: 'Irish terrier',\n",
        "    185: 'Norfolk terrier',\n",
        "    186: 'Norwich terrier',\n",
        "    187: 'Yorkshire terrier',\n",
        "    188: 'wire-haired fox terrier',\n",
        "    189: 'Lakeland terrier',\n",
        "    190: 'Sealyham terrier, Sealyham',\n",
        "    191: 'Airedale, Airedale terrier',\n",
        "    192: 'cairn, cairn terrier',\n",
        "    193: 'Australian terrier',\n",
        "    194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
        "    195: 'Boston bull, Boston terrier',\n",
        "    196: 'miniature schnauzer',\n",
        "    197: 'giant schnauzer',\n",
        "    198: 'standard schnauzer',\n",
        "    199: 'Scotch terrier, Scottish terrier, Scottie',\n",
        "    200: 'Tibetan terrier, chrysanthemum dog',\n",
        "    201: 'silky terrier, Sydney silky',\n",
        "    202: 'soft-coated wheaten terrier',\n",
        "    203: 'West Highland white terrier',\n",
        "    204: 'Lhasa, Lhasa apso',\n",
        "    205: 'flat-coated retriever',\n",
        "    206: 'curly-coated retriever',\n",
        "    207: 'golden retriever',\n",
        "    208: 'Labrador retriever',\n",
        "    209: 'Chesapeake Bay retriever',\n",
        "    210: 'German short-haired pointer',\n",
        "    211: 'vizsla, Hungarian pointer',\n",
        "    212: 'English setter',\n",
        "    213: 'Irish setter, red setter',\n",
        "    214: 'Gordon setter',\n",
        "    215: 'Brittany spaniel',\n",
        "    216: 'clumber, clumber spaniel',\n",
        "    217: 'English springer, English springer spaniel',\n",
        "    218: 'Welsh springer spaniel',\n",
        "    219: 'cocker spaniel, English cocker spaniel, cocker',\n",
        "    220: 'Sussex spaniel',\n",
        "    221: 'Irish water spaniel',\n",
        "    222: 'kuvasz',\n",
        "    223: 'schipperke',\n",
        "    224: 'groenendael',\n",
        "    225: 'malinois',\n",
        "    226: 'briard',\n",
        "    227: 'kelpie',\n",
        "    228: 'komondor',\n",
        "    229: 'Old English sheepdog, bobtail',\n",
        "    230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
        "    231: 'collie',\n",
        "    232: 'Border collie',\n",
        "    233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
        "    234: 'Rottweiler',\n",
        "    235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
        "    236: 'Doberman, Doberman pinscher',\n",
        "    237: 'miniature pinscher',\n",
        "    238: 'Greater Swiss Mountain dog',\n",
        "    239: 'Bernese mountain dog',\n",
        "    240: 'Appenzeller',\n",
        "    241: 'EntleBucher',\n",
        "    242: 'boxer',\n",
        "    243: 'bull mastiff',\n",
        "    244: 'Tibetan mastiff',\n",
        "    245: 'French bulldog',\n",
        "    246: 'Great Dane',\n",
        "    247: 'Saint Bernard, St Bernard',\n",
        "    248: 'Eskimo dog, husky',\n",
        "    249: 'malamute, malemute, Alaskan malamute',\n",
        "    250: 'Siberian husky',\n",
        "    251: 'dalmatian, coach dog, carriage dog',\n",
        "    252: 'affenpinscher, monkey pinscher, monkey dog',\n",
        "    253: 'basenji',\n",
        "    254: 'pug, pug-dog',\n",
        "    255: 'Leonberg',\n",
        "    256: 'Newfoundland, Newfoundland dog',\n",
        "    257: 'Great Pyrenees',\n",
        "    258: 'Samoyed, Samoyede',\n",
        "    259: 'Pomeranian',\n",
        "    260: 'chow, chow chow',\n",
        "    261: 'keeshond',\n",
        "    262: 'Brabancon griffon',\n",
        "    263: 'Pembroke, Pembroke Welsh corgi',\n",
        "    264: 'Cardigan, Cardigan Welsh corgi',\n",
        "    265: 'toy poodle',\n",
        "    266: 'miniature poodle',\n",
        "    267: 'standard poodle',\n",
        "    268: 'Mexican hairless',\n",
        "    269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
        "    270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
        "    271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
        "    272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
        "    273: 'dingo, warrigal, warragal, Canis dingo',\n",
        "    274: 'dhole, Cuon alpinus',\n",
        "    275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
        "    276: 'hyena, hyaena',\n",
        "    277: 'red fox, Vulpes vulpes',\n",
        "    278: 'kit fox, Vulpes macrotis',\n",
        "    279: 'Arctic fox, white fox, Alopex lagopus',\n",
        "    280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
        "    281: 'tabby, tabby cat',\n",
        "    282: 'tiger cat',\n",
        "    283: 'Persian cat',\n",
        "    284: 'Siamese cat, Siamese',\n",
        "    285: 'Egyptian cat',\n",
        "    286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
        "    287: 'lynx, catamount',\n",
        "    288: 'leopard, Panthera pardus',\n",
        "    289: 'snow leopard, ounce, Panthera uncia',\n",
        "    290: 'jaguar, panther, Panthera onca, Felis onca',\n",
        "    291: 'lion, king of beasts, Panthera leo',\n",
        "    292: 'tiger, Panthera tigris',\n",
        "    293: 'cheetah, chetah, Acinonyx jubatus',\n",
        "    294: 'brown bear, bruin, Ursus arctos',\n",
        "    295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
        "    296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
        "    297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
        "    298: 'mongoose',\n",
        "    299: 'meerkat, mierkat',\n",
        "    300: 'tiger beetle',\n",
        "    301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
        "    302: 'ground beetle, carabid beetle',\n",
        "    303: 'long-horned beetle, longicorn, longicorn beetle',\n",
        "    304: 'leaf beetle, chrysomelid',\n",
        "    305: 'dung beetle',\n",
        "    306: 'rhinoceros beetle',\n",
        "    307: 'weevil',\n",
        "    308: 'fly',\n",
        "    309: 'bee',\n",
        "    310: 'ant, emmet, pismire',\n",
        "    311: 'grasshopper, hopper',\n",
        "    312: 'cricket',\n",
        "    313: 'walking stick, walkingstick, stick insect',\n",
        "    314: 'cockroach, roach',\n",
        "    315: 'mantis, mantid',\n",
        "    316: 'cicada, cicala',\n",
        "    317: 'leafhopper',\n",
        "    318: 'lacewing, lacewing fly',\n",
        "    319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
        "    320: 'damselfly',\n",
        "    321: 'admiral',\n",
        "    322: 'ringlet, ringlet butterfly',\n",
        "    323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
        "    324: 'cabbage butterfly',\n",
        "    325: 'sulphur butterfly, sulfur butterfly',\n",
        "    326: 'lycaenid, lycaenid butterfly',\n",
        "    327: 'starfish, sea star',\n",
        "    328: 'sea urchin',\n",
        "    329: 'sea cucumber, holothurian',\n",
        "    330: 'wood rabbit, cottontail, cottontail rabbit',\n",
        "    331: 'hare',\n",
        "    332: 'Angora, Angora rabbit',\n",
        "    333: 'hamster',\n",
        "    334: 'porcupine, hedgehog',\n",
        "    335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
        "    336: 'marmot',\n",
        "    337: 'beaver',\n",
        "    338: 'guinea pig, Cavia cobaya',\n",
        "    339: 'sorrel',\n",
        "    340: 'zebra',\n",
        "    341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
        "    342: 'wild boar, boar, Sus scrofa',\n",
        "    343: 'warthog',\n",
        "    344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
        "    345: 'ox',\n",
        "    346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
        "    347: 'bison',\n",
        "    348: 'ram, tup',\n",
        "    349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
        "    350: 'ibex, Capra ibex',\n",
        "    351: 'hartebeest',\n",
        "    352: 'impala, Aepyceros melampus',\n",
        "    353: 'gazelle',\n",
        "    354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
        "    355: 'llama',\n",
        "    356: 'weasel',\n",
        "    357: 'mink',\n",
        "    358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
        "    359: 'black-footed ferret, ferret, Mustela nigripes',\n",
        "    360: 'otter',\n",
        "    361: 'skunk, polecat, wood pussy',\n",
        "    362: 'badger',\n",
        "    363: 'armadillo',\n",
        "    364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
        "    365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
        "    366: 'gorilla, Gorilla gorilla',\n",
        "    367: 'chimpanzee, chimp, Pan troglodytes',\n",
        "    368: 'gibbon, Hylobates lar',\n",
        "    369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
        "    370: 'guenon, guenon monkey',\n",
        "    371: 'patas, hussar monkey, Erythrocebus patas',\n",
        "    372: 'baboon',\n",
        "    373: 'macaque',\n",
        "    374: 'langur',\n",
        "    375: 'colobus, colobus monkey',\n",
        "    376: 'proboscis monkey, Nasalis larvatus',\n",
        "    377: 'marmoset',\n",
        "    378: 'capuchin, ringtail, Cebus capucinus',\n",
        "    379: 'howler monkey, howler',\n",
        "    380: 'titi, titi monkey',\n",
        "    381: 'spider monkey, Ateles geoffroyi',\n",
        "    382: 'squirrel monkey, Saimiri sciureus',\n",
        "    383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
        "    384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
        "    385: 'Indian elephant, Elephas maximus',\n",
        "    386: 'African elephant, Loxodonta africana',\n",
        "    387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
        "    388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
        "    389: 'barracouta, snoek',\n",
        "    390: 'eel',\n",
        "    391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
        "    392: 'rock beauty, Holocanthus tricolor',\n",
        "    393: 'anemone fish',\n",
        "    394: 'sturgeon',\n",
        "    395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
        "    396: 'lionfish',\n",
        "    397: 'puffer, pufferfish, blowfish, globefish',\n",
        "    398: 'abacus',\n",
        "    399: 'abaya',\n",
        "    400: \"academic gown, academic robe, judge's robe\",\n",
        "    401: 'accordion, piano accordion, squeeze box',\n",
        "    402: 'acoustic guitar',\n",
        "    403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
        "    404: 'airliner',\n",
        "    405: 'airship, dirigible',\n",
        "    406: 'altar',\n",
        "    407: 'ambulance',\n",
        "    408: 'amphibian, amphibious vehicle',\n",
        "    409: 'analog clock',\n",
        "    410: 'apiary, bee house',\n",
        "    411: 'apron',\n",
        "    412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
        "    413: 'assault rifle, assault gun',\n",
        "    414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
        "    415: 'bakery, bakeshop, bakehouse',\n",
        "    416: 'balance beam, beam',\n",
        "    417: 'balloon',\n",
        "    418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
        "    419: 'Band Aid',\n",
        "    420: 'banjo',\n",
        "    421: 'bannister, banister, balustrade, balusters, handrail',\n",
        "    422: 'barbell',\n",
        "    423: 'barber chair',\n",
        "    424: 'barbershop',\n",
        "    425: 'barn',\n",
        "    426: 'barometer',\n",
        "    427: 'barrel, cask',\n",
        "    428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
        "    429: 'baseball',\n",
        "    430: 'basketball',\n",
        "    431: 'bassinet',\n",
        "    432: 'bassoon',\n",
        "    433: 'bathing cap, swimming cap',\n",
        "    434: 'bath towel',\n",
        "    435: 'bathtub, bathing tub, bath, tub',\n",
        "    436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
        "    437: 'beacon, lighthouse, beacon light, pharos',\n",
        "    438: 'beaker',\n",
        "    439: 'bearskin, busby, shako',\n",
        "    440: 'beer bottle',\n",
        "    441: 'beer glass',\n",
        "    442: 'bell cote, bell cot',\n",
        "    443: 'bib',\n",
        "    444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
        "    445: 'bikini, two-piece',\n",
        "    446: 'binder, ring-binder',\n",
        "    447: 'binoculars, field glasses, opera glasses',\n",
        "    448: 'birdhouse',\n",
        "    449: 'boathouse',\n",
        "    450: 'bobsled, bobsleigh, bob',\n",
        "    451: 'bolo tie, bolo, bola tie, bola',\n",
        "    452: 'bonnet, poke bonnet',\n",
        "    453: 'bookcase',\n",
        "    454: 'bookshop, bookstore, bookstall',\n",
        "    455: 'bottlecap',\n",
        "    456: 'bow',\n",
        "    457: 'bow tie, bow-tie, bowtie',\n",
        "    458: 'brass, memorial tablet, plaque',\n",
        "    459: 'brassiere, bra, bandeau',\n",
        "    460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
        "    461: 'breastplate, aegis, egis',\n",
        "    462: 'broom',\n",
        "    463: 'bucket, pail',\n",
        "    464: 'buckle',\n",
        "    465: 'bulletproof vest',\n",
        "    466: 'bullet train, bullet',\n",
        "    467: 'butcher shop, meat market',\n",
        "    468: 'cab, hack, taxi, taxicab',\n",
        "    469: 'caldron, cauldron',\n",
        "    470: 'candle, taper, wax light',\n",
        "    471: 'cannon',\n",
        "    472: 'canoe',\n",
        "    473: 'can opener, tin opener',\n",
        "    474: 'cardigan',\n",
        "    475: 'car mirror',\n",
        "    476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
        "    477: \"carpenter's kit, tool kit\",\n",
        "    478: 'carton',\n",
        "    479: 'car wheel',\n",
        "    480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
        "    481: 'cassette',\n",
        "    482: 'cassette player',\n",
        "    483: 'castle',\n",
        "    484: 'catamaran',\n",
        "    485: 'CD player',\n",
        "    486: 'cello, violoncello',\n",
        "    487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
        "    488: 'chain',\n",
        "    489: 'chainlink fence',\n",
        "    490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
        "    491: 'chain saw, chainsaw',\n",
        "    492: 'chest',\n",
        "    493: 'chiffonier, commode',\n",
        "    494: 'chime, bell, gong',\n",
        "    495: 'china cabinet, china closet',\n",
        "    496: 'Christmas stocking',\n",
        "    497: 'church, church building',\n",
        "    498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
        "    499: 'cleaver, meat cleaver, chopper',\n",
        "    500: 'cliff dwelling',\n",
        "    501: 'cloak',\n",
        "    502: 'clog, geta, patten, sabot',\n",
        "    503: 'cocktail shaker',\n",
        "    504: 'coffee mug',\n",
        "    505: 'coffeepot',\n",
        "    506: 'coil, spiral, volute, whorl, helix',\n",
        "    507: 'combination lock',\n",
        "    508: 'computer keyboard, keypad',\n",
        "    509: 'confectionery, confectionary, candy store',\n",
        "    510: 'container ship, containership, container vessel',\n",
        "    511: 'convertible',\n",
        "    512: 'corkscrew, bottle screw',\n",
        "    513: 'cornet, horn, trumpet, trump',\n",
        "    514: 'cowboy boot',\n",
        "    515: 'cowboy hat, ten-gallon hat',\n",
        "    516: 'cradle',\n",
        "    517: 'crane',\n",
        "    518: 'crash helmet',\n",
        "    519: 'crate',\n",
        "    520: 'crib, cot',\n",
        "    521: 'Crock Pot',\n",
        "    522: 'croquet ball',\n",
        "    523: 'crutch',\n",
        "    524: 'cuirass',\n",
        "    525: 'dam, dike, dyke',\n",
        "    526: 'desk',\n",
        "    527: 'desktop computer',\n",
        "    528: 'dial telephone, dial phone',\n",
        "    529: 'diaper, nappy, napkin',\n",
        "    530: 'digital clock',\n",
        "    531: 'digital watch',\n",
        "    532: 'dining table, board',\n",
        "    533: 'dishrag, dishcloth',\n",
        "    534: 'dishwasher, dish washer, dishwashing machine',\n",
        "    535: 'disk brake, disc brake',\n",
        "    536: 'dock, dockage, docking facility',\n",
        "    537: 'dogsled, dog sled, dog sleigh',\n",
        "    538: 'dome',\n",
        "    539: 'doormat, welcome mat',\n",
        "    540: 'drilling platform, offshore rig',\n",
        "    541: 'drum, membranophone, tympan',\n",
        "    542: 'drumstick',\n",
        "    543: 'dumbbell',\n",
        "    544: 'Dutch oven',\n",
        "    545: 'electric fan, blower',\n",
        "    546: 'electric guitar',\n",
        "    547: 'electric locomotive',\n",
        "    548: 'entertainment center',\n",
        "    549: 'envelope',\n",
        "    550: 'espresso maker',\n",
        "    551: 'face powder',\n",
        "    552: 'feather boa, boa',\n",
        "    553: 'file, file cabinet, filing cabinet',\n",
        "    554: 'fireboat',\n",
        "    555: 'fire engine, fire truck',\n",
        "    556: 'fire screen, fireguard',\n",
        "    557: 'flagpole, flagstaff',\n",
        "    558: 'flute, transverse flute',\n",
        "    559: 'folding chair',\n",
        "    560: 'football helmet',\n",
        "    561: 'forklift',\n",
        "    562: 'fountain',\n",
        "    563: 'fountain pen',\n",
        "    564: 'four-poster',\n",
        "    565: 'freight car',\n",
        "    566: 'French horn, horn',\n",
        "    567: 'frying pan, frypan, skillet',\n",
        "    568: 'fur coat',\n",
        "    569: 'garbage truck, dustcart',\n",
        "    570: 'gasmask, respirator, gas helmet',\n",
        "    571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
        "    572: 'goblet',\n",
        "    573: 'go-kart',\n",
        "    574: 'golf ball',\n",
        "    575: 'golfcart, golf cart',\n",
        "    576: 'gondola',\n",
        "    577: 'gong, tam-tam',\n",
        "    578: 'gown',\n",
        "    579: 'grand piano, grand',\n",
        "    580: 'greenhouse, nursery, glasshouse',\n",
        "    581: 'grille, radiator grille',\n",
        "    582: 'grocery store, grocery, food market, market',\n",
        "    583: 'guillotine',\n",
        "    584: 'hair slide',\n",
        "    585: 'hair spray',\n",
        "    586: 'half track',\n",
        "    587: 'hammer',\n",
        "    588: 'hamper',\n",
        "    589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
        "    590: 'hand-held computer, hand-held microcomputer',\n",
        "    591: 'handkerchief, hankie, hanky, hankey',\n",
        "    592: 'hard disc, hard disk, fixed disk',\n",
        "    593: 'harmonica, mouth organ, harp, mouth harp',\n",
        "    594: 'harp',\n",
        "    595: 'harvester, reaper',\n",
        "    596: 'hatchet',\n",
        "    597: 'holster',\n",
        "    598: 'home theater, home theatre',\n",
        "    599: 'honeycomb',\n",
        "    600: 'hook, claw',\n",
        "    601: 'hoopskirt, crinoline',\n",
        "    602: 'horizontal bar, high bar',\n",
        "    603: 'horse cart, horse-cart',\n",
        "    604: 'hourglass',\n",
        "    605: 'iPod',\n",
        "    606: 'iron, smoothing iron',\n",
        "    607: \"jack-o'-lantern\",\n",
        "    608: 'jean, blue jean, denim',\n",
        "    609: 'jeep, landrover',\n",
        "    610: 'jersey, T-shirt, tee shirt',\n",
        "    611: 'jigsaw puzzle',\n",
        "    612: 'jinrikisha, ricksha, rickshaw',\n",
        "    613: 'joystick',\n",
        "    614: 'kimono',\n",
        "    615: 'knee pad',\n",
        "    616: 'knot',\n",
        "    617: 'lab coat, laboratory coat',\n",
        "    618: 'ladle',\n",
        "    619: 'lampshade, lamp shade',\n",
        "    620: 'laptop, laptop computer',\n",
        "    621: 'lawn mower, mower',\n",
        "    622: 'lens cap, lens cover',\n",
        "    623: 'letter opener, paper knife, paperknife',\n",
        "    624: 'library',\n",
        "    625: 'lifeboat',\n",
        "    626: 'lighter, light, igniter, ignitor',\n",
        "    627: 'limousine, limo',\n",
        "    628: 'liner, ocean liner',\n",
        "    629: 'lipstick, lip rouge',\n",
        "    630: 'Loafer',\n",
        "    631: 'lotion',\n",
        "    632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
        "    633: \"loupe, jeweler's loupe\",\n",
        "    634: 'lumbermill, sawmill',\n",
        "    635: 'magnetic compass',\n",
        "    636: 'mailbag, postbag',\n",
        "    637: 'mailbox, letter box',\n",
        "    638: 'maillot',\n",
        "    639: 'maillot, tank suit',\n",
        "    640: 'manhole cover',\n",
        "    641: 'maraca',\n",
        "    642: 'marimba, xylophone',\n",
        "    643: 'mask',\n",
        "    644: 'matchstick',\n",
        "    645: 'maypole',\n",
        "    646: 'maze, labyrinth',\n",
        "    647: 'measuring cup',\n",
        "    648: 'medicine chest, medicine cabinet',\n",
        "    649: 'megalith, megalithic structure',\n",
        "    650: 'microphone, mike',\n",
        "    651: 'microwave, microwave oven',\n",
        "    652: 'military uniform',\n",
        "    653: 'milk can',\n",
        "    654: 'minibus',\n",
        "    655: 'miniskirt, mini',\n",
        "    656: 'minivan',\n",
        "    657: 'missile',\n",
        "    658: 'mitten',\n",
        "    659: 'mixing bowl',\n",
        "    660: 'mobile home, manufactured home',\n",
        "    661: 'Model T',\n",
        "    662: 'modem',\n",
        "    663: 'monastery',\n",
        "    664: 'monitor',\n",
        "    665: 'moped',\n",
        "    666: 'mortar',\n",
        "    667: 'mortarboard',\n",
        "    668: 'mosque',\n",
        "    669: 'mosquito net',\n",
        "    670: 'motor scooter, scooter',\n",
        "    671: 'mountain bike, all-terrain bike, off-roader',\n",
        "    672: 'mountain tent',\n",
        "    673: 'mouse, computer mouse',\n",
        "    674: 'mousetrap',\n",
        "    675: 'moving van',\n",
        "    676: 'muzzle',\n",
        "    677: 'nail',\n",
        "    678: 'neck brace',\n",
        "    679: 'necklace',\n",
        "    680: 'nipple',\n",
        "    681: 'notebook, notebook computer',\n",
        "    682: 'obelisk',\n",
        "    683: 'oboe, hautboy, hautbois',\n",
        "    684: 'ocarina, sweet potato',\n",
        "    685: 'odometer, hodometer, mileometer, milometer',\n",
        "    686: 'oil filter',\n",
        "    687: 'organ, pipe organ',\n",
        "    688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
        "    689: 'overskirt',\n",
        "    690: 'oxcart',\n",
        "    691: 'oxygen mask',\n",
        "    692: 'packet',\n",
        "    693: 'paddle, boat paddle',\n",
        "    694: 'paddlewheel, paddle wheel',\n",
        "    695: 'padlock',\n",
        "    696: 'paintbrush',\n",
        "    697: \"pajama, pyjama, pj's, jammies\",\n",
        "    698: 'palace',\n",
        "    699: 'panpipe, pandean pipe, syrinx',\n",
        "    700: 'paper towel',\n",
        "    701: 'parachute, chute',\n",
        "    702: 'parallel bars, bars',\n",
        "    703: 'park bench',\n",
        "    704: 'parking meter',\n",
        "    705: 'passenger car, coach, carriage',\n",
        "    706: 'patio, terrace',\n",
        "    707: 'pay-phone, pay-station',\n",
        "    708: 'pedestal, plinth, footstall',\n",
        "    709: 'pencil box, pencil case',\n",
        "    710: 'pencil sharpener',\n",
        "    711: 'perfume, essence',\n",
        "    712: 'Petri dish',\n",
        "    713: 'photocopier',\n",
        "    714: 'pick, plectrum, plectron',\n",
        "    715: 'pickelhaube',\n",
        "    716: 'picket fence, paling',\n",
        "    717: 'pickup, pickup truck',\n",
        "    718: 'pier',\n",
        "    719: 'piggy bank, penny bank',\n",
        "    720: 'pill bottle',\n",
        "    721: 'pillow',\n",
        "    722: 'ping-pong ball',\n",
        "    723: 'pinwheel',\n",
        "    724: 'pirate, pirate ship',\n",
        "    725: 'pitcher, ewer',\n",
        "    726: \"plane, carpenter's plane, woodworking plane\",\n",
        "    727: 'planetarium',\n",
        "    728: 'plastic bag',\n",
        "    729: 'plate rack',\n",
        "    730: 'plow, plough',\n",
        "    731: \"plunger, plumber's helper\",\n",
        "    732: 'Polaroid camera, Polaroid Land camera',\n",
        "    733: 'pole',\n",
        "    734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
        "    735: 'poncho',\n",
        "    736: 'pool table, billiard table, snooker table',\n",
        "    737: 'pop bottle, soda bottle',\n",
        "    738: 'pot, flowerpot',\n",
        "    739: \"potter's wheel\",\n",
        "    740: 'power drill',\n",
        "    741: 'prayer rug, prayer mat',\n",
        "    742: 'printer',\n",
        "    743: 'prison, prison house',\n",
        "    744: 'projectile, missile',\n",
        "    745: 'projector',\n",
        "    746: 'puck, hockey puck',\n",
        "    747: 'punching bag, punch bag, punching ball, punchball',\n",
        "    748: 'purse',\n",
        "    749: 'quill, quill pen',\n",
        "    750: 'quilt, comforter, comfort, puff',\n",
        "    751: 'racer, race car, racing car',\n",
        "    752: 'racket, racquet',\n",
        "    753: 'radiator',\n",
        "    754: 'radio, wireless',\n",
        "    755: 'radio telescope, radio reflector',\n",
        "    756: 'rain barrel',\n",
        "    757: 'recreational vehicle, RV, R.V.',\n",
        "    758: 'reel',\n",
        "    759: 'reflex camera',\n",
        "    760: 'refrigerator, icebox',\n",
        "    761: 'remote control, remote',\n",
        "    762: 'restaurant, eating house, eating place, eatery',\n",
        "    763: 'revolver, six-gun, six-shooter',\n",
        "    764: 'rifle',\n",
        "    765: 'rocking chair, rocker',\n",
        "    766: 'rotisserie',\n",
        "    767: 'rubber eraser, rubber, pencil eraser',\n",
        "    768: 'rugby ball',\n",
        "    769: 'rule, ruler',\n",
        "    770: 'running shoe',\n",
        "    771: 'safe',\n",
        "    772: 'safety pin',\n",
        "    773: 'saltshaker, salt shaker',\n",
        "    774: 'sandal',\n",
        "    775: 'sarong',\n",
        "    776: 'sax, saxophone',\n",
        "    777: 'scabbard',\n",
        "    778: 'scale, weighing machine',\n",
        "    779: 'school bus',\n",
        "    780: 'schooner',\n",
        "    781: 'scoreboard',\n",
        "    782: 'screen, CRT screen',\n",
        "    783: 'screw',\n",
        "    784: 'screwdriver',\n",
        "    785: 'seat belt, seatbelt',\n",
        "    786: 'sewing machine',\n",
        "    787: 'shield, buckler',\n",
        "    788: 'shoe shop, shoe-shop, shoe store',\n",
        "    789: 'shoji',\n",
        "    790: 'shopping basket',\n",
        "    791: 'shopping cart',\n",
        "    792: 'shovel',\n",
        "    793: 'shower cap',\n",
        "    794: 'shower curtain',\n",
        "    795: 'ski',\n",
        "    796: 'ski mask',\n",
        "    797: 'sleeping bag',\n",
        "    798: 'slide rule, slipstick',\n",
        "    799: 'sliding door',\n",
        "    800: 'slot, one-armed bandit',\n",
        "    801: 'snorkel',\n",
        "    802: 'snowmobile',\n",
        "    803: 'snowplow, snowplough',\n",
        "    804: 'soap dispenser',\n",
        "    805: 'soccer ball',\n",
        "    806: 'sock',\n",
        "    807: 'solar dish, solar collector, solar furnace',\n",
        "    808: 'sombrero',\n",
        "    809: 'soup bowl',\n",
        "    810: 'space bar',\n",
        "    811: 'space heater',\n",
        "    812: 'space shuttle',\n",
        "    813: 'spatula',\n",
        "    814: 'speedboat',\n",
        "    815: \"spider web, spider's web\",\n",
        "    816: 'spindle',\n",
        "    817: 'sports car, sport car',\n",
        "    818: 'spotlight, spot',\n",
        "    819: 'stage',\n",
        "    820: 'steam locomotive',\n",
        "    821: 'steel arch bridge',\n",
        "    822: 'steel drum',\n",
        "    823: 'stethoscope',\n",
        "    824: 'stole',\n",
        "    825: 'stone wall',\n",
        "    826: 'stopwatch, stop watch',\n",
        "    827: 'stove',\n",
        "    828: 'strainer',\n",
        "    829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
        "    830: 'stretcher',\n",
        "    831: 'studio couch, day bed',\n",
        "    832: 'stupa, tope',\n",
        "    833: 'submarine, pigboat, sub, U-boat',\n",
        "    834: 'suit, suit of clothes',\n",
        "    835: 'sundial',\n",
        "    836: 'sunglass',\n",
        "    837: 'sunglasses, dark glasses, shades',\n",
        "    838: 'sunscreen, sunblock, sun blocker',\n",
        "    839: 'suspension bridge',\n",
        "    840: 'swab, swob, mop',\n",
        "    841: 'sweatshirt',\n",
        "    842: 'swimming trunks, bathing trunks',\n",
        "    843: 'swing',\n",
        "    844: 'switch, electric switch, electrical switch',\n",
        "    845: 'syringe',\n",
        "    846: 'table lamp',\n",
        "    847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
        "    848: 'tape player',\n",
        "    849: 'teapot',\n",
        "    850: 'teddy, teddy bear',\n",
        "    851: 'television, television system',\n",
        "    852: 'tennis ball',\n",
        "    853: 'thatch, thatched roof',\n",
        "    854: 'theater curtain, theatre curtain',\n",
        "    855: 'thimble',\n",
        "    856: 'thresher, thrasher, threshing machine',\n",
        "    857: 'throne',\n",
        "    858: 'tile roof',\n",
        "    859: 'toaster',\n",
        "    860: 'tobacco shop, tobacconist shop, tobacconist',\n",
        "    861: 'toilet seat',\n",
        "    862: 'torch',\n",
        "    863: 'totem pole',\n",
        "    864: 'tow truck, tow car, wrecker',\n",
        "    865: 'toyshop',\n",
        "    866: 'tractor',\n",
        "    867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
        "    868: 'tray',\n",
        "    869: 'trench coat',\n",
        "    870: 'tricycle, trike, velocipede',\n",
        "    871: 'trimaran',\n",
        "    872: 'tripod',\n",
        "    873: 'triumphal arch',\n",
        "    874: 'trolleybus, trolley coach, trackless trolley',\n",
        "    875: 'trombone',\n",
        "    876: 'tub, vat',\n",
        "    877: 'turnstile',\n",
        "    878: 'typewriter keyboard',\n",
        "    879: 'umbrella',\n",
        "    880: 'unicycle, monocycle',\n",
        "    881: 'upright, upright piano',\n",
        "    882: 'vacuum, vacuum cleaner',\n",
        "    883: 'vase',\n",
        "    884: 'vault',\n",
        "    885: 'velvet',\n",
        "    886: 'vending machine',\n",
        "    887: 'vestment',\n",
        "    888: 'viaduct',\n",
        "    889: 'violin, fiddle',\n",
        "    890: 'volleyball',\n",
        "    891: 'waffle iron',\n",
        "    892: 'wall clock',\n",
        "    893: 'wallet, billfold, notecase, pocketbook',\n",
        "    894: 'wardrobe, closet, press',\n",
        "    895: 'warplane, military plane',\n",
        "    896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
        "    897: 'washer, automatic washer, washing machine',\n",
        "    898: 'water bottle',\n",
        "    899: 'water jug',\n",
        "    900: 'water tower',\n",
        "    901: 'whiskey jug',\n",
        "    902: 'whistle',\n",
        "    903: 'wig',\n",
        "    904: 'window screen',\n",
        "    905: 'window shade',\n",
        "    906: 'Windsor tie',\n",
        "    907: 'wine bottle',\n",
        "    908: 'wing',\n",
        "    909: 'wok',\n",
        "    910: 'wooden spoon',\n",
        "    911: 'wool, woolen, woollen',\n",
        "    912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
        "    913: 'wreck',\n",
        "    914: 'yawl',\n",
        "    915: 'yurt',\n",
        "    916: 'web site, website, internet site, site',\n",
        "    917: 'comic book',\n",
        "    918: 'crossword puzzle, crossword',\n",
        "    919: 'street sign',\n",
        "    920: 'traffic light, traffic signal, stoplight',\n",
        "    921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
        "    922: 'menu',\n",
        "    923: 'plate',\n",
        "    924: 'guacamole',\n",
        "    925: 'consomme',\n",
        "    926: 'hot pot, hotpot',\n",
        "    927: 'trifle',\n",
        "    928: 'ice cream, icecream',\n",
        "    929: 'ice lolly, lolly, lollipop, popsicle',\n",
        "    930: 'French loaf',\n",
        "    931: 'bagel, beigel',\n",
        "    932: 'pretzel',\n",
        "    933: 'cheeseburger',\n",
        "    934: 'hotdog, hot dog, red hot',\n",
        "    935: 'mashed potato',\n",
        "    936: 'head cabbage',\n",
        "    937: 'broccoli',\n",
        "    938: 'cauliflower',\n",
        "    939: 'zucchini, courgette',\n",
        "    940: 'spaghetti squash',\n",
        "    941: 'acorn squash',\n",
        "    942: 'butternut squash',\n",
        "    943: 'cucumber, cuke',\n",
        "    944: 'artichoke, globe artichoke',\n",
        "    945: 'bell pepper',\n",
        "    946: 'cardoon',\n",
        "    947: 'mushroom',\n",
        "    948: 'Granny Smith',\n",
        "    949: 'strawberry',\n",
        "    950: 'orange',\n",
        "    951: 'lemon',\n",
        "    952: 'fig',\n",
        "    953: 'pineapple, ananas',\n",
        "    954: 'banana',\n",
        "    955: 'jackfruit, jak, jack',\n",
        "    956: 'custard apple',\n",
        "    957: 'pomegranate',\n",
        "    958: 'hay',\n",
        "    959: 'carbonara',\n",
        "    960: 'chocolate sauce, chocolate syrup',\n",
        "    961: 'dough',\n",
        "    962: 'meat loaf, meatloaf',\n",
        "    963: 'pizza, pizza pie',\n",
        "    964: 'potpie',\n",
        "    965: 'burrito',\n",
        "    966: 'red wine',\n",
        "    967: 'espresso',\n",
        "    968: 'cup',\n",
        "    969: 'eggnog',\n",
        "    970: 'alp',\n",
        "    971: 'bubble',\n",
        "    972: 'cliff, drop, drop-off',\n",
        "    973: 'coral reef',\n",
        "    974: 'geyser',\n",
        "    975: 'lakeside, lakeshore',\n",
        "    976: 'promontory, headland, head, foreland',\n",
        "    977: 'sandbar, sand bar',\n",
        "    978: 'seashore, coast, seacoast, sea-coast',\n",
        "    979: 'valley, vale',\n",
        "    980: 'volcano',\n",
        "    981: 'ballplayer, baseball player',\n",
        "    982: 'groom, bridegroom',\n",
        "    983: 'scuba diver',\n",
        "    984: 'rapeseed',\n",
        "    985: 'daisy',\n",
        "    986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
        "    987: 'corn',\n",
        "    988: 'acorn',\n",
        "    989: 'hip, rose hip, rosehip',\n",
        "    990: 'buckeye, horse chestnut, conker',\n",
        "    991: 'coral fungus',\n",
        "    992: 'agaric',\n",
        "    993: 'gyromitra',\n",
        "    994: 'stinkhorn, carrion fungus',\n",
        "    995: 'earthstar',\n",
        "    996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
        "    997: 'bolete',\n",
        "    998: 'ear, spike, capitulum',\n",
        "    999: 'toilet tissue, toilet paper, bathroom tissue'\n",
        " }\n",
        "\n",
        "py_vars = Py_vars()\n",
        "\n",
        "def int_parameter(level, maxval):\n",
        "    import TPT.data.augmix_ops\n",
        "    return TPT.data.augmix_ops.int_parameter(level, maxval)\n",
        "\n",
        "def float_parameter(level, maxval):\n",
        "    import TPT.data.augmix_ops\n",
        "    return TPT.data.augmix_ops.float_parameter(level, maxval)\n",
        "\n",
        "def sample_level(n):\n",
        "    import TPT.data.augmix_ops\n",
        "    return TPT.data.augmix_ops.sample_level(n)\n",
        "\n",
        "def autocontrast(pil_img, _):\n",
        "    import TPT.data.augmix_ops\n",
        "    return TPT.data.augmix_ops.autocontrast(pil_img, _)\n",
        "\n",
        "def equalize(pil_img, _):\n",
        "    import TPT.data.augmix_ops\n",
        "    return TPT.data.augmix_ops.equalize(pil_img, _)\n",
        "\n",
        "def posterize(pil_img, level):\n",
        "    import TPT.data.augmix_ops\n",
        "    return TPT.data.augmix_ops.posterize(pil_img, level)\n",
        "\n",
        "def rotate(pil_img, level):\n",
        "    import TPT.data.augmix_ops\n",
        "    return TPT.data.augmix_ops.rotate(pil_img, level)\n",
        "\n",
        "def solarize(pil_img, level):\n",
        "    import TPT.data.augmix_ops\n",
        "    return TPT.data.augmix_ops.solarize(pil_img, level)\n",
        "\n",
        "def shear_x(pil_img, level):\n",
        "    import TPT.data.augmix_ops\n",
        "    return TPT.data.augmix_ops.shear_x(pil_img, level)\n",
        "\n",
        "def shear_y(pil_img, level):\n",
        "    import TPT.data.augmix_ops\n",
        "    return TPT.data.augmix_ops.shear_y(pil_img, level)\n",
        "\n",
        "def translate_x(pil_img, level):\n",
        "    import TPT.data.augmix_ops\n",
        "    return TPT.data.augmix_ops.translate_x(pil_img, level)\n",
        "\n",
        "def translate_y(pil_img, level):\n",
        "    import TPT.data.augmix_ops\n",
        "    return TPT.data.augmix_ops.translate_y(pil_img, level)\n",
        "\n",
        "def color(pil_img, level):\n",
        "    import TPT.data.augmix_ops\n",
        "    return TPT.data.augmix_ops.color(pil_img, level)\n",
        "\n",
        "def contrast(pil_img, level):\n",
        "    import TPT.data.augmix_ops\n",
        "    return TPT.data.augmix_ops.contrast(pil_img, level)\n",
        "\n",
        "def brightness(pil_img, level):\n",
        "    import TPT.data.augmix_ops\n",
        "    return TPT.data.augmix_ops.brightness(pil_img, level)\n",
        "\n",
        "def sharpness(pil_img, level):\n",
        "    import TPT.data.augmix_ops\n",
        "    return TPT.data.augmix_ops.sharpness(pil_img, level)\n",
        "\n",
        "def random_crop(img):\n",
        "    from torchvision.transforms import RandomCrop\n",
        "    size = 224\n",
        "    return RandomCrop(size)(img)\n",
        "\n",
        "def random_horizontal_flip(img):\n",
        "    from torchvision.transforms import RandomHorizontalFlip\n",
        "    return RandomHorizontalFlip()(img)\n",
        "\n",
        "def random_resize_crop(img):\n",
        "    from torchvision.transforms import RandomResizedCrop\n",
        "    size = 224\n",
        "    return RandomResizedCrop(size, scale=(0.8,1.0),  )(img)\n",
        "\n",
        "def random_vertical_flip(img):\n",
        "    from torchvision.transforms import RandomVerticalFlip\n",
        "    return RandomVerticalFlip()(img)\n",
        "\n",
        "post_augmentations = [\n",
        "    random_crop, random_horizontal_flip, random_resize_crop\n",
        "]\n",
        "\n",
        "augmentations_basic = [\n",
        "    random_crop, random_horizontal_flip, random_vertical_flip\n",
        "]\n",
        "\n",
        "augmentations_all = [\n",
        "    autocontrast, equalize, posterize, rotate, solarize, shear_x, shear_y,\n",
        "    translate_x, translate_y, color, contrast, brightness, sharpness\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset download and preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt4rvjCchgrm"
      },
      "source": [
        "We download Imagenet-a and ImagenetV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EoshPfM-hgrm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_2851/1980139655.py:23: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(path=output_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download and extraction complete.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "\n",
        "# Define the URL for the ImageNet-A dataset\n",
        "url_a = \"https://people.eecs.berkeley.edu/~hendrycks/imagenet-a.tar\"\n",
        "url_v2 = \"https://huggingface.co/datasets/vaishaal/ImageNetV2/resolve/main/imagenetv2-matched-frequency.tar.gz\"\n",
        "\n",
        "# Define the local filename to save the dataset\n",
        "local_filename_a = \"imagenet-a.tar\"\n",
        "local_filename_v2 = \"imagenetv2-matched-frequency-format-val.tar\"\n",
        "\n",
        "# download a file from a URL\n",
        "def download_file(url, local_filename):\n",
        "    with requests.get(url, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "        with open(local_filename, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "\n",
        "def extract_tar_file(file_name, output_dir='.'):\n",
        "    with tarfile.open(file_name, 'r') as tar:\n",
        "        tar.extractall(path=output_dir)\n",
        "\n",
        "# Download the imagenet-a dataset\n",
        "if not os.path.exists(\"data/imagenet-a\"):\n",
        "    download_file(url_a, local_filename_a)\n",
        "    extract_tar_file(local_filename_a, './data')\n",
        "    os.remove(local_filename_a)\n",
        "\n",
        "\n",
        "# Download the imagenet-v2 dataset\n",
        "if not os.path.exists(\"data/imagenetv2-matched-frequency-format-val\"):\n",
        "    download_file(url_v2, local_filename_v2)\n",
        "    extract_tar_file(local_filename_v2, './data')\n",
        "    os.remove(local_filename_v2)\n",
        "\n",
        "print(\"Download and extraction complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_DkWHkMhgrm"
      },
      "source": [
        "We define the class _Augmixer_ that will be used to augment our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YrtN0cL-hgrn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class Augmixer(object):\n",
        "    def __init__(self, preprocess, n_views=64, augmix=False,\n",
        "                    severity=1):\n",
        "        \"\"\"\n",
        "        :param preprocess: function to preprocess the image (usually CLIP preprocess)\n",
        "        :param n_views: number of views to generate\n",
        "        :param augmix: whether to mix original and augmentation or only apply augmentation\n",
        "        :param severity: severity of the augmentation\n",
        "        \"\"\"\n",
        "        self.preprocess = preprocess\n",
        "        self.n_views = n_views-1\n",
        "        self.aug_list = augmentations_basic\n",
        "        self.post_auglist = post_augmentations\n",
        "        self.severity = severity\n",
        "        self.augmix = augmix\n",
        "\n",
        "    def __call__(self, x):\n",
        "        if self.augmix:\n",
        "            image = self.preprocess(x)\n",
        "            views = [self.augment(x) for _ in range(self.n_views)]\n",
        "\n",
        "        return torch.stack([image] + views, 0)\n",
        "\n",
        "    def augment(self, x):\n",
        "        if self.augmix:\n",
        "            img = _augmix(x, self.preprocess, self.aug_list, self.severity)\n",
        "        else:\n",
        "            img = self.preprocess(x)\n",
        "        for _ in range(random.randint(1,2)):\n",
        "            img = random.choice(self.post_auglist)(img)\n",
        "\n",
        "        return img\n",
        "\n",
        "\n",
        "def get_preaugment():\n",
        "    return transforms.Compose([\n",
        "            transforms.RandomResizedCrop(224)\n",
        "        ])\n",
        "\n",
        "def _augmix(image, preprocess, aug_list, severity=1):\n",
        "    preaugment = get_preaugment()\n",
        "    x_orig = preaugment(image)\n",
        "    x_processed = preprocess(x_orig)\n",
        "    if len(aug_list) == 0:\n",
        "        return x_processed\n",
        "    w = np.float32(np.random.dirichlet([1.0, 1.0, 1.0]))\n",
        "    m = np.float32(np.random.beta(1.0, 1.0))\n",
        "\n",
        "    mix = torch.zeros_like(x_processed)\n",
        "    for i in range(3):\n",
        "        x_aug = x_orig.copy()\n",
        "        for _ in range(np.random.randint(1, 4)):\n",
        "            x_aug = np.random.choice(aug_list)(x_aug)\n",
        "            # x_aug = np.random.choice(aug_list)(x_aug, severity)\n",
        "        mix += w[i] * preprocess(x_aug)\n",
        "    mix = m * x_processed + (1 - m) * mix\n",
        "    return mix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdeeJdy1hgrn"
      },
      "source": [
        "We extend the dataset _ImageFolder_ class to handle the case of augmentation (to avoid having an extra dimension of size (1) in the tensor).\n",
        "Custom Sampler was used to have replicable results when testing only on a subset of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "N4ePCbAihgrn"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "\n",
        "def my_collate(batch):\n",
        "    # Unpack the batch\n",
        "    images, label, path = zip(*batch)\n",
        "\n",
        "    # Remove the extra dimension and stack the images and labels\n",
        "    images = torch.stack([img.squeeze(0) for img in images]).squeeze(0)\n",
        "    labels = torch.tensor(label[0]).unsqueeze(0)\n",
        "\n",
        "    return images, labels, path\n",
        "\n",
        "\n",
        "class AugmixFolder(datasets.ImageFolder):\n",
        "    def __init__(self, root,transform):\n",
        "        super(AugmixFolder, self).__init__(root, transform=transform)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, label = super(AugmixFolder, self).__getitem__(index)\n",
        "        path = self.imgs[index][0]\n",
        "        if isinstance(self.transform, Augmixer):\n",
        "            return img.squeeze(0), label, path\n",
        "        return img, label, path\n",
        "\n",
        "class CustomSampler(torch.utils.data.Sampler):\n",
        "    def __init__(self, indices, from_idx: int = 0):\n",
        "        self.indices = indices[from_idx:]\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pSCzy9Ahgrn"
      },
      "source": [
        "We define the function _get\\_data_ to get the datasets and split them if requires and get the dictionary of class mappings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rl6XOGdkhgrn"
      },
      "outputs": [],
      "source": [
        "from torch.utils import data\n",
        "from torch.utils.data import random_split\n",
        "import shutil\n",
        "\n",
        "def get_data(dataset_name, batch_size, transform, shuffle=True, train_size=0.8, val_size=0.1, from_idx=0):\n",
        "    \"\"\"\n",
        "    Loads the dataset and splits it into training, validation and test sets. Available datsets:\n",
        "    [\"cifar10\", \"cifar100\", \"imagenet_v2\", \"imagenet_a\"]\n",
        "    :param dataset_name: str: name of the dataset\n",
        "    :param batch_size: int: batch size\n",
        "    :param transform: function: preprocessing function\n",
        "    :param shuffle: bool: shuffle the dataset\n",
        "    :param train_size: float: proportion of the dataset to include in the training set\n",
        "    :param val_size: float: proportion of the dataset to include in the validation set\n",
        "    :return: tuple: training, validation and test dataloaders\n",
        "    \"\"\"\n",
        "    if dataset_name == \"cifar10\":\n",
        "        download = not (os.path.exists(os.path.join(\"data/cifar-10-python\")))\n",
        "        dataset = AugmixFolder(root=\"./data\", download=download, transform=transform)\n",
        "        id2class = {dataset.class_to_idx[c] : c for c in dataset.classes}\n",
        "    elif dataset_name == \"cifar100\":\n",
        "        download = not (os.path.exists(os.path.join(\"data/cifar-100-python\")))\n",
        "        dataset = AugmixFolder(root=\"./data\", download=download, transform=transform)\n",
        "        id2class = {dataset.class_to_idx[c] : c for c in dataset.classes}\n",
        "    elif dataset_name == \"imagenet_v2\":\n",
        "        root = \"./data/imagenetv2-matched-frequency-format-val\"\n",
        "        filtered_root = \"./data/imagenetv2-random-partition\"\n",
        "        if not os.path.exists(filtered_root):\n",
        "            os.makedirs(filtered_root, exist_ok=True)\n",
        "            all_subfolders = [d.name for d in os.scandir(root) if d.is_dir()]\n",
        "            selected_subfolders = random.sample(all_subfolders, 500)\n",
        "\n",
        "            for folder in selected_subfolders:\n",
        "                src_folder = os.path.join(root, folder)\n",
        "                dest_folder = os.path.join(filtered_root, folder)\n",
        "                if not os.path.exists(dest_folder):\n",
        "                    shutil.copytree(src_folder, dest_folder, dirs_exist_ok=True) \n",
        "        else:\n",
        "            print(\"Using existing partition of ImagenetV2, ensure you're using a checkpoint\")\n",
        "\n",
        "        dataset = AugmixFolder(root=filtered_root, transform=transform)\n",
        "        dataset.class_to_idx = {cls: i for i, cls in enumerate(dataset.classes)}\n",
        "        id2class = {dataset.class_to_idx[c] : py_vars.num2class_v2[int(c)] for c in dataset.classes}\n",
        "    elif dataset_name == \"imagenet_a\":\n",
        "        dataset = AugmixFolder(root=\"./data/imagenet-a\", transform=transform)\n",
        "        dataset.class_to_idx = {cls: i for i, cls in enumerate(dataset.classes)}\n",
        "        id2class = {dataset.class_to_idx[c] : py_vars.num2class[c] for c in dataset.classes}\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset {dataset_name}\")\n",
        "    \n",
        "    n = len(dataset)\n",
        "    n_train = int(train_size * n)\n",
        "    n_val = int(val_size * n)\n",
        "    n_test = n - n_train - n_val\n",
        "    \n",
        "    if(n_train + n_val == 0):\n",
        "        train_loader, val_loader = None, None\n",
        "        if batch_size == 1:\n",
        "            test_loader = data.DataLoader(dataset, batch_size=batch_size, \n",
        "                                          sampler=CustomSampler(range(n), from_idx=from_idx), collate_fn=my_collate)\n",
        "        else:\n",
        "            test_loader = data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                          sampler=CustomSampler(range(n), from_idx=from_idx))\n",
        "    else:\n",
        "        train_dataset, val_dataset, test_dataset = random_split(dataset, [n_train, n_val, n_test])\n",
        "\n",
        "        train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=my_collate)\n",
        "        val_loader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=my_collate)\n",
        "        test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=my_collate)\n",
        "\n",
        "    return train_loader, val_loader, test_loader, list(id2class.values()), id2class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkkzaqxVhgrn"
      },
      "source": [
        "# 2. CLIP\n",
        "<div style=\"display: flex;\">\n",
        "    <div>\n",
        "        <p>\n",
        "        CLIP is a neural network which efficiently learns visual concepts from natural language supervision. CLIP can be applied to any visual classification benchmark by simply providing the names of the visual categories to be recognized, similar to the “zero-shot” capabilities of GPT-2 and GPT-3.\n",
        "        It is trained on a wide variety of images with a wide variety of natural language supervision that’s abundantly available on the internet. By design, the network can be instructed in natural language to perform a great variety of classification benchmarks, without directly optimizing for the benchmark’s performance.\n",
        "        </p>\n",
        "        <p>\n",
        "        Although both models have the same accuracy on the ImageNet test set, CLIP’s performance is much more representative of how it will fare on datasets that measure accuracy in different, non-ImageNet settings. For instance, ObjectNet checks a model’s ability to recognize objects in many different poses and with many different backgrounds inside homes while ImageNet Rendition and ImageNet Sketch check a model’s ability to recognize more abstract depictions of objects.\n",
        "        </p>\n",
        "        <p>\n",
        "        Source: <a href=\"https://openai.com/index/clip/\">CLIP-OpenAI</a>\n",
        "        </p>\n",
        "    </div>\n",
        "    <div>\n",
        "        <img src='https://github.com/rogergheser/DS-DLProject/blob/main/source/images/CLIP_results.png?raw=1' width=100%>\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "### 2.1. Data Collection\n",
        "A new dataset, WIT (WebImageText), was created from 400 million (image, text) pairs sourced from the internet, designed to cover a broad range of visual concepts.\n",
        "\n",
        "\n",
        "\n",
        "### 2.2. Pre-Training Method\n",
        "CLIP uses a contrastive objective to learn representations. Referring to the section 1 of figure 3, the loss function is:\n",
        "\n",
        "$ L_{t2i} = -\\frac{1}{N} \\sum \\limits _{i=1} ^{N} \\frac{exp(I_{i} \\cdot T_{i}/τ)}{\\sum \\limits _{k=1} ^{N} exp(I_{k} \\cdot T_{i}/τ)} $\n",
        "\n",
        "$ L_{i2t} = -\\frac{1}{N} \\sum \\limits _{i=1} ^{N} \\frac{exp(I_{i} \\cdot T_{i}/τ)}{\\sum \\limits _{k=1} ^{N} exp(I_{i} \\cdot T_{k}/τ)} $\n",
        "\n",
        "$ L = E_{(I_{i}, T_{i}) _{i=1} ^{N}\\sim Data}[L_{t2i} + L_{i2t}] $\n",
        "\n",
        "Unlike previous methods that predict the exact text of captions, CLIP only needs to identify which text matches which image. This method is more scalable and efficient.\n",
        "\n",
        "<img src='https://github.com/rogergheser/DS-DLProject/blob/main/source/images/clip-pretraining.svg?raw=1' width=50%>\n",
        "\n",
        "Figure 1. Summary of CLIP approach. While standard image models jointly train an image feature extractor and a linear classifier to predict\n",
        "some label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training\n",
        "examples. At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the\n",
        "target dataset’s classes.\n",
        "\n",
        "\n",
        "### 2.3. Model Architecture\n",
        "\n",
        "CLIP jointly trains an image encoder and a text encoder. The encoders are optimized to maximize the cosine similarity between the correct image-text pairs in a batch. At test time, the text encoder can synthesize a zero-shot classifier by embedding the names or descriptions of target dataset classes.\n",
        "\n",
        "### 2.4. Experiments and Results\n",
        "\n",
        "CLIP's zero-shot transfer performance was evaluated on over 30 datasets, including tasks such as OCR, action recognition, and fine-grained object classification. CLIP often performed competitively with fully supervised models. Notably, CLIP achieved the same accuracy as ResNet-50 on ImageNet without using any training examples from ImageNet. The study also demonstrates that CLIP is more computationally efficient and robust compared to traditional supervised models.\n",
        "\n",
        "<img src=\"https://github.com/rogergheser/DS-DLProject/blob/main/source/images/clip-inference.svg?raw=1\" width=50%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0HOgWxehgrn"
      },
      "source": [
        "# 3. Context Optimization (CoOp)\n",
        "\n",
        "Context Optimization (CoOp) is a method designed to enhance vision-language models, such as CLIP, by optimizing textual context in a data-driven manner. This approach circumvents the need for manual prompt tuning by modeling context words with continuous vectors that are learned end-to-end from the data.\n",
        "\n",
        "<img src='https://github.com/rogergheser/DS-DLProject/blob/main/source/images/overview_coop.png?raw=1' width=70%>\n",
        "\n",
        "### 3.1. Key Features\n",
        "\n",
        "#### 3.1.1 Unified Context:\n",
        "\n",
        "CoOp implements two approaches one with the use of a unified context for all classes, sharing the same context tokens across different classes, or a class-specific context, where context vectors are independent to each class. In our project we used the former, **unified context**. The prompt given to the text encoder is represented as:\n",
        "\n",
        "$ t = [V]_1[V]_2...[V]_M[CLASS]$\n",
        "\n",
        "Here, $ [V]_m $ are learnable vectors representing context tokens, $ M $ is a hyperparameter specifying the number of context tokens and  $ [CLASS] $ is a placeholder for the class name.\n",
        "The context tokens ( $ [V]_m $ ) are continuous vectors with the same dimension as word embeddings (e.g., 512 for CLIP), allowing them to be optimized during training.\n",
        "\n",
        "\n",
        "#### 3.1.2. End-to-End Learning:\n",
        "\n",
        "CoOp optimizes these context tokens in an end-to-end manner, leveraging the model's pre-trained knowledge without modifying the pre-trained parameters.\n",
        "\n",
        "#### 3.1.3. Prediction Probability:\n",
        "\n",
        "The prediction probability for an image $ x $ belonging to class $ i $ is computed as:\n",
        "\n",
        "$ p(y=i|\\mathbf{x}) = \\frac{exp(cos(\\mathbf{w_i},\\mathbf{f}/τ))}{\\sum \\limits _{j=1} ^{K}exp(cos(\\mathbf{w_j},\\mathbf{f}/τ))} $\n",
        "\n",
        "where $ τ $ is a temperature parameter, $ cos(⋅,⋅) $ denotes cosine similarity, $ f $ represents the image features and $ w_i $ are the weight vectors generated by the text encoder.\n",
        "\n",
        "#### 3.1.4. Optimization Objective:\n",
        "\n",
        "The objective of CoOp is to minimize the cross-entropy loss ( $ L = -\\Sigma_{x,y \\in D} log(p(x|y)) $ ) over the training data, updating the context tokens to improve performance\n",
        "\n",
        "### 3.2. Performance and Results\n",
        "The authors of CoOp present the following table of results.\n",
        "\n",
        "As the table shows the overall performance of models using CoOp increases\n",
        "\n",
        "<img src='https://github.com/rogergheser/DS-DLProject/blob/main/source/images/coop_comparisons_table.png?raw=1' width=50%>\n",
        "\n",
        "#### 3.2.1. Few-Shot Learning:\n",
        "\n",
        "CoOp significantly improves few-shot learning performance. For instance, with ResNet-50, CoOp achieves 63.33% accuracy on ImageNet with 4 context tokens, compared to 58.18% for zero-shot CLIP.\n",
        "\n",
        "#### 3.2.2. Domain Generalization:\n",
        "\n",
        "CoOp demonstrates robust performance across various datasets, showing resilience to distribution shifts. On the DOSCO-2k benchmark, CoOp outperforms zero-shot CLIP in most cases, indicating its robustness and adaptability.\n",
        "\n",
        "#### 3.2.3. Broad Applicability:\n",
        "\n",
        "CoOp performs well in a variety of tasks, including generic object recognition (ImageNet, Caltech101), fine-grained recognition (aircraft models, textures), and action recognition (UCF101).\n",
        "\n",
        "### 3.3. Conclusion\n",
        "CoOp represents a significant advancement in enhancing vision-language models' performance, particularly in few-shot learning scenarios. By optimizing context tokens in a data-driven manner, CoOp leverages pre-trained knowledge effectively, achieving state-of-the-art results without additional manual tuning. This approach is highly adaptable, showing robust performance across different datasets and tasks, making it a powerful tool for vision-language integration in diverse applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI-ZKrKUhgro"
      },
      "source": [
        "![Overview CoOp](source/images/overview_coop.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwNCQo6Yhgro"
      },
      "source": [
        "Figure 2: Overview of Context Optimization (CoOp). The main idea is to model a prompt’s context using a set of learnable\n",
        "vectors, which can be optimized through minimizing the classification loss. Two designs are proposed: one is unified context,\n",
        "which shares the same context vectors with all classes; and the other is class-specific context, which learns for each class a\n",
        "specific set of context vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHGgYZAuhgrp"
      },
      "source": [
        "### CLIP + CoOp implementation\n",
        "\n",
        "Prompt Learner credits to Francesco Tonini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FoVILVolhgrp"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class PromptLearner(nn.Module):\n",
        "    def __init__(self, clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
        "        super().__init__()\n",
        "        n_cls = len(classnames)\n",
        "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
        "        clip_imsize = clip_model.visual.input_resolution\n",
        "        _tokenizer = _Tokenizer()\n",
        "\n",
        "        # Use given words to initialize context vectors\n",
        "        if ctx_init:\n",
        "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
        "            n_ctx = len(ctx_init.split(\" \"))\n",
        "            prompt = clip.tokenize(ctx_init).to(clip_model.token_embedding.weight.device)\n",
        "            with torch.no_grad():\n",
        "                embedding = clip_model.token_embedding(prompt)\n",
        "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
        "            prompt_prefix = ctx_init\n",
        "        else:\n",
        "            if csc:\n",
        "                print(\"Initializing class-specific contexts\")\n",
        "                ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim)\n",
        "            else:\n",
        "                print(\"Initializing a generic context\")\n",
        "                ctx_vectors = torch.empty(n_ctx, ctx_dim)\n",
        "\n",
        "            torch.nn.init.normal_(ctx_vectors, std=0.02)\n",
        "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
        "\n",
        "        print(f\"Initial context: '{prompt_prefix}'\")\n",
        "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
        "\n",
        "        self.ctx_init_state = ctx_vectors.detach().clone()\n",
        "        # These are the `prompts` we want to optimize\n",
        "        self.ctx = nn.Parameter(ctx_vectors)\n",
        "\n",
        "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
        "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
        "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
        "\n",
        "        # print(\"+++\")\n",
        "        # print(\"Prompts:\")\n",
        "        # for p in prompts:\n",
        "        #     print(p)\n",
        "        # print(\"+++\")\n",
        "\n",
        "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(clip_model.token_embedding.weight.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding = clip_model.token_embedding(tokenized_prompts)\n",
        "\n",
        "        # These token vectors will be saved when in save_model(),\n",
        "        # but they should be ignored in load_model() as we want to use\n",
        "        # those computed using the current class names\n",
        "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
        "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])  # CLS, EOS\n",
        "\n",
        "        self.n_cls = n_cls\n",
        "        self.n_ctx = n_ctx\n",
        "        self.tokenized_prompts = tokenized_prompts\n",
        "        self.name_lens = name_lens\n",
        "        self.class_token_position = class_token_position\n",
        "\n",
        "    def forward(self):\n",
        "        prefix = self.token_prefix\n",
        "        suffix = self.token_suffix\n",
        "        ctx = self.ctx\n",
        "\n",
        "        # If CoOp, expand the ctx for all classes\n",
        "        if ctx.dim() == 2:\n",
        "            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
        "\n",
        "        if self.class_token_position == \"end\":\n",
        "            prompts = torch.cat(\n",
        "                [\n",
        "                    prefix,  # (n_cls, 1, dim)\n",
        "                    ctx,     # (n_cls, n_ctx, dim)\n",
        "                    suffix,  # (n_cls, *, dim)\n",
        "                ],\n",
        "                dim=1,\n",
        "            )\n",
        "\n",
        "        elif self.class_token_position == \"middle\":\n",
        "            half_n_ctx = self.n_ctx // 2\n",
        "            prompts = []\n",
        "            for i in range(self.n_cls):\n",
        "                name_len = self.name_lens[i]\n",
        "                prefix_i = prefix[i : i + 1, :, :]\n",
        "                class_i = suffix[i : i + 1, :name_len, :]\n",
        "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
        "                ctx_i_half1 = ctx[i : i + 1, :half_n_ctx, :]\n",
        "                ctx_i_half2 = ctx[i : i + 1, half_n_ctx:, :]\n",
        "                prompt = torch.cat(\n",
        "                    [\n",
        "                        prefix_i,     # (1, 1, dim)\n",
        "                        ctx_i_half1,  # (1, n_ctx//2, dim)\n",
        "                        class_i,      # (1, name_len, dim)\n",
        "                        ctx_i_half2,  # (1, n_ctx//2, dim)\n",
        "                        suffix_i,     # (1, *, dim)\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "                prompts.append(prompt)\n",
        "            prompts = torch.cat(prompts, dim=0)\n",
        "\n",
        "        elif self.class_token_position == \"front\":\n",
        "            prompts = []\n",
        "            for i in range(self.n_cls):\n",
        "                name_len = self.name_lens[i]\n",
        "                prefix_i = prefix[i : i + 1, :, :]\n",
        "                class_i = suffix[i : i + 1, :name_len, :]\n",
        "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
        "                ctx_i = ctx[i : i + 1, :, :]\n",
        "                prompt = torch.cat(\n",
        "                    [\n",
        "                        prefix_i,  # (1, 1, dim)\n",
        "                        class_i,   # (1, name_len, dim)\n",
        "                        ctx_i,     # (1, n_ctx, dim)\n",
        "                        suffix_i,  # (1, *, dim)\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "                prompts.append(prompt)\n",
        "            prompts = torch.cat(prompts, dim=0)\n",
        "\n",
        "        else:\n",
        "            raise ValueError\n",
        "\n",
        "        return prompts\n",
        "\n",
        "    def reset(self):\n",
        "        ctx_vectors = self.ctx_init_state\n",
        "        with torch.no_grad():\n",
        "            self.ctx.copy_(ctx_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6nIotFyGhgrp"
      },
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, clip_model):\n",
        "        super().__init__()\n",
        "        self.transformer = clip_model.transformer\n",
        "        self.positional_embedding = clip_model.positional_embedding\n",
        "        self.ln_final = clip_model.ln_final\n",
        "        self.text_projection = clip_model.text_projection\n",
        "\n",
        "    def forward(self, prompts, tokenized_prompts):\n",
        "        x = prompts + self.positional_embedding\n",
        "        x = x.permute(1, 0, 2)  # [batch_size, n_ctx, transformer.width] -> [n_ctx, batch_size, transformer.width]\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)  # [n_ctx, batch_size, transformer.width] -> [batch_size, n_ctx, transformer.width]\n",
        "        x = self.ln_final(x)\n",
        "\n",
        "        # Take features from the eot embedding (eot_token is the highest number in each sequence)\n",
        "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vs5yYtROhgrq"
      },
      "outputs": [],
      "source": [
        "class OurCLIP(nn.Module):\n",
        "\n",
        "    def __init__(self, classnames, n_ctx, ctx_init, class_token_position, backbone=\"RN50\", csc=False):\n",
        "        super().__init__()\n",
        "        clip_model, _ = clip.load(backbone)\n",
        "        clip_model = clip_model.float()\n",
        "\n",
        "        self.prompt_learner = PromptLearner(clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=csc)\n",
        "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
        "        self.encode_text = clip_model.encode_text\n",
        "        self.image_encoder = clip_model.visual\n",
        "        self.text_encoder = TextEncoder(clip_model)\n",
        "        self.logit_scale = clip_model.logit_scale\n",
        "\n",
        "    def forward(self, image):\n",
        "        image_features = self.image_encoder(image)\n",
        "\n",
        "        prompts = self.prompt_learner()\n",
        "        tokenized_prompts = self.tokenized_prompts\n",
        "        text_features = self.text_encoder(prompts, tokenized_prompts)\n",
        "\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logits = logit_scale * image_features @ text_features.t()\n",
        "\n",
        "        return logits, text_features\n",
        "\n",
        "    def reset(self):\n",
        "        self.prompt_learner.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1-mYMtZxlEtL"
      },
      "outputs": [],
      "source": [
        "def load_pretrained_coop(backbone, _model, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Loads coop pretrained context\n",
        "    \"\"\"\n",
        "    # TODO Makes this function cleaner and more robust\n",
        "    if backbone.lower() == \"rn50\":\n",
        "        _backbone = \"rn50\"\n",
        "    elif backbone.lower() == \"rn101\":\n",
        "        _backbone = \"rn101\"\n",
        "    elif backbone.lower() == \"vit_b16\" or backbone.lower() == \"vit-b/16\":\n",
        "        _backbone = \"vit_b16\"\n",
        "    elif backbone.lower() == \"vit_b32\" or backbone.lower() == \"vit-b/32\":\n",
        "        _backbone = \"vit_b32\"\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown backbone {backbone}\")\n",
        "\n",
        "    path = f\"bin/coop/{_backbone}_ep50_16shots/nctx4_cscFalse_ctpend/seed1/prompt_learner/model.pth.tar-50\"\n",
        "    assert os.path.exists(path), f\"Path {path} does not exist\"\n",
        "\n",
        "    pretrained_ctx = torch.load(path, device)['state_dict']['ctx']\n",
        "    assert pretrained_ctx.size()[0] == _model.prompt_learner.n_ctx, f\"Number of context tokens mismatch: {_model.prompt_learner.n_ctx} vs {pretrained_ctx.size()[0]}\"\n",
        "    with torch.no_grad():\n",
        "        _model.prompt_learner.ctx.copy_(pretrained_ctx)\n",
        "        _model.prompt_learner.ctx_init_state = pretrained_ctx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-78mV5nhgrq"
      },
      "source": [
        "# 4. CoCa (Contrastive Captioner)\n",
        "\n",
        "CoCa focuses on pretraining image-text foundation models for computer vision tasks. It is a minimalist model that merges the capabilities of contrastive approaches (like CLIP) and generative methods (like SimVLM). CoCa's design includes an image-text encoder-decoder framework optimized with both contrastive loss and captioning loss, facilitating efficient computation and high performance in various tasks.\n",
        "\n",
        "### 4.1 Introduction\n",
        "\n",
        "The goal of CoCa is to unify different paradigms, such as single-encoder models focusing on image classification, dual-encoder models leveraging contrastive loss for image-text pairs, and encoder-decoder models for generative pretraining, into a single model that can efficiently handle both vision and vision-language tasks.\n",
        "<img src='https://github.com/rogergheser/DS-DLProject/blob/main/source/images/coca_fig1.png?raw=1' width=80%>\n",
        "\n",
        "Figure 4: Overview of Contrastive Captioners (CoCa) pretraining as image-text foundation models.\n",
        "The pretrained CoCa can be used for downstream tasks including visual recognition, vision-language\n",
        "alignment, image captioning and multimodal understanding with zero-shot transfer, frozen-feature\n",
        "evaluation or end-to-end finetuning\n",
        "### 4.2 Model Architecture and Objectives\n",
        "\n",
        "CoCa's architecture includes:\n",
        "\n",
        "- Image Encoder: Transforms images into latent representations.\n",
        "- Text Decoder: Split into two parts—unimodal and multimodal. The unimodal decoder layers process text independently, while the multimodal layers integrate image features through cross-attention.\n",
        "- Contrastive Loss: Applied between unimodal image and text embeddings to align their representations.\n",
        "- Captioning Loss: Applied to the outputs of the multimodal decoder to predict text tokens autoregressively.\n",
        "<img src='https://github.com/rogergheser/DS-DLProject/blob/main/source/images/coca_fig2.png?raw=1' width=50%>\n",
        "Figure 5: Detailed illustration of CoCa architecture. A simple encoder-decoder approach that seamlessly combines the three training paradigms\n",
        "\n",
        "Similar to standard image-text encoderdecoder models, CoCa encodes images to latent representations by a neural network encoder, for\n",
        "example, vision transformer (ViT), and decodes texts with a causal masking transformer decoder. Unlike standard decoder transformers, CoCa omits cross-attention in the first half of the decoder layers to encode unimodal text representations, and cascades the rest of the decoder layers, cross-attending to the image encoder for multimodal image-text representations. As a result, the CoCa decoder simultaneously produces both unimodal and multimodal text representations that allow us to apply both contrastive and generative objectives:\n",
        "\n",
        "$ L_{CoCa} = λ_{Con} · L_{Con} + λ_{Cap} · L_{Cap} $\n",
        "\n",
        "where $ λ_{Con} $ and $ λ_{Cap} $ are loss weighting hyper-parameters, $ L_{Con} $ is the Dual-Encoder Contrastive Learning loss and $ L_{Cap} $ is the Encoder-Decoder Captioning loss.\n",
        "### 4.3. Pretraining Approach\n",
        "\n",
        "CoCa is pretrained on both web-scale alt-text data and annotated images, treating all labels as text. This unified approach leverages natural language supervision for comprehensive representation learning.\n",
        "\n",
        "### 4.4. Performance and Results\n",
        "CoCa demonstrates state-of-the-art performance across a range of tasks, including:\n",
        "\n",
        "- Visual Recognition: Achieves 86.3% zero-shot top-1 accuracy on ImageNet and 91.0% with fine-tuning.\n",
        "- Crossmodal Retrieval: Excels in tasks like MSCOCO and Flickr30K retrieval.\n",
        "- Multimodal Understanding: Shows strong results in VQA, SNLI-VE, and NLVR2.\n",
        "- Image Captioning: Performs well on datasets like MSCOCO and NoCaps.\n",
        "\n",
        "### 4.5. Methodology\n",
        "CoCa's training involves a single stage that integrates contrastive learning and captioning. The model's architecture enables efficient computation and effective learning of both unimodal and multimodal representations.\n",
        "\n",
        "### 4.6. Conclusion\n",
        "\n",
        "CoCa sets a new benchmark in image-text pretraining by combining contrastive and generative objectives in a streamlined architecture. Its versatility and high performance across various tasks highlight the potential of unified pretraining models in advancing computer vision and vision-language understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ez-IJeYhgrq"
      },
      "source": [
        "The following is the definition of a helper Captioner class that is used to generate captions for the filtered images. The class is initialized with the CoCa model. The class has a method _generate_caption_ that takes as input the filtered images and returns the captions generated by the CoCa model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ox6K38knhgrq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import open_clip\n",
        "from typing import List\n",
        "\n",
        "class Captioner():\n",
        "    def __init__(self, model_name, version, device):\n",
        "        self.caption_model, _ , self.transform = open_clip.create_model_and_transforms(\n",
        "            model_name=model_name,\n",
        "            pretrained=version,\n",
        "            cache_dir='./.dl-cache'\n",
        "            )\n",
        "        self.caption_model.to(device)\n",
        "        self.tokenizer = open_clip.get_tokenizer(model_name)\n",
        "        self.device = device\n",
        "\n",
        "    def _tokenize(self, x: str) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Tokenizes the input text using the tokenizer.\n",
        "\n",
        "        Args:\n",
        "            x (str): The input text.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The tokenized text.\n",
        "        \"\"\"\n",
        "        x_tokenized = self.tokenizer(x).squeeze()\n",
        "        start_token = 49406\n",
        "        end_token = 49407\n",
        "        assert x_tokenized[0] == start_token\n",
        "        return x_tokenized[:list(x_tokenized).index(end_token)]\n",
        "\n",
        "    def _generate_macro(self, im: torch.Tensor, prompt: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Generates captions for the input images.\n",
        "\n",
        "        Args:\n",
        "            im (torch.Tensor): The input images.\n",
        "            prompt (int): The prompt for caption generation.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The generated captions.\n",
        "        \"\"\"\n",
        "        text=torch.ones((im.shape[0], 1), device=self.device, dtype=torch.long)*prompt\n",
        "\n",
        "        generated = self.caption_model.generate(\n",
        "                    im,\n",
        "                    text=text,\n",
        "                    generation_type='top_p')\n",
        "        return generated\n",
        "\n",
        "    def get_test_transform(self) -> transforms.Compose:\n",
        "        \"\"\"\n",
        "        Returns the test transformation for images.\n",
        "\n",
        "        Returns:\n",
        "            transforms.Compose: The test transformation.\n",
        "        \"\"\"\n",
        "        return transforms.Compose(\n",
        "            [transforms.Resize(size=224, max_size=None, antialias=None),\n",
        "            transforms.CenterCrop(size=(224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))]\n",
        "        )\n",
        "\n",
        "    def generate_captions(self, images: torch.Tensor, prompt: int) -> List[str]:\n",
        "        \"\"\"\n",
        "        Generates captions for the input images.\n",
        "\n",
        "        Args:\n",
        "            images (torch.Tensor): The input images.\n",
        "            prompt (int): The prompt for caption generation.\n",
        "\n",
        "        Returns:\n",
        "            List[str]: The generated captions.\n",
        "        \"\"\"\n",
        "        self.caption_model.eval()\n",
        "\n",
        "        outputs = []\n",
        "        prompt_extended = self._tokenize(prompt).to(self.device)\n",
        "\n",
        "        generated = self._generate_macro(\n",
        "            images,\n",
        "            prompt_extended)\n",
        "\n",
        "        assert len(generated) == len(images)\n",
        "        for i in range(len(generated)):\n",
        "            outputs.append(open_clip.decode(generated[i]).split(\"<end_of_text>\")[0].replace(\"<start_of_text>\", \"\"))\n",
        "        return outputs\n",
        "\n",
        "def get_caption_logits(captioner:Captioner, captions, id2class):\n",
        "    \"\"\"\n",
        "    Gets caption logits based on cosine similarity\n",
        "    \"\"\"\n",
        "    device = captioner.device\n",
        "    classes = list(id2class.values())\n",
        "    tokenizer = factory.get_tokenizer(\"coca_ViT-L-14\")\n",
        "    with torch.cuda.amp.autocast(), torch.no_grad():\n",
        "        class_tokens = tokenizer([f\"A photo of {cls}\" for cls in classes])\n",
        "        class_features = captioner.caption_model.encode_text(class_tokens.to(device), normalize=False)\n",
        "\n",
        "        caption_tokens = tokenizer(captions)\n",
        "        caption_features = captioner.caption_model.encode_text(caption_tokens.to(device))\n",
        "\n",
        "        scale = captioner.caption_model.logit_scale.exp()\n",
        "        caption_logits = F.normalize(caption_features) @ F.normalize(class_features).T\n",
        "\n",
        "\n",
        "    return (caption_logits * scale).softmax(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nwu1LoGhgrq"
      },
      "source": [
        "# 5. Test-Time Prompt Tuning (TPT)\n",
        "\n",
        "Test-Time Prompt Tuning (TPT) is a method proposed to enhance zero-shot generalization capabilities of pre-trained vision-language models, such as CLIP, on new, unseen tasks by dynamically adjusting the text prompts during the test phase. This approach leverages the model's inherent knowledge and adapts it to specific tasks without the need for extra training data or annotations.\n",
        "\n",
        "### 5.1. Zero-Shot Generalization:\n",
        "\n",
        "TPT allows models like CLIP to perform well on new tasks by tuning prompts in real-time based on the input image. It operates in a purely zero-shot manner, thereby enhancing the model's ability to generalize to new domains without additional training or annotations.\n",
        "At the inference stage, the only information available is the single test sample $ X_{test} $ without label information. TPT, therefore, manages to optimize the prompt  $ p $ at test time based on the single test sample. In general, our objective can be formulated in the form of\n",
        "\n",
        "$  \\mathbf{p^∗} = arg min _\\mathbf{p} L(F; \\mathbf{p}; X_{test}) $\n",
        "\n",
        "for some carefully constructed loss, where $ \\mathbf{p^∗} $ is the optimal prompt distribution.\n",
        "\n",
        "### 5.2. Adaptive Prompt Optimization:\n",
        "\n",
        "During the test phase, TPT optimizes the text prompt by minimizing the entropy across the outputs of multiple augmented views of a single test image. This ensures consistent predictions across different views, making the model more robust.\n",
        "We generate N randomly augmented views of the test image using a family of random augmentations $ A $,\n",
        "and minimize the entropy of the averaged prediction probability distribution:\n",
        "\n",
        "$ \\mathbf{p^∗} = arg min_{\\mathbf{p}} − {\\sum \\limits _{i=1} ^{K}}\\tilde{p}_{\\mathbf{p}} (y_i|jX_{test}) log \\tilde{p}_{\\mathbf{p}}(y_i|X_{test}) $\n",
        "\n",
        "where $ \\tilde{p}_{\\mathbf{p}}(y_i|X_test) = \\frac{1}{N} {\\sum \\limits _{i=1} ^{N}}p_{\\mathbf{p}} (y_i|A_{i}(X_{test})) $\n",
        "\n",
        "Here,  $ p_{\\mathbf{p}}(y_i|A_i(X_{test})) $ is the vector of class probabilities produced by the model when provided with\n",
        "prompt $ \\mathbf{p} $ and the i-th augmented view of the test image.\n",
        "\n",
        "### 5.3. Confidence Selection:\n",
        "\n",
        "TPT incorporates a confidence selection mechanism to filter out noisy augmentations that could lead to inaccurate predictions. Only high-confidence (i.e. with entropy under a certain threshold $ τ $) views are used for prompt optimization, improving the overall reliability of the model's predictions. The benefits of this approach can be observed in Figure 4. (b) of the [TPT paper](https://arxiv.org/pdf/2209.07511).\n",
        "\n",
        "<img src=\"https://github.com/rogergheser/DS-DLProject/blob/main/source/images/confidence_selection_benefits.png?raw=1\" width=35%>\n",
        "<break>\n",
        "<img src=\"https://github.com/rogergheser/DS-DLProject/blob/main/source/images/confidence_selection_table.png?raw=1\" width=100%>\n",
        "\n",
        "### 5.4. Applications\n",
        "\n",
        "#### 5.4.1. Image Classification:\n",
        "\n",
        "For image classification tasks, TPT generates multiple augmented views of a test image and tunes the prompt to achieve consistent predictions. This improves the zero-shot accuracy of models like CLIP without requiring additional labeled data.\n",
        "Because labels are not available for test time tuning, the loss for prompt tuning must be unsupervised. The goal is to promote the consistency\n",
        "of the model’s predictions across different augmented views of a given test image.\n",
        "\n",
        "### 5.5. Benefits\n",
        "\n",
        "#### 5.5.1. Enhanced Generalization:\n",
        "\n",
        "By tuning prompts dynamically, TPT helps models maintain high performance even when faced with distribution shifts or unseen categories.\n",
        "\n",
        "#### 5.5.2. Improved Performance:\n",
        "\n",
        "Experimental results have shown that TPT can significantly boost the performance of vision-language models on various benchmarks, achieving state-of-the-art results in many cases.\n",
        "\n",
        "For more detailed information, you can refer to the original research paper on TPT: [Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models](https://arxiv.org/pdf/2209.07511).\n",
        "\n",
        "![TPT Overview](source/images/tpt_overview.png)\n",
        "\n",
        "Figure 1: Test-time Prompt Tuning (TPT) for image classification. We tune adaptive prompts on\n",
        "the fly with a single test sample, without the need for additional training data or annotations. TPT\n",
        "optimizes the prompt to encourage consistent predictions across augmented views by minimizing the\n",
        "marginal entropy. We introduce confidence selection to filter out noisy augmentations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_rYqzNNhgrq"
      },
      "source": [
        "# 6. Our Approach\n",
        "We propose an ensemble model that combines the strengths of the afore mentioned methods with the additional contribution of CoCa. Given an image we generate a set of augmentations and filter them based on the entropy of the predictions. We then generate captions corresponding to the filtered images using [CoCa](https://arxiv.org/abs/2205.01917) (Contrastive Captioner) and stack the logits of the filtered images. After doing so we apply the TPT step and minimise the entropy of the stacked logits. After backpropagatin we generate a final prediction and reset the network and the optimizer.\n",
        "\n",
        "<img src='source/images/architecture-schema.png'>\n",
        "\n",
        "### 6.1. Image Augmentations\n",
        "We generate a set of augmentations for each image in the batch. We augment the images using [augmix](https://arxiv.org/abs/1912.02781) and then randomly choose from horizontal flip, random vertical flip and random crop augmentations to apply again.\n",
        "\n",
        "### 6.2. Filtering\n",
        "We filter the images based on the entropy of the predictions. We only keep the **10th percentile** of the images with the lowest entropy according to the ablation study in **TPT**.\n",
        "\n",
        "### 6.3. Caption Generation\n",
        "We generate captions for the filtered images using CoCa which is a pretrained model that generates captions for images using a contrastive loss. The captions generated by CoCa provide a more detailed and informative description of the content of the image, often capturing fine-grained details and multiple objects present in the image. We then work out the similarity between the captions and the prompt plus the class names. A first attempt leveraged the CLIP text encoder to determine a similarity score but this failed to grasp similarity between **hypernyms** and **hyponyms** e.g. between 'dog' and 'pug'. Hence we employed the text encoder from CoCa which was able to better recognise these relationships.\n",
        "\n",
        "The choice of using CoCa was related to memory constraints and to provide greater similarity between the underlying backbones and architectures involved in the classification pipeline. However, future work could leverage other captioning models such as [LLava](https://arxiv.org/abs/2304.08485), [BLIP](https://arxiv.org/abs/2201.12086) or the more recent [BLIP-2](https://arxiv.org/abs/2301.12597).\n",
        "\n",
        "### 6.4. Stacking\n",
        "To stack the logits we tried three different approaches: a standard deviation based approach proposed by Yang. et al. [Image-Caption Encoding for Improving Zero-Shot Generalisation](https://arxiv.org/pdf/2402.02662), a harmonic mean of the logits (ours) and an **entropy-weighted average** of the logits (ours).\n",
        "\n",
        "Let $f$ be the logits of the CLIP model, $f_{caption}$ be the logits of the CoCa model, $\\sigma$ be the standard deviation, $H$ be the entropy and $\\alpha$ be a normalising weight. $f$ represents the similarities between the images and the class names, while $f_{caption}$ represents the similarities between the captions and the class names.\n",
        "\n",
        "#### Standard Deviation - ICE loss from [Image-Caption Encoding for Improving Zero-Shot Generalisation](https://arxiv.org/pdf/2402.02662)\n",
        "\n",
        "The first approach calculates the standard deviation of the caption logits and then adds a re-weighted version of the caption logits to the clip logits. The approach has been revisited in order to use CoCa instead of CLIP for the caption logits as it was found to be more effective than the solution proposed by the authors.\n",
        "\n",
        "\n",
        "$ f = f + \\lambda \\cdot f_{caption} $\n",
        "\n",
        "and \n",
        "\n",
        "$ \\lambda = \\epsilon || \\sigma(f), \\sigma(f_{caption}) || $ \n",
        "\n",
        "where \",\" denotes concatenation and $|| \\cdot ||$ denotes the L2 norm and $\\epsilon$ is a hyperparameter that controls the strength of the regularisation.\n",
        "\n",
        "#### Harmonic Mean\n",
        "\n",
        "The second approach calculates the harmonic mean of the two logits. However this approach has shown to be less effective than the other two as the ensemble is suppose to combine the strengths of the two models to compensate for each other weaknesses, whereas the harmonic punishes discrepancy between the two models which is when our approach tries to be most effective.\n",
        "\n",
        "$ f = \\frac{2 \\cdot f \\cdot f_{caption}}{f + f_{caption}} $\n",
        "\n",
        "#### Entropy-Weighted Average\n",
        "\n",
        "The third approach calculates the entropy of the caption logits and CLIP logits and then calculates the weighted average of the two logits based on the entropy of the caption logits.\n",
        "\n",
        "$ A = \\frac{1}{1 + H({f})} $ Weight for CLIP logits\n",
        "\n",
        "$ B = \\frac{1}{1 + H({f_{caption}})} $ Weight for caption logits\n",
        "\n",
        "$ C = A + B $ Normalising factor for the two weights\n",
        "\n",
        "$ f = \\frac{A}{C} f + \\frac{B}{C}f_{caption} $ Weighted average of the two logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwomUafGBzv_"
      },
      "source": [
        "# 7. Project Setup\n",
        "\n",
        "We import the last necessary libraries and define some constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sRxyKtbvBzwA"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "import io\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import sys\n",
        "import torchvision\n",
        "torchvision.disable_beta_transforms_warning()\n",
        "import torch.amp\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import zipfile\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "try:\n",
        "    from torchvision.transforms import InterpolationMode\n",
        "    BICUBIC = InterpolationMode.BICUBIC\n",
        "except ImportError:\n",
        "    BICUBIC = Image.BICUBIC\n",
        "\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "from open_clip import factory\n",
        "import clip\n",
        "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI2ybjKCBzwA"
      },
      "source": [
        "Before starting, at last, we need the four backbones of COOP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OF0vLZujBzwA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=18ypxfd82RR0pizc5MM1ZWDYDk4j0BtPF\n",
            "From (redirected): https://drive.google.com/uc?id=18ypxfd82RR0pizc5MM1ZWDYDk4j0BtPF&confirm=t&uuid=d1d77df6-54d0-45db-9647-54eca36a822f\n",
            "To: /home/sagemaker-user/notebook_test/bin/coop/backbones_COOP.zip\n",
            "100%|██████████| 137M/137M [00:05<00:00, 26.2MB/s] \n"
          ]
        }
      ],
      "source": [
        "# Create a directory\n",
        "output_dir = './bin/coop'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Download the models from the link given in the original github repository\n",
        "if len(os.listdir(output_dir)) == 0:\n",
        "    url = 'https://drive.google.com/uc?id=18ypxfd82RR0pizc5MM1ZWDYDk4j0BtPF'\n",
        "    output_file = os.path.join(output_dir, 'backbones_COOP.zip')\n",
        "    gdown.download(url, output_file, quiet=False)\n",
        "\n",
        "    # Path to the downloaded zip file\n",
        "    zip_file = os.path.join(output_dir, 'backbones_COOP.zip')\n",
        "\n",
        "    # Extract the contents\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(output_dir)\n",
        "\n",
        "    os.remove(zip_file)\n",
        "else:\n",
        "    print(f\"{output_dir} is not empty\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RZVEsizwNqcF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mrn101_ep50_16shots\u001b[0m/  \u001b[01;34mvit_b16_ep50_16shots\u001b[0m/\n",
            "\u001b[01;34mrn50_ep50_16shots\u001b[0m/   \u001b[01;34mvit_b32_ep50_16shots\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "# Run this only if download happened\n",
        "%mv bin/coop/to_gdrive/* bin/coop/\n",
        "%rmdir bin/coop/to_gdrive/\n",
        "%ls bin/coop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLI8v-_2OY9E"
      },
      "source": [
        "# Implementation Details\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw8X6BrHhgrs"
      },
      "source": [
        "The whole project is built on top of the <cite>tpt_eval.py</cite> file. The file is structured in order to make the main required imports and it implements the **Test Time Adaptation** step. There are two main functions:\n",
        "1. tta_net_train\n",
        "2. tpt_train_loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UChn3ubKBzwA"
      },
      "source": [
        "In order to guide the update step which perform gradient descent to update the prompt lerner parameters, we use _avg_entropy_ as loss function as shown in [Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models](https://arxiv.org/pdf/2209.07511)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3yQhn6Fohgrs"
      },
      "outputs": [],
      "source": [
        "def avg_entropy(outputs):\n",
        "    logits = outputs - outputs.logsumexp(dim=-1, keepdim=True) # logits = outputs.log_softmax(dim=1) [N, 1000]\n",
        "    avg_logits = logits.logsumexp(dim=0) - np.log(logits.shape[0]) # avg_logits = logits.mean(0) [1, 1000]\n",
        "    min_real = torch.finfo(avg_logits.dtype).min\n",
        "    avg_logits = torch.clamp(avg_logits, min=min_real)\n",
        "    return -(avg_logits * torch.exp(avg_logits)).sum(dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBnSbuRwhgrs"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HHqXJYFUmgZz"
      },
      "outputs": [],
      "source": [
        "def get_loss_function():\n",
        "    loss_function = torch.nn.CrossEntropyLoss()\n",
        "    return loss_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6YB6lxT2hgrs"
      },
      "outputs": [],
      "source": [
        "def show_image(image, label):\n",
        "    image = image.numpy()\n",
        "    plt.title(f\"Image of {label}\")\n",
        "    img = np.transpose((image * 255).astype(np.uint8), (1, 2, 0))\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "def entropy(p):\n",
        "    \"\"\"\n",
        "    Given a tensor p representing a probability distribution, returns the entropy of the distribution\n",
        "    \"\"\"\n",
        "    return -torch.sum(p * torch.log(p + 1e-7))\n",
        "\n",
        "def get_index(path):\n",
        "    \"\"\"\n",
        "    Given a directory path, returns the highest index of the files in the directory or zero\n",
        "    \"\"\"\n",
        "    import re\n",
        "    try:\n",
        "        files = os.listdir(path)\n",
        "        indices = [int(re.findall(r'\\d+', file)[0]) for file in files]\n",
        "        return max(indices) + 1\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "class ToUint8Transform:\n",
        "    \"\"\"Transform to convert images to uint8\"\"\"\n",
        "    def __call__(self, tensor):\n",
        "        return (tensor.mul(255)).byte()  # Use .byte() to convert to uint8\n",
        "\n",
        "def generate_augmented_batch(original_tensor, num_images, augmix_module):\n",
        "    batch = [original_tensor]  # Start with the original image\n",
        "\n",
        "    # Generate num_images-1 augmented images\n",
        "    for _ in range(num_images):\n",
        "        augmented_image = augmix_module(original_tensor.unsqueeze(0)).squeeze(0)\n",
        "        batch.append(augmented_image)\n",
        "\n",
        "    # Convert list of tensors to a single tensor\n",
        "    batch_tensor = torch.stack(batch)\n",
        "    return batch_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7AAAKahhgrs"
      },
      "source": [
        "## Metrics and reports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "QYwz9Cqthgrs"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def create_run_info(dataset_name, backbone, ice_loss, test_accuracy, run_name, ensamble_method):\n",
        "    info = {\n",
        "        \"exp_name\": run_name,\n",
        "        \"dataset\": dataset_name,\n",
        "        \"backbone\": backbone,\n",
        "        \"top1\": test_accuracy,\n",
        "        \"ice_loss\": ice_loss,\n",
        "        \"ensamble method\": ensamble_method if ice_loss else None\n",
        "    }\n",
        "    with open(f\"runs/{run_name}/final_result.txt\", \"w\") as file:\n",
        "        json.dump(info, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tG5KKsU7hgrs"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "\n",
        "    def get_avg(self):\n",
        "        \"\"\"\n",
        "        Returns the average value of the meter, -1 if no values have been added\n",
        "        \"\"\"\n",
        "        if self.count == 0:\n",
        "            return -1\n",
        "        return self.sum / self.count * 100.00"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "SXg_u5DXhgrs"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def batch_report(inputs:torch.Tensor, outputs: torch.Tensor, final_prediction:torch.Tensor,\n",
        "                 target:torch.Tensor, id2classes: dict, batch_n:int):\n",
        "    \"\"\"\n",
        "    Creates a report in the batch_report/ dir showing augmentation images and their confidence\n",
        "    Then shows the average confidence prediction\n",
        "    :param: inputs: torch.Tensor: batch of images\n",
        "    :param: outputs: torch.Tensor: batch of outputs\n",
        "    :param: final_prediction: torch.Tensor: average prediction\n",
        "    :param: target: torch.Tensor: batch with target label\n",
        "    :param: id2classes: dict: mapping from class index to class name\n",
        "    :param: batch_n: int: batch number\n",
        "    \"\"\"\n",
        "    max_plots = 10\n",
        "\n",
        "    probabilities, predictions = outputs.cpu().topk(5)\n",
        "    probabilities = probabilities.detach().numpy()\n",
        "    predictions = predictions.detach()\n",
        "\n",
        "    clip_mean = [0.48145466, 0.4578275, 0.40821073]\n",
        "    clip_std = [0.26862954, 0.26130258, 0.27577711]\n",
        "\n",
        "    mean = torch.tensor(clip_mean).reshape(1, 3, 1, 1)\n",
        "    std = torch.tensor(clip_std).reshape(1, 3, 1, 1)\n",
        "\n",
        "    # Denormalize the batch of images\n",
        "    unnormalize = transforms.Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
        "    denormalized_images = unnormalize(inputs)\n",
        "\n",
        "    # Visualise the input using matplotlib\n",
        "    images = [image.numpy().transpose(1, 2, 0) for image in denormalized_images.cpu()] # Convert to numpy and transpose to (H, W, C)\n",
        "\n",
        "    # Visualise the input using matplotlib\n",
        "    label = id2classes[target[0].item()]\n",
        "\n",
        "    plt.figure(figsize=(16,16))\n",
        "    plt.title(f\"Image batch of {label} - min entropy {max_plots} percentile selected\\n{datetime.now()}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    for i, image in enumerate(images[:max_plots]):\n",
        "        plt.subplot(6,4, 2*i+1)\n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(6,4, 2*i+2)\n",
        "        y = np.arange(probabilities.shape[-1])\n",
        "        plt.grid()\n",
        "        plt.barh(y, probabilities[i])\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.gca().set_axisbelow(True)\n",
        "        plt.yticks(y, [id2classes[pred] for pred in predictions[i].numpy()])\n",
        "        plt.xlabel(\"probability\")\n",
        "    # Original image\n",
        "    plt.subplot(6,4, 22)\n",
        "    plt.imshow(images[0])\n",
        "    plt.axis('off')\n",
        "    plt.xlabel(\"Original image\")\n",
        "\n",
        "    # Final prediction\n",
        "    plt.subplot(6,4, 2*i+1)\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    avg_prob, avg_pred = final_prediction.cpu().topk(5)\n",
        "    avg_prob = avg_prob.detach().numpy()\n",
        "    avg_pred = avg_pred.detach()\n",
        "    plt.subplot(6,4,23)\n",
        "    y = np.arange(avg_prob.shape[-1])\n",
        "    plt.grid()\n",
        "    plt.barh(y, avg_prob[0])\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.gca().set_axisbelow(True)\n",
        "    plt.yticks(y, [id2classes[index] for index in avg_pred[0].numpy()])\n",
        "    plt.xlabel(\"Final prediction (avg entropy)\")\n",
        "\n",
        "    plt.savefig(f\"batch_reports/Batch{batch_n}.png\")\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "loofeUE_hgrs"
      },
      "outputs": [],
      "source": [
        "def make_histogram(no_tpt_acc: dict, tpt_acc: dict, no_tpt_label: str, tpt_label: str, save_path:str=None, worst_case=False)-> Image:\n",
        "    \"\"\"\n",
        "    Creates histogram for class accuracies and log it with tensorboard to save the plot\n",
        "    :param: no_tpt_acc: dict: class accuracies before TPT\n",
        "    :param: tpt_acc: dict: class accuracies after TPT\n",
        "    :param: no_tpt_label: str: label for the no_tpt_acc\n",
        "    :param: tpt_label: str: label for the tpt_acc\n",
        "    :param: save_path: str: path to save the plot. If None, the plot is not saved\n",
        "\n",
        "    :return: PIL.Image: image of the plot\n",
        "    \"\"\"\n",
        "\n",
        "    no_tpt_acc = {k: v for k, v in no_tpt_acc.items() if v != -1}\n",
        "    tpt_acc = {k: v for k, v in tpt_acc.items() if v != -1}\n",
        "\n",
        "\n",
        "    if worst_case:\n",
        "        worse_no_tpt_acc, worse_tpt_acc = {}, {}\n",
        "        for key in tpt_acc.keys():\n",
        "            if tpt_acc[key] < no_tpt_acc[key]:\n",
        "                worse_no_tpt_acc[key] = no_tpt_acc[key]\n",
        "                worse_tpt_acc[key] = tpt_acc[key]\n",
        "\n",
        "        no_tpt_acc = worse_no_tpt_acc\n",
        "        tpt_acc = worse_tpt_acc\n",
        "\n",
        "    classes = list(no_tpt_acc.keys())\n",
        "    x = np.arange(len(classes))\n",
        "    width = 0.35\n",
        "\n",
        "    fig, ax = plt.subplots(dpi=500)\n",
        "    ax.bar(x - width/2, no_tpt_acc.values(), width, color='b', label=no_tpt_label)\n",
        "    ax.bar(x + width/2, tpt_acc.values(), width, color='r', label=tpt_label)\n",
        "    plt.legend()\n",
        "\n",
        "    ax.set_ylabel('Accuracy')\n",
        "    ax.set_title('Class accuracies')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(classes, rotation=-90, fontsize=7.1-(len(classes)/200*7))\n",
        "\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    buf.seek(0)\n",
        "\n",
        "    image = Image.open(buf)\n",
        "    image = np.array(image)\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Vq19Uyulhgrs"
      },
      "outputs": [],
      "source": [
        "def report_predictions(idx:int, predictions:str, values:float, target:str):\n",
        "    \"\"\"\n",
        "    Saves the predictions to a file in the batch_predictions/ directory\n",
        "    :param: idx: int: index of the batch\n",
        "    :param: predictions: str: list of predictions\n",
        "    :param: values: float: list of probabilities for each prediction\n",
        "    :param: target: str: target class\n",
        "    \"\"\"\n",
        "    dir = 'batch_predictions/'\n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(dir)\n",
        "\n",
        "    with open(f\"{dir}batch_{idx}.txt\", 'w') as f:\n",
        "        f.write(f\"Target: {target}\\n\")\n",
        "        for pred, value in zip(predictions, values[0]):\n",
        "            f.write(f\"\\t{pred}: {value:.2f}\\n\")\n",
        "        f.write(f\"{datetime.now()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "L4tGROFdhgrs"
      },
      "outputs": [],
      "source": [
        "def compute_accuracies(no_tpt_class_acc:dict[AverageMeter], tpt_class_acc:dict[AverageMeter]):\n",
        "    \"\"\"\n",
        "    Computes the average accuracy for each class before and after TPT\n",
        "    :param: no_tpt_class_acc: dict: class accuracies before TPT\n",
        "    :param: tpt_class_acc: dict: class accuracies after TPT\n",
        "\n",
        "    :return: dict, dict: no_tpt_accuracies, accuracies\n",
        "    \"\"\"\n",
        "    no_tpt_accuracies = {key: val.get_avg() for key, val in no_tpt_class_acc.items()}\n",
        "    accuracies = {key: val.get_avg() for key, val in tpt_class_acc.items()}\n",
        "\n",
        "    return no_tpt_accuracies, accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "16zdAoPIhgrt"
      },
      "outputs": [],
      "source": [
        "def caption_report(images, image_logits, caption_logits, ice_scores, label, outputs, caption_prediction, id2class, idx):\n",
        "    \"\"\"\n",
        "    Generates a report for the captions generated by the model\n",
        "    :param: images: torch.Tensor: batch of images\n",
        "    :param: label: torch.Tensor: batch of labels\n",
        "    :param: outputs: list: list of captions\n",
        "    :param: caption_prediction: torch.Tensor: average prediction from caption logits\n",
        "    :param: id2class: dict: mapping from class index to class name\n",
        "    :param: idx: int: index of the batch\n",
        "    \"\"\"\n",
        "\n",
        "    ice_probabilities, ice_predictions = ice_scores.topk(5)\n",
        "    cap_probabilities = caption_logits.gather(1, ice_predictions)\n",
        "    img_probabilities = image_logits.gather(1, ice_predictions)\n",
        "\n",
        "    ice_probabilities = ice_probabilities.cpu().detach()\n",
        "    ice_predictions = ice_predictions.cpu().detach()\n",
        "    cap_probabilities = cap_probabilities.cpu().detach()\n",
        "    img_probabilities = img_probabilities.cpu().detach()\n",
        "\n",
        "    clip_mean = [0.48145466, 0.4578275, 0.40821073]\n",
        "    clip_std = [0.26862954, 0.26130258, 0.27577711]\n",
        "\n",
        "    mean = torch.tensor(clip_mean).reshape(1, 3, 1, 1)\n",
        "    std = torch.tensor(clip_std).reshape(1, 3, 1, 1)\n",
        "\n",
        "    # Denormalize the batch of images\n",
        "    unnormalize = transforms.Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
        "    denormalized_images = unnormalize(images)\n",
        "\n",
        "    # Visualise the input using matplotlib\n",
        "    images = [image.numpy().transpose(1, 2, 0) for image in denormalized_images.cpu()] # Convert to numpy and transpose to (H, W, C)\n",
        "    label = [lab.item() for lab in label.cpu()] if label.shape[0] > 1 else label.item()\n",
        "\n",
        "    plt.figure(figsize=(16, 16), dpi=300)\n",
        "    plt.title(f\"Captions generated from the {idx}th batch --- caption prediction {id2class[caption_prediction.item()]}\") if isinstance(label, list) else plt.title(f\"Caption for {id2class[label]} class\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    for i, image in enumerate(images[:9]):\n",
        "        plt.subplot(6,4, 2*i+1)\n",
        "\n",
        "        plt.title(id2class[label[i]]) if isinstance(label, list) else id2class[label]\n",
        "        plt.xlabel(outputs[i])\n",
        "        plt.imshow(image)\n",
        "\n",
        "        plt.subplot(6,4, 2*i+2)\n",
        "        width=0.35\n",
        "        y = np.arange(ice_probabilities.shape[-1])\n",
        "        plt.grid()\n",
        "        plt.barh(y-width, ice_probabilities[i], width*2/3, color=\"green\", label='ICE')\n",
        "        plt.barh(y, cap_probabilities[i], width*2/3, color=\"red\", label='CAP')\n",
        "        plt.barh(y+width, img_probabilities[i], width*2/3, color=\"blue\", label='IMG')\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.gca().set_axisbelow(True)\n",
        "        plt.yticks(y, [id2class[pred] for pred in ice_predictions[i].numpy()])\n",
        "        plt.xlim(0,1)\n",
        "        plt.xlabel(\"probability\")\n",
        "\n",
        "    plt.legend()\n",
        "    plt.subplots_adjust(hspace=0.5)  # Increase vertical space between subplots\n",
        "\n",
        "    plt.savefig(f\"caption_reports/batch_{idx}.png\")\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Try81afYhgrt"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "def confusion_matrix(true_labels: list[int], predicted_labels: list[int], class_names: list[str], with_numbers:bool = False, save_path:str=Optional[str])->tuple:\n",
        "    \"\"\"\n",
        "    Computes and returns the confusion matrix given a batch of images and labels\n",
        "    Then plots the results\n",
        "    :param true_labels: list: ground truth labels\n",
        "    :param predicted_labels: list: predicted labels\n",
        "    :param class_names: list: class names\n",
        "    :return: tuple: confusion matrix and figure\n",
        "    \"\"\"\n",
        "    from sklearn.metrics import confusion_matrix as confusion_matrix_comp\n",
        "\n",
        "    assert len(true_labels) == len(predicted_labels), \"Differing prediction and ground truth size\"\n",
        "\n",
        "    cm = confusion_matrix_comp(true_labels, predicted_labels)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(40, 40))\n",
        "    cax = ax.matshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names, rotation=90, fontsize=10)\n",
        "    plt.yticks(tick_marks, class_names, fontsize=10)\n",
        "\n",
        "    if with_numbers:\n",
        "        thresh = cm.max() / 2.\n",
        "        for i in range(cm.shape[0]):\n",
        "            for j in range(cm.shape[1]):\n",
        "                plt.text(j, i, format(cm[i, j], 'd'), #d is decimal format: the values will be formatted as integers\n",
        "                        ha=\"center\", va=\"center\",color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label', fontsize=12)\n",
        "    plt.xlabel('Predicted label', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return fig, cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "AjfHwRTthgrt"
      },
      "outputs": [],
      "source": [
        "def average_class_error(cm, class_names: list[str], k:int = 1, save_path: Optional[str]=None):\n",
        "    \"\"\"\n",
        "    Computes and returns the average class error given a batch of images and labels\n",
        "    Then plots the results\n",
        "    :param cm: np.array: confusion matrix\n",
        "    :param class_names: list: class names\n",
        "    :param k: int: top k classes to consider\n",
        "    :param save_path: str: path to save the results\n",
        "    \"\"\"\n",
        "    # Compute class-wise error\n",
        "    class_wise_error = 1 - np.diag(cm) / np.sum(cm, axis=1)\n",
        "\n",
        "    # Sort class error and class names based on error rates\n",
        "    sorted_indices = np.argsort(class_wise_error)  # Sort in descending order\n",
        "    sorted_class_error = class_wise_error[sorted_indices]\n",
        "    sorted_class_names = [class_names[i] for i in sorted_indices]\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(range(len(sorted_class_names)), sorted_class_error, color='blue')\n",
        "    plt.ylabel('Classes')\n",
        "    plt.xlabel('Error Rate')\n",
        "    plt.title('Error Rate per Class')\n",
        "    plt.yticks(range(len(sorted_class_names)), sorted_class_names)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "    else:\n",
        "        plt.show()\n",
        "    return class_wise_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vULeWoKchgrt"
      },
      "source": [
        "## Main Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hN-rOgYhgrt"
      },
      "source": [
        "AdamW is our optimizer of choice with a learning rate of 0.005 as suggested by the authors of TPT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "4C-qD5Pshgrt"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(params, lr):\n",
        "    optimizer = torch.optim.AdamW(params, lr)\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMSDkTcBhgrt"
      },
      "source": [
        "This function takes as input the logits of the model and the augmentations of the image and returns the best augmentations and their corresponding logits based on the entropy of the predictions.\n",
        "This is where _confidence selection_ comes into play by selecting the 10th percentile of the images, see TPT ablation study [Confidence Selection](#53-confidence-selection)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "EbVL7wVYhgrt"
      },
      "outputs": [],
      "source": [
        "def filter_on_entropy(inputs:torch.Tensor, outputs:torch.Tensor, p_percentile:int=10, return_original:bool=False):\n",
        "    \"\"\"\n",
        "    Return all inputs and outputs where prediction entropy is in the 'p' percentile\n",
        "    :param: inputs: torch.Tensor: batch of inputs\n",
        "    :param: outputs: torch.Tensor: batch of outputs\n",
        "    :param: p_percentile: int: percentile threshold\n",
        "    :param: return_original: bool: return the original image of the batch\n",
        "    \"\"\"\n",
        "\n",
        "    p_threshold = np.percentile([entropy(t).item() for t in outputs], p_percentile)\n",
        "    entropies = [entropy(t).item() for t in outputs]\n",
        "    entropies = [0 if val > p_threshold else 1 for val in entropies]\n",
        "    indices = torch.nonzero(torch.tensor(entropies)).squeeze(1)\n",
        "\n",
        "    if return_original and 0 not in indices:\n",
        "        torch.cat((torch.tensor([0]), indices))\n",
        "\n",
        "    return inputs[indices], outputs[indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzxDg8zqhgrt"
      },
      "source": [
        "This cell contains the main function used to construct the ICE (Image-Caption Encoding) logits. We defined 3 different approaches to stack the logits of the filtered images. Which can be called with the following options:\n",
        "- std_dev\n",
        "- **entropy** (our proposal)\n",
        "- harmonic_mean\n",
        "\n",
        "For further details about the formulation refer to [6.4 Stacking](#64-stacking)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "RVTeQ2Tchgrt"
      },
      "outputs": [],
      "source": [
        "def add_caption_loss(net: OurCLIP, captioner: Captioner, batch, text_features, id2classes, prompt=\"a \", ensamble_method=\"entropy\", K=200, debug=False):\n",
        "    \"\"\"\n",
        "    Adds caption loss to the filtered_outputs using the given captioner.\n",
        "\n",
        "    Args:\n",
        "        net (OurCLIP): The network used to generate the text features.\n",
        "        captioner (Captioner): The captioner object used to generate captions.\n",
        "        batch (tuple): Tuple containing filtered inputs and outputs, batch_idx and label\n",
        "        text_features: The text features of the labels computed by the model.\n",
        "        id2classes (dict): The mapping from class index to class name.\n",
        "        prompt (str): The prompt used for generating captions. Default is \"a \".\n",
        "        _lambda (float): The value of lambda used for computing the weighted logit summation\n",
        "        K (int): The number of top classes to consider. Default is 200.\n",
        "        debug (bool): Whether to print debug information. Default is False.\n",
        "\n",
        "    Returns:\n",
        "\n",
        "        The updated filtered_outputs with caption loss added.\n",
        "        The caption prediction from the average of all the logits\n",
        "    \"\"\"\n",
        "    batch_idx, filtered_inputs, filtered_outputs, label = batch\n",
        "    # Compute captions for each augmentation using coca functions\n",
        "    device = filtered_inputs.device\n",
        "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "        captions = captioner.generate_captions(filtered_inputs, prompt)\n",
        "\n",
        "    caption_logits = get_caption_logits(captioner, captions, id2classes)\n",
        "    image_logits = filtered_outputs\n",
        "    \n",
        "\n",
        "    ice_scores = torch.zeros_like(image_logits)\n",
        "\n",
        "    if ensamble_method == \"std_dev\":\n",
        "        std_devs = torch.stack((image_logits.std(dim=1), caption_logits.std(dim=1)), dim=1)\n",
        "        coef = 0.08 * F.normalize(std_devs, dim=1)\n",
        "        coef = coef[:, 1].unsqueeze(1).expand(-1, K)\n",
        "        # Sum the image and caption scores to obtain the ICE scores\n",
        "        ice_scores = image_logits + coef * caption_logits\n",
        "    elif ensamble_method == \"entropy\":\n",
        "        for batch in range(image_logits.shape[0]):\n",
        "            A = 1/(1 + entropy(image_logits[batch]).item())\n",
        "            B = 1/(1 + entropy(caption_logits[batch]).item())\n",
        "            C = A + B\n",
        "            ice_scores[batch] = (A/C * image_logits[batch] + B/C * caption_logits[batch])\n",
        "    elif ensamble_method == \"harmonic_mean\":\n",
        "        for batch in range(image_logits.shape[0]):\n",
        "            ice_scores[batch] = (2 * image_logits[batch] * caption_logits[batch]).div(image_logits[batch] + caption_logits[batch])\n",
        "    else:\n",
        "        raise ValueError(\"Ensamble method not implemented\")\n",
        "\n",
        "    caption_prediction = torch.mean(caption_logits, dim=0)\n",
        "    if debug:\n",
        "        caption_report(filtered_inputs, image_logits, caption_logits, ice_scores, label, captions, caption_prediction, id2classes, batch_idx)\n",
        "\n",
        "    if batch_idx % LOG_FREQUENCY == 0:\n",
        "        caption_report(filtered_inputs, image_logits, caption_logits, ice_scores, label, captions, caption_prediction, id2classes, batch_idx)\n",
        "    return ice_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Csqe-iNbBzwA"
      },
      "source": [
        "_tta_net_train_: This is the main test-time adaptation function which takes the model and works out the best augmentations and their corresponding logits based on the entropy of the predictions. It then generates captions for the filtered images and stacks the logits of the filtered images. Ultimately, it backpropagates the average entropy loss and updates the prompt learner parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "NudetjV8BzwA"
      },
      "outputs": [],
      "source": [
        "def tta_net_train(batch, net, optimizer, scaler, id2classes, device=\"cuda\", captioner=None, debug=False):\n",
        "    batch_idx, inputs, targets = batch\n",
        "\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs, text_features = net(inputs)\n",
        "    outputs = outputs.softmax(dim=-1)\n",
        "\n",
        "    filtered_inputs, filtered_outputs = filter_on_entropy(inputs, outputs, p_percentile=10, return_original=debug)\n",
        "    if captioner is not None:\n",
        "        batch = (batch_idx, filtered_inputs, filtered_outputs, targets)\n",
        "        filtered_outputs = add_caption_loss(net, captioner, batch, text_features, id2classes, debug=debug, ensamble_method=ENSAMBLE_METHOD)\n",
        "\n",
        "    avg_predictions = torch.mean(filtered_outputs, dim=0).unsqueeze(0)\n",
        "    prediction_entropy = entropy(avg_predictions).item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    loss = avg_entropy(filtered_outputs)\n",
        "\n",
        "    if scaler is None:\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    else:\n",
        "        with torch.cuda.amp.autocast():\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "    # show batch\n",
        "    if debug:\n",
        "        batch_report(filtered_inputs, filtered_outputs, avg_predictions, targets, id2classes, batch_n=batch_idx)\n",
        "    if batch_idx % LOG_FREQUENCY == 0:\n",
        "        batch_report(filtered_inputs, filtered_outputs, avg_predictions, targets, id2classes, batch_n=batch_idx)\n",
        "\n",
        "    prediction = avg_predictions.argmax(dim=1)\n",
        "    return loss.item(), prediction, prediction_entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHg24vu2BzwA"
      },
      "source": [
        "_tpt_train_loop_: is the main function which takes every element of the dataset, updates the network calling _tta_net_train_, evaluate the prediction with the new network and then it resets the network for the next sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "cbJhlU1SBzwA"
      },
      "outputs": [],
      "source": [
        "def tpt_train_loop(data_loader, net, optimizer, cost_function, scaler, writer, id2classes, device=\"cuda\", captioner=None, debug=False, checkpoint=None):\n",
        "\n",
        "    if checkpoint:\n",
        "        offset, cumulative_loss, top1, top5, no_tpt_class_acc, tpt_class_acc = checkpoint\n",
        "    else:\n",
        "        offset = 0\n",
        "        cumulative_loss = AverageMeter()\n",
        "        top1 = AverageMeter()\n",
        "        top5 = AverageMeter()\n",
        "\n",
        "        no_tpt_class_acc = {c: AverageMeter() for c in id2classes.values()}\n",
        "        tpt_class_acc = {c: AverageMeter() for c in id2classes.values()}\n",
        "\n",
        "    loss_diff = 0.0\n",
        "    optimizer_state = deepcopy(optimizer.state_dict())\n",
        "\n",
        "    try:\n",
        "        pbar = tqdm(data_loader, desc=\"Testing\", position=0, leave=True, initial=offset, total=len(data_loader)+offset)\n",
        "        for batch_idx, (inputs, targets, _) in enumerate(data_loader):\n",
        "            batch_idx += offset # offset to continue from a checkpoint\n",
        "\n",
        "            # Reset the prompt_learner to its initial state and the optimizer to its initial state\n",
        "            with torch.no_grad():\n",
        "                net.reset()\n",
        "                optimizer.load_state_dict(optimizer_state)\n",
        "\n",
        "            _loss, no_tpt_prediction, no_tpt_prediction_entropy = tta_net_train((batch_idx, inputs, targets), net, optimizer, scaler, id2classes, device=device, captioner=captioner, debug=debug)\n",
        "\n",
        "            net.eval()\n",
        "            with torch.no_grad():\n",
        "                # Classification with the updated net\n",
        "                inputs = inputs[0].unsqueeze(0).to(device)\n",
        "                targets = targets.to(device)\n",
        "                outputs, _ = net(inputs)\n",
        "                loss = cost_function(outputs, targets)\n",
        "                prediction = outputs.argmax(dim=1)\n",
        "                prediction_entropy = entropy(prediction).item()\n",
        "\n",
        "                cumulative_loss.update(loss.item())\n",
        "\n",
        "            # Update accuracies\n",
        "            _key = id2classes[targets.item()]\n",
        "            if no_tpt_prediction.item() == targets.item():\n",
        "                no_tpt_class_acc[_key].update(1)\n",
        "            else:\n",
        "                no_tpt_class_acc[_key].update(0)\n",
        "\n",
        "            values, predictions = outputs.topk(5)\n",
        "            if prediction == targets:\n",
        "                top1.update(1)\n",
        "                tpt_class_acc[_key].update(1)\n",
        "            else:\n",
        "                top1.update(0)\n",
        "                tpt_class_acc[_key].update(0)\n",
        "\n",
        "            if targets.item() in predictions:\n",
        "                top5.update(1)\n",
        "            else:\n",
        "                top5.update(0)\n",
        "\n",
        "            if debug:\n",
        "                top5_str = [id2classes[pred] for pred in predictions[0].tolist()]\n",
        "                target_str = id2classes[targets.item()]\n",
        "                report_predictions(batch_idx, top5_str, values, target_str)\n",
        "\n",
        "            loss_diff =  _loss - loss.item() # comparison of loss with and without TPT\n",
        "            entropy_diff = prediction_entropy - no_tpt_prediction_entropy # comparison of entropy with and without TPT\n",
        "            # Log Values\n",
        "            writer.add_scalar(\"Delta_loss/test\", loss_diff, batch_idx)\n",
        "            writer.add_scalar(\"Delta_entropy/test\", entropy_diff, batch_idx)\n",
        "            writer.add_scalar(\"Top-1\", top1.get_avg(), batch_idx)\n",
        "            writer.add_scalar(\"Top-5\", top5.get_avg(), batch_idx)\n",
        "\n",
        "            if batch_idx % LOG_FREQUENCY == 0 :#and batch_idx > 10:\n",
        "                logger.info(f\"[LOSS] Batch {batch_idx} - Delta loss: {loss_diff:.5f}, Delta entropy: {entropy_diff:.5f}\")\n",
        "                no_tpt_accuracies, accuracies = compute_accuracies(no_tpt_class_acc, tpt_class_acc)\n",
        "                histogram = make_histogram(no_tpt_accuracies, accuracies,\n",
        "                                        'No TPT', 'TPT', save_path=f\"runs/{RUN_NAME}/class_accuracy%{batch_idx}e.png\")\n",
        "                writer.add_image(f\"Class accuracies%{batch_idx}e\", histogram, batch_idx, dataformats=\"HWC\")\n",
        "                logger.info(f\"[ACC] Batch num:{batch_idx} - Top1: {top1.get_avg()}, Top5: {top5.get_avg()}\")\n",
        "\n",
        "                dump_object = batch_idx, cumulative_loss, top1, top5, no_tpt_class_acc, tpt_class_acc\n",
        "                pickle.dump(dump_object, open(f\"runs/{RUN_NAME}/checkpoint%{batch_idx}.pkl\", \"wb\"))\n",
        "\n",
        "\n",
        "            pbar.set_postfix(test_loss=loss.item(), top1=top1.get_avg(), top5=top5.get_avg())\n",
        "            pbar.update(1)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"User keyboard interrupt\")\n",
        "    if batch_idx % LOG_FREQUENCY != 0 or batch_idx == len(data_loader) + offset:#and batch_idx > 10:\n",
        "        logger.info(f\"[LOSS] Batch {batch_idx} - Delta loss: {loss_diff:.5f}, Delta entropy: {entropy_diff:.5f}\")\n",
        "        no_tpt_accuracies, accuracies = compute_accuracies(no_tpt_class_acc, tpt_class_acc)\n",
        "        histogram = make_histogram(no_tpt_accuracies, accuracies,\n",
        "                                'No TPT', 'TPT', save_path=f\"runs/{RUN_NAME}/class_accuracy%{batch_idx}e.png\")\n",
        "        writer.add_image(f\"Class accuracies%{batch_idx}e\", histogram, batch_idx, dataformats=\"HWC\")\n",
        "        logger.info(f\"[ACC] Batch num:{batch_idx} - Top1: {top1.get_avg()}, Top5: {top5.get_avg()}\")\n",
        "\n",
        "        dump_object = batch_idx, cumulative_loss, top1, top5, no_tpt_class_acc, tpt_class_acc\n",
        "        pickle.dump(dump_object, open(f\"runs/{RUN_NAME}/checkpoint%{batch_idx}.pkl\", \"wb\"))\n",
        "\n",
        "    # Draw histogram of class accuracies\n",
        "    no_tpt_accuracies, accuracies = compute_accuracies(no_tpt_class_acc, tpt_class_acc)\n",
        "    image = make_histogram(no_tpt_accuracies, accuracies, 'No TPT','TPT', save_path=f\"runs/{RUN_NAME}/accuracy_by_class.png\")\n",
        "    image = make_histogram(no_tpt_accuracies, accuracies, 'No TPT','TPT', save_path=f\"runs/{RUN_NAME}/accuracy_by_worst_class.png\", worst_case=True)\n",
        "\n",
        "    writer.add_image(\"Class accuracies\", image, 0, dataformats=\"HWC\")\n",
        "\n",
        "    return cumulative_loss.get_avg() , top1.get_avg()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9-NbRxlBzwB"
      },
      "source": [
        "In the main we load and augment the data, we load CLIP and COOP. We use AdamW as optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSN1lltmhgru"
      },
      "source": [
        "### MAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "5X1yoJagihsX"
      },
      "outputs": [],
      "source": [
        "# Global variables\n",
        "RUN_NAME = \"test\"\n",
        "ENSAMBLE_METHOD = \"entropy\"\n",
        "LOG_FREQUENCY = 10\n",
        "DEBUG = True\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main(\n",
        "    dataset_name=\"imagenet_a\",\n",
        "    backbone=\"ViT-B/16\",\n",
        "    device=\"mps\",\n",
        "    batch_size=64,\n",
        "    learning_rate=0.005,\n",
        "    run_name=RUN_NAME,\n",
        "    n_ctx=4,\n",
        "    ctx_init=\"a_photo_of_a\",\n",
        "    class_token_position=\"end\",\n",
        "    csc=False,\n",
        "    ice_loss=True,\n",
        "    debug=DEBUG\n",
        "):\n",
        "\n",
        "    checkpoints = [file for file in os.listdir(f\"runs/{RUN_NAME}\") if file.startswith(\"checkpoint\")]\n",
        "    if len(checkpoints) > 0:\n",
        "        files = sorted(checkpoints,\n",
        "                        key=lambda x: int(x.split(\"%\")[1].split(\".\")[0]),\n",
        "                        reverse=True)\n",
        "        checkpoint = pickle.load(open(f\"runs/{RUN_NAME}/{files[0]}\", \"rb\"))\n",
        "        from_idx = checkpoint[0]\n",
        "    else:\n",
        "        checkpoint = None\n",
        "        from_idx = 0\n",
        "\n",
        "    seed = 0\n",
        "    print(\"Using manual seed {}\".format(seed))\n",
        "    torch.manual_seed(seed)\n",
        "    # Create a logger for the experiment\n",
        "    run_name = RUN_NAME\n",
        "    writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
        "\n",
        "    _, preprocess = clip.load(backbone, device=device)\n",
        "\n",
        "    data_transform = Augmixer(preprocess, batch_size, augmix=True, severity=1)\n",
        "    # Get dataloaders\n",
        "    _, _, test_loader, classnames, id2class = get_data(\n",
        "        dataset_name, 1, data_transform, train_size=0, val_size=0, from_idx=from_idx\n",
        "    )\n",
        "\n",
        "    # Instantiate the network and move it to the chosen device (GPU)\n",
        "    net = OurCLIP(\n",
        "        classnames=classnames,\n",
        "        n_ctx=n_ctx,\n",
        "        ctx_init=ctx_init,\n",
        "        class_token_position=class_token_position,\n",
        "        backbone=backbone,\n",
        "        csc=csc,\n",
        "    ).to(device)\n",
        "\n",
        "    load_pretrained_coop(backbone, net, device)\n",
        "\n",
        "    print(\"Turning off gradients in both the image and the text encoder\")\n",
        "    for name, param in net.named_parameters():\n",
        "        if \"prompt_learner\" not in name:\n",
        "            param.requires_grad_(False)\n",
        "\n",
        "    print(f\"Total parameters: {sum(p.numel() for p in net.parameters()):,}\")\n",
        "    print(\n",
        "        f\"Total trainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\"\n",
        "    )\n",
        "\n",
        "    trainable_param = net.prompt_learner.parameters()\n",
        "    optimizer = get_optimizer(trainable_param, learning_rate)\n",
        "\n",
        "    cost_function = get_loss_function()\n",
        "\n",
        "    if device == 'cuda':\n",
        "        scaler = torch.cuda.amp.GradScaler(init_scale=1000)\n",
        "    else:\n",
        "        scaler = None\n",
        "\n",
        "    # Instantiate the captioner if needed\n",
        "    captioner = None\n",
        "    if ice_loss:\n",
        "        model_name = \"coca_ViT-L-14\"\n",
        "        version = \"laion2B-s13B-b90k\"\n",
        "        captioner = Captioner(model_name=model_name, version=version, device=device)\n",
        "\n",
        "    print(f\"Beginning testing with TPT + ice_loss={ice_loss}:\")\n",
        "    test_loss, test_accuracy = tpt_train_loop(test_loader, net, optimizer, cost_function, scaler, writer, id2classes=id2class, device=device, captioner=captioner, debug=debug, checkpoint=checkpoint)\n",
        "    print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
        "\n",
        "    create_run_info(dataset_name, backbone, ice_loss, test_accuracy, run_name, ENSAMBLE_METHOD)\n",
        "\n",
        "    writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "P85jhCUwhgru"
      },
      "outputs": [],
      "source": [
        "def run_experiment(\n",
        "    dataset_name=\"imagenet_a\",\n",
        "    backbone=\"ViT-B/16\",\n",
        "    device=\"mps\",\n",
        "    batch_size=64,\n",
        "    learning_rate=0.005,\n",
        "    run_name=RUN_NAME,\n",
        "    n_ctx=4,\n",
        "    ctx_init=\"a_photo_of_a\",\n",
        "    class_token_position=\"end\",\n",
        "    csc=False,\n",
        "    ice_loss=True,\n",
        "    debug=DEBUG):\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "            DEVICE = \"cuda\"\n",
        "    elif torch.backends.mps.is_available():\n",
        "        DEVICE = \"mps\"\n",
        "    else:\n",
        "        DEVICE = \"cpu\"\n",
        "\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    os.makedirs(f\"runs/{RUN_NAME}\", exist_ok=True)\n",
        "\n",
        "    # Remove this or handle checkpoint case\n",
        "    log_path = f\"runs/{RUN_NAME}/log.log\"\n",
        "\n",
        "    file_handler = logging.FileHandler(log_path)\n",
        "    stderr_handler = logging.StreamHandler(sys.stderr)\n",
        "\n",
        "    file_handler.setLevel(logging.DEBUG)\n",
        "    stderr_handler.setLevel(logging.ERROR)\n",
        "\n",
        "    file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "    stderr_formatter = logging.Formatter('\\r%(levelname)s - %(message)s')\n",
        "\n",
        "    logger.addHandler(file_handler)\n",
        "    logger.addHandler(stderr_handler)\n",
        "\n",
        "    main(dataset_name, backbone, DEVICE, batch_size, learning_rate, run_name, n_ctx, ctx_init, class_token_position, csc, ice_loss, debug)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running the experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following, we report the instructions to run the experiments on the ImageNetA and ImageNetV2 datasets. In case you want to run the experiments looking at the actual logits value, just set the global variable DEBUG to True. Some reports are generated while running the experiments.\n",
        "\n",
        "In order to resume any given training, it is sufficient to rerun the given cell.\n",
        "\n",
        "Args:\n",
        "- **dataset**: The dataset to be used. It can be either 'imageneta' or 'imagenetv2'.\n",
        "- **backbone**: The backbone to be used. Default is 'ViT-B/16'.\n",
        "- **batch_size**: Sample in a single batch of augmentations. Default is 64.\n",
        "- **ice_loss**: Boolean to enable captioning loss. Default is True."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To run baseline, no captioning involved, just TPT+CoOp\n",
        "RUN_NAME=\"baseline\"\n",
        "run_experiment(ice_loss=False, dataset_name=\"imagenet_a\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'batch_predictions/*': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# If you run with DEBUG=True, use this cell to empty the output_folders\n",
        "!rm -r batch_predictions/* batch_reports/* caption_reports/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To run with entropy average ensamble method (default)\n",
        "ENSAMBLE_METHOD = \"entropy\"\n",
        "RUN_NAME=\"entropy-avg\"\n",
        "run_experiment(ice_loss=True, dataset_name=\"imagenet_a\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENSAMBLE_METHOD = \"std_dev\"\n",
        "RUN_NAME = \"stddev\"\n",
        "run_experiment(ice_loss=True, dataset_name=\"imagenet_a\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENSAMBLE_METHOD = \"harmonic_mean\"\n",
        "RUN_NAME = \"harmonic-mean\"\n",
        "run_experiment(ice_loss=True, dataset_name=\"imagenet_a\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. Results and Future works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The overall results show that the entropy-weighted average of the logits outperforms the other two approaches. The ensemble model of CLIP with TPT+CoOp and CoCa achieved a **+2%** improvement in Top-1 accuracy on ImageNetA and ImageNetV2 with respect of the baseline which does not use CoCa. \n",
        "\n",
        "The entropy-weighted average of the logits outperforms [Image Caption Encoding for Improving Zero-Shot Generalisation](https://arxiv.org/pdf/2402.02662) which employs the standard deviation approach. Moreover, we found that using the logits of CoCa instead of CLIP for the caption logits is more effective than the solution proposed by the authors. This comes from the fact that the hidden size of CoCa is greater than the one of CLIP, so it can capture more information and generate more accurate captions.\n",
        "\n",
        "In the table below results are reported for the three different stacking approaches. In addition, the results of the afore mentioned paper are reported for comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Results](source/images/results.png)\n",
        "\n",
        "\\* Due to computational resource limitations we weren't able to fit the entire dataset + encoding in memory, hence we had to use a subset of the ImageNet-v2 dataset. We took a random sample of 500 classes and repeated the experiment 5 times. The results are the average of the 5 runs.\n",
        "\n",
        "\\*\\* Results of the paper [Image Caption Encoding for Improving Zero-Shot Generalisation](https://arxiv.org/pdf/2402.02662) using ICE loss with CLIP text encoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comment on results\n",
        "As the experiment shows, stacking the logits of the filtered images with the entropy-weighted average approach outperforms the other two approaches. The ensemble model of CLIP with TPT+CoOp and CoCa achieved a **+2%** improvement in Top-1 accuracy on ImageNetA with respect to the baseline. For what concerns the ImageNetV2 dataset, the improvement is still **≈ +2%** if the standard deviation is taken into account. As shown in the table the trend of the runs using entropy average tends to include very positive results with higher top-1 reached than the runs of the baseline. \n",
        "\n",
        "### 8.1 Future Works\n",
        "In the future we would like to explore the following:\n",
        "- Implement stacking with other captioning models such as [LLava](https://arxiv.org/abs/2304.08485), [BLIP](https://arxiv.org/abs/2201.12086) or the more recent [BLIP-2](https://arxiv.org/abs/2301.12597).\n",
        "- Implement the proposed approach on ImageNet-v2 without subsampling the dataset.\n",
        "- Implement the proposed approach on ImageNet variants\n",
        "- Implement another approach which instead of stacking exploits captions to improve CLIP classification by making prompts more informative, although this approach clashes with CoOp and TPT as these make prompt unreadable."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
